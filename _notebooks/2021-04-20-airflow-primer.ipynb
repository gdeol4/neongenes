{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A primer on Apache Airflow\n",
    "> Definitions and diagrams I drew about the very basics of Airflow, as a reference.\n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [data-engineering, python, airflow, DAG]\n",
    "- image: images/go-with-flow.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is airflow?\n",
    "Airflow is a platform used to author, schedule, and monitor workflows.\n",
    "It’s essentially a queuing system that runs on a metadata database and a scheduler that runs tasks. Workflows are written as Directed Acyclic Graphs (DAGs). A workflow and DAG are interchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are DAGs?\n",
    "A DAG is a collection of tasks you want to run and are organized in a way that illustrates dependencies and relationships between tasks.\n",
    "\n",
    "The image below shows how a DAG is a unidirectional, acyclic graph, where each node in the graph is a task and edges define dependencies among tasks. There is no case where you should be able to go backwards from a forward node to one that's already been executed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/airflow1_1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DAG can be broken up into smaller and smaller jobs and gives the user full control by generating dynamic pipelines written in code. Airflow DAGs are also extensible and can scale. DAGs are powerful because they allow for collaborative, manageable, and testable workflows. A bonus is that Airflow is developed in python and can interface with any python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/airflow1_2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above shows how Airflow divides the tasks into branches so that if one fails, there is still output from the other. Also, the processing time is reduced as parallel computing occurs. The chances of failure should decrease overall as each task is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are tasks executed?\n",
    "An operator represents a single task in a workflow that helps carry out your task (running a python function for example).\n",
    "Operators determine what actually gets to be done when your dag runs.\n",
    "A task is an operator when instantiated. It is something on which the worker works upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow Architecture\n",
    "\n",
    "![](images/airflow1_3.jpeg)\n",
    "\n",
    "- Metadata — is a relational database with info on task state, such as the top ten tasks consuming the most memory, it contains all data pertaining to jobs currently running as well as historical data.\n",
    "\n",
    "- Scheduler — decides which task to run, when, and in what order.\n",
    "\n",
    "- Web server— the UI which is essentially a flask app that talks to the metadata.\n",
    "\n",
    "- Executor — performs the task at ground level. The executor is a message queuing process that figures out which workers will execute which tasks. The default is the sequential executor — which cannot run tasks in parallel — meaning it can’t be used for production level code. The local executor can be used too which will run tasks till all resources on the server are at capacity. This is good for a moderate amount of DAGs. Both of these are used in single node clusters and therefore cannot be used to scaled.\n",
    "\n",
    "- Multi node clusters — have the same components and only the scheduler and web server are placed in the same node (master), the workers are placed in a separate instance. This set up works well because it allows for scaling by letting you add more multi-node clusters (celery is the executor of choice here for python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not dealing with terabytes of data then it's better to have the scheduler, web server, and executor together in the master node/cluster. The downside is that this single cluster approach runs everything on the same machine, so if you make a change to a DAG/scheduler, then you need to restart the entire workflow — even tasks that were in the process of executing. Celery avoids this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/airflow1_4.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do build a distributed workflow with celery then a queuing system component is needed (like Redis). For local workflows, the queuing is handled by the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The life cycle of a task\n",
    "\n",
    "1. The scheduler periodically checks the DAG folder to see if there are any DAGS that need to be run.\n",
    "\n",
    "2. If any DAGS are found pending execution, the scheduler creates a diagram for it, which is an instantiation of a DAG in real time.\n",
    "\n",
    "3. The scheduler will update the DAG state to running in the metadata and the tasks will execute.\n",
    "\n",
    "4. The scheduler then reads the DAG and puts the tasks in order of execution into the queuing system in the form of a message. Each message contains info like DAG ID, TASK ID, and function to be executed.\n",
    "\n",
    "5. The status of these tasks changes to queued at that point.\n",
    "\n",
    "6. The executor then begins to execute tasks and sends fail/success messages for the tasks to the metadata.\n",
    "\n",
    "7. The scheduler finally updates the status of the diagram when all tasks have run to success or failure."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
