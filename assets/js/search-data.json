{
  
    
        "post0": {
            "title": "Scaffold splitting and initial model training using chemprop",
            "content": "Chemprop . I found the chemprop library in the paper A Deep Learning Approach to Antibiotic Discovery30102-1&quot;), it uses the directed message passing neural network (D-MPNN) described in Analyzing Learned Molecular Representations for Property Prediction. . The drug discovery workflow described in the paper uses a training dataset of 2335 molecules with binary e.coli growth inhibition labels to train a classification model. . The D-MPNN architecture as described in the paper: . The D-MPNN architecture translates the graph representation of a molecule into a continuous vector via a directed bond-based message-passing approach. This builds a molecular representation by iteratively aggregating the features of individual atoms and bonds. The model operates by passing “messages” along bonds that encode information about neighboring atoms and bonds. By applying this message passing operation multiple times, the model constructs higher-level bond messages that contain information about larger chemical substructures. The highest-level bond messages are then combined into a single continuous vector representing the entire molecule. . The researchers generated molecular features using RDKit to tackle overfitting, increased algorithm robustness by using an ensemble of classifiers, and optimized hyperparameters with bayesian optimization. The model achieved a ROC-AUC of 0.896 on test data. . This notebook will apply the neural network to the dataset of blood-brain barrier molecules gathered in the previous notebooks. . Scaffold splitting . A distributional shift is a change in the data distribution between an algorithm&#39;s training dataset, and a dataset it encounters when deployed. Distributional shifts are common in the field of molecular property prediction where chemical space is huge and different areas of it exhibit high structural heterogeneity. The shifts tend to be large and difficult to handle for machine learning models. . Scaffold splitting is one solution to this distributional shift. It&#39;s first described in &quot;The properties of known drugs. 1. Molecular frameworks: . A molecular scaffold reduces the chemical structure of a compound to its core components, essentially by removing all side chains and only keeping ring systems and parts that link together ring systems. An additional option for making molecular scaffolds even more general is to “forget” the identities of the bonds and atoms by replacing all atoms with carbons and all bonds with single bonds. . A Murcko scaffold essentially extracts the molecular backbone by extracting the ring structures and the linkers that connect them. These scaffolds collapse molecules into bins based on their generated Murcko scaffold, reducing the number of highly similar structures. When splitting the data by scaffold, molecules sharing a scaffold are in the same split, meaning no similar molecules will be found across different splits. Any bins larger than half of the desired test set size are placed into the training set, to guarantee the scaffold diversity of the validation and test sets. All remaining bins are placed randomly into the training, validation, and test sets until each set has reached its desired size. Clustering molecules by scaffold ensures that the model will be trained on structurally diverse folds when cross-validating. . Compared to a random split, a scaffold split is a more challenging and realistic evaluation setting as it more closely approximates the split present in real-world property prediction data. . A molecule compared to its scaffold below: . Molecular compound and its generic Bemis-Murcko scaffold . from rdkit import Chem from rdkit.Chem.Scaffolds import MurckoScaffold # define compound via its SMILES string smiles = &quot;CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC&quot; # convert SMILES string to RDKit mol object mol = Chem.MolFromSmiles(smiles) # create RDKit mol object corresponding to Bemis-Murcko scaffold of original compound mol_scaffold = MurckoScaffold.GetScaffoldForMol(mol) # make the scaffold generic by replacing all atoms with carbons and all bonds with single bonds mol_scaffold_generic = MurckoScaffold.MakeScaffoldGeneric(mol_scaffold) # convert the generic scaffold mol object back to a SMILES string format smiles_scaffold_generic = Chem.CanonSmiles(Chem.MolToSmiles(mol_scaffold_generic)) # display compound and its generic Bemis-Murcko scaffold display(mol) print(smiles) display(mol_scaffold_generic) print(smiles_scaffold_generic) . CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC . C1CCC(CCC2C3CCCCC3CC3CCCCC32)CC1 . Generating generic Bemis-Murcko scaffolds . Below I&#39;ve written code to generate the scaffolds and a few other related molecular representations. But binning and writing the code to split them myself would take too much time so I use a built-in feature of chemprop to split the data before model training. . # import packages import chemprop import pandas as pd import datamol as dm pd.options.mode.chained_assignment = None # default=&#39;warn&#39; # loading the datasets MolNet = pd.read_csv(&quot;data/MoleculeNet.csv&quot;) B3DB = pd.read_csv(&quot;data/B3DB.csv&quot;) b3_molecules = pd.read_csv(&quot;data/b3_molecules.csv&quot;) . scaffold_split function . This function generates and sanitizes mols, generates a scaffold from the mol object, generalizes the scaffold, and lastly converts the scaffold to a SMILES string, returning a dataframe. . def scaffold_split(df): df[&quot;mol&quot;] = [Chem.MolFromSmiles(x) for x in df[&quot;standard_smiles&quot;]] # generating mols from the standard_smiles column df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=True) for x in df[&#39;mol&#39;]] # sanitize mol objects df = df.dropna() # dropping NA values df[&quot;scaffold&quot;] = [MurckoScaffold.GetScaffoldForMol(x) for x in df[&quot;mol&quot;]] # generating scaffolds from mol object df[&quot;mol_scaffold_generic&quot;] = [MurckoScaffold.MakeScaffoldGeneric(x) for x in df[&quot;scaffold&quot;]] # generalizing scaffolds # convert the generic scaffold mol object back to a SMILES string format df[&quot;smiles_scaffold_generic&quot;] = [Chem.CanonSmiles(Chem.MolToSmiles(x)) for x in df[&quot;mol_scaffold_generic&quot;]] return df . # the data prior to processing MolNet.head(1) . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | . # results of the scaffold generating function data_split = scaffold_split(b3_molecules) data_split.head(1) . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey scaffold mol_scaffold_generic smiles_scaffold_generic . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | C1CCC2CCCCC2C1 | . chemprop_prep function . This function serves to prepare the data to be used by the chemprop library. Specifically, the input data must contain a SMILES string in the first column and the target value - a binary value in this experiment. The resulting dataframe is saved as a CSV file to later be used by chemprop. . # A function to process the data for use with the chemprop library def chemprop_prep(df, filename): df = df.drop([&quot;mol&quot;, &quot;SMILES&quot;, &quot;selfies&quot;, &quot;inchi&quot;, &quot;inchikey&quot;], axis=1) # drop all columns except the smiles and target df[&quot;smiles&quot;] = df[&quot;standard_smiles&quot;] # use standard smiles inplace of smiles df = df.drop([&quot;standard_smiles&quot;], axis=1) # drop this column now df = df[[&quot;smiles&quot;, &quot;BBB+/BBB-&quot;]] # reorder the columns with smiles first and target second df.to_csv(&#39;./data/&#39; + filename + &#39;.csv&#39;, index=False) # save the file return df . # Processing the three different datasets molnet_chemprop = chemprop_prep(MolNet, &#39;molnet_chemprop&#39;) B3DB_chemprop = chemprop_prep(B3DB, &#39;B3DB_chemprep&#39;) b3_mol_chemprop = chemprop_prep(b3_molecules, &#39;b3_mol_chemprop&#39;) . # results of processing molnet_chemprop.head(1) . smiles BBB+/BBB- . 0 CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | 1 | . Model training with chemprop . I ran three experiments with slight differences to test out the predictive ability on the blood-brain barrier molecules: . The split type is set to scaffold balanced; the number of folds is 10, class balancing is set to true, and 200 features are generated with RDKits 2d descriptor generator. The features are scaled when calculated and thus no further scaling is specified. . | This run sets the number of training epochs within each fold at 100. The rest of the settings mentioned above are kept the same. . | The last run has the number of folds increased from 10 to 30 while epochs per fold are set to 20. . | These models are trained using the MoleculeNet data (~2035 molecules), further tests will use the combined dataset with ~8100 molecules. . Model training 1 . arguments = [ &#39;--data_path&#39;, &#39;./data/molnet_chemprop.csv&#39;, &#39;--dataset_type&#39;, &#39;classification&#39;, &#39;--save_dir&#39;, &#39;./data/chemprop_checkpoints/&#39;, &#39;--split_type&#39;, &#39;scaffold_balanced&#39;, # &#39;--separate_val_path&#39;, &#39;./data/chemprop_B3DB.csv&#39;, &#39;--num_folds&#39;, &#39;10&#39;, &#39;--class_balance&#39;, &#39;--features_generator&#39;, &#39;rdkit_2d_normalized&#39;, &#39;--no_features_scaling&#39;, &#39;--quiet&#39; ] args = chemprop.args.TrainArgs().parse_args(arguments) mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training) . Model training 2 . arguments = [ &#39;--data_path&#39;, &#39;./data/molnet_chemprop.csv&#39;, &#39;--dataset_type&#39;, &#39;classification&#39;, &#39;--save_dir&#39;, &#39;./data/chemprop_checkpoints/&#39;, &#39;--split_type&#39;, &#39;scaffold_balanced&#39;, # &#39;--separate_val_path&#39;, &#39;./data/chemprop_B3DB.csv&#39;, &#39;--num_folds&#39;, &#39;10&#39;, &#39;--class_balance&#39;, &#39;--features_generator&#39;, &#39;rdkit_2d_normalized&#39;, &#39;--no_features_scaling&#39;, &#39;--quiet&#39;, &#39;--epochs&#39;, &#39;100&#39; ] args = chemprop.args.TrainArgs().parse_args(arguments) mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training) . Model training 3 . arguments = [ &#39;--data_path&#39;, &#39;./data/molnet_chemprop.csv&#39;, &#39;--dataset_type&#39;, &#39;classification&#39;, &#39;--save_dir&#39;, &#39;./data/chemprop_checkpoints/&#39;, &#39;--split_type&#39;, &#39;scaffold_balanced&#39;, # &#39;--separate_val_path&#39;, &#39;./data/chemprop_B3DB.csv&#39;, &#39;--num_folds&#39;, &#39;30&#39;, &#39;--class_balance&#39;, &#39;--features_generator&#39;, &#39;rdkit_2d_normalized&#39;, &#39;--no_features_scaling&#39;, &#39;--quiet&#39;, &#39;--epochs&#39;, &#39;20&#39;, ] args = chemprop.args.TrainArgs().parse_args(arguments) mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training) . Results . Experiment 1: . 10-fold cross validation . Seed 0 ==&gt; test auc = 0.928767 Seed 1 ==&gt; test auc = 0.903879 Seed 2 ==&gt; test auc = 0.955724 Seed 3 ==&gt; test auc = 0.862450 Seed 4 ==&gt; test auc = 0.890246 Seed 5 ==&gt; test auc = 0.892326 Seed 6 ==&gt; test auc = 0.886606 Seed 7 ==&gt; test auc = 0.951849 Seed 8 ==&gt; test auc = 0.906486 Seed 9 ==&gt; test auc = 0.937866 . Overall test auc = 0.911620 +/- 0.029164 Elapsed time = 0:20:54 . This run was the fastest and used the default number of epochs and fold. The overall AUC for the test data was 0.911620 +/- 0.029164. . Experiment 2: . 10-fold cross validation . Seed 0 ==&gt; test auc = 0.929177 Seed 1 ==&gt; test auc = 0.852848 Seed 2 ==&gt; test auc = 0.944112 Seed 3 ==&gt; test auc = 0.862913 Seed 4 ==&gt; test auc = 0.890246 Seed 5 ==&gt; test auc = 0.893487 Seed 6 ==&gt; test auc = 0.881697 Seed 7 ==&gt; test auc = 0.941933 Seed 8 ==&gt; test auc = 0.920437 Seed 9 ==&gt; test auc = 0.917494 . Overall test auc = 0.903435 +/- 0.030385 Elapsed time = 16:49:39 . This run took the most time, nearing almost 17 hours with 100 epochs for each fold. The overall AUC for the test data was 0.903435 +/- 0.030385 which underperformed compared to the model in the first experiment that ran using default parameters. . Experiment 3: . 30-fold cross validation . Seed 0 ==&gt; test auc = 0.928630 Seed 1 ==&gt; test auc = 0.907636 Seed 2 ==&gt; test auc = 0.943526 Seed 3 ==&gt; test auc = 0.882607 Seed 4 ==&gt; test auc = 0.890246 Seed 5 ==&gt; test auc = 0.892559 Seed 6 ==&gt; test auc = 0.870606 Seed 7 ==&gt; test auc = 0.947479 Seed 8 ==&gt; test auc = 0.903720 Seed 9 ==&gt; test auc = 0.930991 Seed 10 ==&gt; test auc = 0.795291 Seed 11 ==&gt; test auc = 0.894284 Seed 12 ==&gt; test auc = 0.936275 Seed 13 ==&gt; test auc = 0.930272 Seed 14 ==&gt; test auc = 0.950706 Seed 15 ==&gt; test auc = 0.919849 Seed 16 ==&gt; test auc = 0.877632 Seed 17 ==&gt; test auc = 0.871467 Seed 18 ==&gt; test auc = 0.888110 Seed 19 ==&gt; test auc = 0.905920 Seed 20 ==&gt; test auc = 0.921946 Seed 21 ==&gt; test auc = 0.928851 Seed 22 ==&gt; test auc = 0.870864 Seed 23 ==&gt; test auc = 0.865779 Seed 24 ==&gt; test auc = 0.902960 Seed 25 ==&gt; test auc = 0.946100 Seed 26 ==&gt; test auc = 0.900540 Seed 27 ==&gt; test auc = 0.923984 Seed 28 ==&gt; test auc = 0.900345 Seed 29 ==&gt; test auc = 0.905261 . Overall test auc = 0.904481 +/- 0.031870 Elapsed time = 0:38:51 . This run took less than an hour (~40 minutes), with 20 training epochs per fold. Increasing the number of folds and epochs seems to bring down the model&#39;s accuracy compared to the default settings. The overall AUC for the test data was AUC= 0.904481 +/- 0.031870, which still marginally outperformed the model in experiment 2 that took nearly 17 hours. .",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/12/data-splitting-and-training.ipynb.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/12/data-splitting-and-training.ipynb.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a drug discovery web app - the roadmap",
            "content": "My goal: . The end goal for this project is to build a web app dashboard to monitor the accuracy of machine learning models on generating and classifying molecules with blood brain barrier permeability. . My goal is to train a classification model using the D-MPNN architecture, screen the Drug Repurposing Hub for molecules identified as blood brain barrier permeable, mutate the top 10 molecules with the STONED SELFIES algorithm, check these molecules for existing drugs, predict toxicity and other molecular properties. . B3_screen -&gt; a web app that classifies molecules as blood brain barrier permeable or not. .",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/smiles/cheminformatics/drug%20discovery/rdkit/2021/11/11/project-1-machine-learning.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/drug%20discovery/rdkit/2021/11/11/project-1-machine-learning.html",
            "date": " • Nov 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Processing the B3DB brain-blood barrier dataset",
            "content": "Gathering data on blood brain barrier permeability . This post is a part of a fullstack machine learning web app project and this notebook contains the data needed to build and train the models. The goal in this post is to clean and preprocess the B3DB dataset and merge it with the blood brain permeability data from MoleculeNet. The merged dataset should contain nearly 10,000 molecules labeled with their ability to pass through the blood brain barrier. The notebook provided in the B3DB repository also contains an interesting PCA plot, which is a good starting place for EDA when the data is merged. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from rdkit import Chem from rdkit.Chem import AllChem from sklearn.decomposition import PCA import datamol as dm %matplotlib inline . Removing, remapping, and creating features . This dataset has features that can be dropped since they won&#39;t contribute to model training. Some features are then calculated from the SMILES data. . # reading in the data bbb_df = pd.read_csv(&quot;data/B3DB_classification.tsv&quot;, sep=&quot; t&quot;) # dropping columns bbb_df = bbb_df.drop([&quot;CID&quot;, &quot;logBB&quot;, &quot;Inchi&quot;, &quot;threshold&quot;, &quot;reference&quot;, &quot;group&quot;, &quot;comments&quot;, &quot;NO.&quot;, &quot;IUPAC_name&quot;, &quot;compound_name&quot;], axis=1) # mapping given labels to binary bbb_df[&#39;BBB+/BBB-&#39;] = bbb_df[&#39;BBB+/BBB-&#39;].map({&#39;BBB+&#39;: 1, &#39;BBB-&#39;: 0}) . Feature generation . The function below processes and generates features such as mol objects, selfies, inchi, and inchikeys for each molecule using the datamol library. . # preprocessing function for molecules def preprocess_smiles(df): df[&quot;mol&quot;] = [dm.to_mol(x) for x in df[&#39;SMILES&#39;]] # generating mols from SMILES df[&quot;mol&quot;] = [dm.fix_mol(x) for x in df[&#39;mol&#39;]] # Fixing mols df = df.dropna() # dropping NA values df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=False) for x in df[&#39;mol&#39;]] # sanitize mol objects df[&quot;mol&quot;] = [dm.standardize_mol(x, disconnect_metals=False, normalize=True, reionize=True, uncharge=False, stereo=True) for x in df[&#39;mol&#39;]] # standardize mol objects df[&quot;standard_smiles&quot;] = [dm.standardize_smiles(x) for x in df[&#39;SMILES&#39;]] # standardize SMILES df[&quot;selfies&quot;] = [dm.to_selfies(x) for x in df[&#39;mol&#39;]] # generate SELFIES df[&quot;inchi&quot;] = [dm.to_inchi(x) for x in df[&#39;mol&#39;]] # Generating InChi df[&quot;inchikey&quot;] = [dm.to_inchikey(x) for x in df[&#39;mol&#39;]] # Generating InChIKey return df data_clean = preprocess_smiles(bbb_df) # Making a copy of the dataframe for later B3DB = data_clean data_clean.head() # Saving the data as B3DB; after where its found data_clean.to_csv(&#39;./data/B3DB.csv&#39;, index=False) . Merging the two datasets . The datasets would have ideally added up to 9846 molecules but that doesnt account for duplicates. Since the B3DB consists of data from across 50 studies, I assumed most of the molecules from MoleculeNet would appear in B3DB. . Counting the unique inchikey values confirmed this, leaving the final dataset with 8091 molecules. This number is lower than the sum of both dataframes but it adds 284 novel compounds to the final dataset. . # Loading the MoleculeNet dataset MolNet = pd.read_csv(&quot;data/MoleculeNet.csv&quot;) # concatenating both dataframes final_df = pd.concat([MolNet, B3DB]) . # Number of molecules before filtering for duplicates final_df.shape . (9846, 7) . # inchikey values should be unique to each molecule final_df.inchikey.value_counts() . FXHJGPDCPMCUKW-UHFFFAOYSA-N 4 UHSKFQJFRQCDBE-UHFFFAOYSA-N 4 CSIVCTHRYRVJCI-UHFFFAOYSA-N 4 UUQMNUMQCIQDMZ-UHFFFAOYSA-N 4 XHMYQXZLVLGNKX-UHFFFAOYSA-N 3 .. XYGVIBXOJOOCFR-BTJKTKAUSA-N 1 NINYZUDVKTUKIA-UHFFFAOYSA-N 1 BVCKFLJARNKCSS-ZJLJEUSSSA-N 1 HOCWPKXKMNXINF-CJIHYQBJSA-N 1 UWHAHBDBSBVMIY-VUXXLBMGSA-N 1 Name: inchikey, Length: 8091, dtype: int64 . # Dropping duplicates based on unique inchikey values final_df = final_df.drop_duplicates(subset=&#39;inchikey&#39;, keep=&quot;first&quot;) # Saving the final dataframe as b3_molecules final_df.to_csv(&#39;./data/b3_molecules.csv&#39;, index=False) . PCA analysis . PCA does not discard any variables and instead it reduces the number of dimensions by constructing principal components. Principal components describe variation and account for the varied influences of the original features. . Each SMILES string will have a morgan fingerprint generated. These fingerprints are binary 2048 bit arrays which describe molecular structure. . Three plots were made, the first using the MolNet dataset, the B3DB data next, and the combined data last. . # MolNet # compute ECFP6 Morgan fingerprints with radius 3 fps_molnet = [] for idx, row in MolNet.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_molnet.append(fp.ToBitString()) # Computing ECFP6 fingerprints for B3DB fps_B3DB = [] for idx, row in B3DB.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_B3DB.append(fp.ToBitString()) # Computing ECFP6 fingerprints for B3DB fps_final = [] for idx, row in final_df.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_final.append(fp.ToBitString()) # Create a numpy array and use the u1 datatype (uint8 8-bit unsigned integer) fps_arr_molnet = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_molnet]) fps_arr_B3DB = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_B3DB]) fps_arr_final = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_final]) # PCA on MolNet molecules molnet_fps = pd.DataFrame(fps_arr_molnet, index=MolNet.index) molnet_fps = pd.concat([MolNet, molnet_fps], axis=1) pca_molnet = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_molnet) molnet_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] molnet_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] # PCA on B3DB molecules B3DB_fps = pd.DataFrame(fps_arr_B3DB, index=B3DB.index) B3DB_fps = pd.concat([B3DB, B3DB_fps], axis=1) pca_B3DB = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_B3DB) B3DB_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] B3DB_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] # PCA on final set of molecules final_fps = pd.DataFrame(fps_arr_final, index=final_df.index) final_fps = pd.concat([final_df, final_fps], axis=1) pca_final = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_final) final_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] final_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] . PCA Visualizations . PCA of molecules in the MoleculeNet dataset . fig_molnet = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=molnet_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the MolecularNet dataset&#39;) plt.show() . PCA of molecules in the B3DB dataset . fig_B3DB = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=B3DB_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the B3DB dataset&#39;) plt.show() . PCA of molecules in the combined and filtered dataset . fig_final = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=final_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the combined and filtered dataset&#39;) plt.show() .",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/09/using-the-B3DB-dataset.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/09/using-the-B3DB-dataset.html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Preprocessing a dataset of blood brain barrier molecules with Datamol",
            "content": "SMILES (Simplified Molecular Input Line Entry System) is a standard notation representing the molecular structure of a compound as a string representation that can be understood by a computer. The SMILES notation consists of a handful of rules which allow for converting the string to an image or graph. SMILES can then be easily used for generating further representations to train machine learning models with. . import datamol as dm import pandas as pd pd.options.mode.chained_assignment = None # default=&#39;warn&#39; . BBBP_df = pd.read_csv(&quot;data/BBBP.csv&quot;) BBBP_df.head() . num name p_np smiles . 0 1 | Propanolol | 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | . 1 2 | Terbutylchlorambucil | 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | . 2 3 | 40730 | 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | . 3 4 | 24 | 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | . 4 5 | cloxacillin | 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | . The dataframe shows 4 named columns, including the &quot;num&quot; of the molecule, the name, a binary label for blood brain barrier permeability status &quot;p_np&quot;, and the SMILES string. . # The name and number can be dropped BBBP_df = BBBP_df.drop([&quot;num&quot;, &quot;name&quot;], axis=1) # Checking the data for null values BBBP_df[&quot;smiles&quot;].isnull().values.any() # Renaming the binary label to &quot;BBB+/BBB-&quot; for clarity BBBP_df.columns = [&#39;BBB+/BBB-&#39;, &#39;SMILES&#39;] . BBBP_df . BBB+/BBB- SMILES . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | . 1 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | . 2 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | . 3 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | . 4 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | . ... ... | ... | . 2045 1 | C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl | . 2046 1 | [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](... | . 2047 1 | [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=... | . 2048 1 | C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC... | . 2049 1 | [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]... | . 2050 rows × 2 columns . Mols and smiles need to be sanitized as it will leave us with SMILES that are complete nonesense, for example, errors resulting from kekulization. . . RDkit generates the alternate position of double bonds, and then (in a second step they call &quot;aromatization&quot;) labels the ring as aromatic. In panel (2), there are three possible Lewis structures contributing to the actual structure (i.e. there is resonance), so the software would have to generate all three to be able to search for identical structures. [1] . Below is a function using datamol to preprocess the dataset, including steps to generate mol objects, SELFIES, inchi, and inchikeys for each molecule. The function also standardizes mols and SMILES, drops NA values, and returns a dataframe. . def preprocess_smiles(df): df[&quot;mol&quot;] = [dm.to_mol(x) for x in df[&#39;SMILES&#39;]] # generating mols from SMILES df[&quot;mol&quot;] = [dm.fix_mol(x) for x in df[&#39;mol&#39;]] # Fixing mols df = df.dropna() # dropping NA values df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=False) for x in df[&#39;mol&#39;]] # sanitize mol objects df[&quot;mol&quot;] = [dm.standardize_mol(x, disconnect_metals=False, normalize=True, reionize=True, uncharge=False, stereo=True) for x in df[&#39;mol&#39;]] # standardize mol objects df[&quot;standard_smiles&quot;] = [dm.standardize_smiles(x) for x in df[&#39;SMILES&#39;]] # standardize SMILES df[&quot;selfies&quot;] = [dm.to_selfies(x) for x in df[&#39;mol&#39;]] # generate SELFIES df[&quot;inchi&quot;] = [dm.to_inchi(x) for x in df[&#39;mol&#39;]] # Generating InChi df[&quot;inchikey&quot;] = [dm.to_inchikey(x) for x in df[&#39;mol&#39;]] # Generating InChIKey return df . Running the function and taking a look at the outputs . data_clean = preprocess_smiles(BBBP_df) . data_clean.shape . (2039, 7) . data_clean.head() . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | . 1 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)(C)OC(=O)CCCc1ccc(N(CCCl)CCCl)cc1 | [C][C][Branch1][C][C][Branch1][C][C][O][C][=Br... | InChI=1S/C18H27Cl2NO2/c1-18(2,3)23-17(22)6-4-5... | SZXDOYFHSIIZCF-UHFFFAOYSA-N | . 2 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC1COc2c(N3CCN(C)CC3)c(F)cc3c(=O)c(C(=O)O)cn1c23 | [C][C][C][O][C][=C][Branch1][N][N][C][C][N][Br... | InChI=1S/C18H20FN3O4/c1-10-9-26-17-14-11(16(23... | GSDSWSVVBLHKDQ-UHFFFAOYSA-N | . 3 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(=O)NCCCOc1cccc(CN2CCCCC2)c1 | [C][C][=Branch1][C][=O][N][C][C][C][O][C][=C][... | InChI=1S/C17H26N2O2/c1-15(20)18-9-6-12-21-17-8... | FAXLXLJWHQJMPK-UHFFFAOYSA-N | . 4 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | Cc1onc(-c2ccccc2Cl)c1C(=O)N[C@@H]1C(=O)N2[C@@H... | [C][C][O][N][=C][Branch1][#Branch2][C][=C][C][... | InChI=1S/C19H18ClN3O5S/c1-8-11(12(22-28-8)9-6-... | LQOLIRLGBULYKD-JKIFEVAISA-N | . The data contains a 3:1 ratio of positive to negative labels, which creates a bias towards molecules with blood brain permeability properties. This may need to be addressed when training models. The next steps are to save the cleaned data for further analysis. . counts = data_clean[&#39;BBB+/BBB-&#39;].value_counts().to_dict() print(counts) . {1: 1560, 0: 479} . data_clean.to_csv(&#39;./data/MoleculeNet.csv&#39;, index=False) . References . Urbaczek, Sascha. A consistent cheminformatics framework for automated virtual screening. Ph.D. Thesis, Universität Hamburg, August 2014. URL: http://ediss.sub.uni-hamburg.de/volltexte/2015/7349/; URN: urn:nbn:de:gbv:18-73491; PDF via Semantic Scholar |",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/10/22/working-with-SMILES.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/10/22/working-with-SMILES.html",
            "date": " • Oct 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Writing a function in python to perform a restriction enzyme digest",
            "content": "Creating a restriction enzyme dictionary . Restriction enzymes are proteins produced by bacteria that cleave DNA at specific sites along the molecule. The enzyme functions on a specific, short nucleotide sequence and cuts the DNA only at that specific site, which is known as restriction site or target sequence. In the bacterial cell, restriction enzymes cleave foreign DNA, thus eliminating infecting organisms. The activity of a restriction enzyme can be defined by its recognition site on the DNA sequence and the position relative to the recognition site, at which it cuts the DNA. . # create enzyme dictionary restrictionEnzymes = {} # add &quot;bamH1&quot; and &quot;sma1&quot; enzymes, their target sequence, and position releative to the recognition site restrictionEnzymes[&#39;bamH1&#39;] = [&#39;ggatcc&#39;,0] restrictionEnzymes[&#39;sma1&#39;] = [&#39;cccggg&#39;,2] # a function to calculate the molecular weight of dna sequences def oligoMolecularWeight(sequence): # create a dictionairy of DNA basepair molecular weights dnaMolecularWeight = {&#39;a&#39;:313.2,&#39;c&#39;:289.2,&#39;t&#39;:304.2,&#39;g&#39;:329.2} # initialize molecular weight molecularWeight = 0.0 # iterate through DNA sequnce and update weight of sequence for base in sequence: molecularWeight += dnaMolecularWeight[base] return molecularWeight # the primary function for restriction digest def digest(sequence, enzyme): # set target sequence target = restrictionEnzymes[enzyme][0] # enzyme cut position relative to recognition site cutPosition = restrictionEnzymes[enzyme][1] # a list to collect DNA fragments fragments = [] # counter for the position of the last restriction site; beginning of sequence found = 0 # a variable to store the position of the last cut; end of sequence lastCut = found # variable to set where to search for the next site from searchFrom = lastCut while found != -1: found = sequence.find(target, searchFrom) if found != -1: fragment = sequence[lastCut:found+cutPosition] mwt = oligoMolecularWeight(fragment) fragments.append((fragment,mwt)) else: fragment = sequence[lastCut:] mwt = oligoMolecularWeight(fragment) fragments.append((fragment,mwt)) lastCut = found + cutPosition searchFrom = lastCut + 1 return fragments . Running the function on a test sequence results in the following: . digestSequence = &quot;gcgatgctaggatccgcgatcgcgtacgatcgtacgcggtacggacggatccttctc&quot; . digested_dna = digest(digestSequence,&#39;bamH1&#39;) . print(digested_dna) . [(&#39;gcgatgcta&#39;, 2800.7999999999997), (&#39;ggatccgcgatcgcgtacgatcgtacgcggtacggac&#39;, 11478.400000000005), (&#39;ggatccttctc&#39;, 3345.1999999999994)] .",
            "url": "https://www.gurkamal.com//python/bioinformatics/restriction%20enzyme/function/2021/10/18/restriction-enzyme-digest.html",
            "relUrl": "/python/bioinformatics/restriction%20enzyme/function/2021/10/18/restriction-enzyme-digest.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "A primer on Apache Airflow",
            "content": "What is airflow? . Airflow is a platform used to author, schedule, and monitor workflows. It’s essentially a queuing system that runs on a metadata database and a scheduler that runs tasks. Workflows are written as Directed Acyclic Graphs (DAGs). A workflow and DAG are interchangeable. . What are DAGs? . A DAG is a collection of tasks you want to run and are organized in a way that illustrates dependencies and relationships between tasks. . The image below shows how a DAG is a unidirectional, acyclic graph, where each node in the graph is a task and edges define dependencies among tasks. There is no case where you should be able to go backwards from a forward node to one that&#39;s already been executed. . . A DAG can be broken up into smaller and smaller jobs and gives the user full control by generating dynamic pipelines written in code. Airflow DAGs are also extensible and can scale. DAGs are powerful because they allow for collaborative, manageable, and testable workflows. A bonus is that Airflow is developed in python and can interface with any python API. . . The image above shows how Airflow divides the tasks into branches so that if one fails, there is still output from the other. Also, the processing time is reduced as parallel computing occurs. The chances of failure should decrease overall as each task is independent. . How are tasks executed? . An operator represents a single task in a workflow that helps carry out your task (running a python function for example). Operators determine what actually gets to be done when your dag runs. A task is an operator when instantiated. It is something on which the worker works upon. . Airflow Architecture . Metadata — is a relational database with info on task state, such as the top ten tasks consuming the most memory, it contains all data pertaining to jobs currently running as well as historical data. . | Scheduler — decides which task to run, when, and in what order. . | Web server— the UI which is essentially a flask app that talks to the metadata. . | Executor — performs the task at ground level. The executor is a message queuing process that figures out which workers will execute which tasks. The default is the sequential executor — which cannot run tasks in parallel — meaning it can’t be used for production level code. The local executor can be used too which will run tasks till all resources on the server are at capacity. This is good for a moderate amount of DAGs. Both of these are used in single node clusters and therefore cannot be used to scaled. . | Multi node clusters — have the same components and only the scheduler and web server are placed in the same node (master), the workers are placed in a separate instance. This set up works well because it allows for scaling by letting you add more multi-node clusters (celery is the executor of choice here for python). . | . If you&#39;re not dealing with terabytes of data then it&#39;s better to have the scheduler, web server, and executor together in the master node/cluster. The downside is that this single cluster approach runs everything on the same machine, so if you make a change to a DAG/scheduler, then you need to restart the entire workflow — even tasks that were in the process of executing. Celery avoids this. . . If you do build a distributed workflow with celery then a queuing system component is needed (like Redis). For local workflows, the queuing is handled by the system. . The life cycle of a task . The scheduler periodically checks the DAG folder to see if there are any DAGS that need to be run. . | If any DAGS are found pending execution, the scheduler creates a diagram for it, which is an instantiation of a DAG in real time. . | The scheduler will update the DAG state to running in the metadata and the tasks will execute. . | The scheduler then reads the DAG and puts the tasks in order of execution into the queuing system in the form of a message. Each message contains info like DAG ID, TASK ID, and function to be executed. . | The status of these tasks changes to queued at that point. . | The executor then begins to execute tasks and sends fail/success messages for the tasks to the metadata. . | The scheduler finally updates the status of the diagram when all tasks have run to success or failure. . |",
            "url": "https://www.gurkamal.com//data-engineering/python/airflow/dag/2021/04/20/airflow-primer.html",
            "relUrl": "/data-engineering/python/airflow/dag/2021/04/20/airflow-primer.html",
            "date": " • Apr 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.gurkamal.com//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.gurkamal.com//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}