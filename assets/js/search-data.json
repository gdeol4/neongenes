{
  
    
        "post0": {
            "title": "Connecting BigQuery and Jupyter",
            "content": "Importing libraries . # pip install --upgrade &#39;google-cloud-bigquery[bqstorage,pandas]&#39; import os import warnings warnings.filterwarnings(&#39;ignore&#39;) . Google cloud platform setup . Create a project . Go to the google cloud platform console and either choose an existing project or create a new one . . Create a service account . Navigate to the left side menu and proceed to &quot;IAM &amp; Admin&quot; then to &quot;Service Accounts&quot; . . Set a service account name: . . Set the &quot;Role&quot; to &quot;Owner&quot;: . . Confirm that the account was created and click on Actions and then &quot;Manage Keys&quot;: . . Create a JSON private key . . Navigate to API&#39;s and Services and enable the BigQuery API: . . Set the environment variable: . os.environ[&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;]=&quot;C:/Users/gurka/Downloads/bigquery_key.json&quot; . Executing BigQuery Jupyter cells . Loading magic command . The BigQuery client library for Python provides a magic command that lets you run queries with minimal code. To load the magic commands from the client library, paste the following code into the first cell of the notebook. . %load_ext google.cloud.bigquery . Running a test on public data . The BigQuery client library for Python provides a cell magic, %%bigquery, which runs a SQL query and returns the results as a Pandas DataFrame. Enter the following code in the next cell to return total births by year: . %%bigquery SELECT source_year AS year, COUNT(is_male) AS birth_count FROM `bigquery-public-data.samples.natality` GROUP BY year ORDER BY year DESC LIMIT 15 . Query complete after 0.02s: 100%|████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 999.12query/s] Downloading: 100%|███████████████████████████████████████████████████████████████████| 15/15 [00:01&lt;00:00, 9.74rows/s] . year birth_count . 0 2008 | 4255156 | . 1 2007 | 4324008 | . 2 2006 | 4273225 | . 3 2005 | 4145619 | . 4 2004 | 4118907 | . 5 2003 | 4096092 | . 6 2002 | 4027376 | . 7 2001 | 4031531 | . 8 2000 | 4063823 | . 9 1999 | 3963465 | . 10 1998 | 3945192 | . 11 1997 | 3884329 | . 12 1996 | 3894874 | . 13 1995 | 3903012 | . 14 1994 | 3956925 | .",
            "url": "https://www.gurkamal.com//python/gcp/bigquery/sql/2022/05/16/introduction-to-bigquery.html",
            "relUrl": "/python/gcp/bigquery/sql/2022/05/16/introduction-to-bigquery.html",
            "date": " • May 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Accessing the NCBI Entrez database using Biopython",
            "content": "Background . This tutorial uses the NCBI API to interface with Entrez. To get started, the necessary libraries need to be imported and an email needs to be provided (so NCBI can contact you about your query if needed). . email_address = &quot;gurkamal.dev@gmail.com&quot; . The goal will be to find the chloroquine resistance transporter (CRT) gene in the parasite Plasmodium flaciparum in the nucleotide database. . Loading Libraries . from Bio import Entrez, Medline, SeqIO . What is a handle? . A handle is essentially a “wrapper” around text information. . Handles provide two benefits over plain text information: . They provide a standard way to deal with information stored in different ways. The text information can be in a file, or in a string stored in memory, or the output from a command line program, or at some remote website, but the handle provides a common way of dealing with information in all of these formats. . They allow text information to be read incrementally, instead of all at once. This is really important when dealing with huge text files which would use up all of the memory if you had to load them all. . Handles can deal with text information that is being read (reading from a file) or written (writing information to a file). In the case of a “read” handle, commonly used functions are read(), which reads the entire text information from the handle, and readline(), which reads information one line at a time. For “write” handles, the function write() is regularly used. . Retrieving information . To see the available databases: . handle = Entrez.einfo() #Read and store the Entrez query record returned rec = Entrez.read(handle) . {&#39;DbList&#39;: [&#39;pubmed&#39;, &#39;protein&#39;, &#39;nuccore&#39;, &#39;ipg&#39;, &#39;nucleotide&#39;, &#39;structure&#39;, &#39;genome&#39;, &#39;annotinfo&#39;, &#39;assembly&#39;, &#39;bioproject&#39;, &#39;biosample&#39;, &#39;blastdbinfo&#39;, &#39;books&#39;, &#39;cdd&#39;, &#39;clinvar&#39;, &#39;gap&#39;, &#39;gapplus&#39;, &#39;grasp&#39;, &#39;dbvar&#39;, &#39;gene&#39;, &#39;gds&#39;, &#39;geoprofiles&#39;, &#39;homologene&#39;, &#39;medgen&#39;, &#39;mesh&#39;, &#39;ncbisearch&#39;, &#39;nlmcatalog&#39;, &#39;omim&#39;, &#39;orgtrack&#39;, &#39;pmc&#39;, &#39;popset&#39;, &#39;proteinclusters&#39;, &#39;pcassay&#39;, &#39;protfam&#39;, &#39;pccompound&#39;, &#39;pcsubstance&#39;, &#39;seqannot&#39;, &#39;snp&#39;, &#39;sra&#39;, &#39;taxonomy&#39;, &#39;biocollections&#39;, &#39;gtr&#39;]} . Searching for a specific gene . handle = Entrez.esearch(db=&quot;nucleotide&quot;, term=&#39;CRT[Gene Name] AND &quot;Plasmodium falciparum&quot;[Organism]&#39;) #read the result that is returned rec_list = Entrez.read(handle) rec_list . {&#39;Count&#39;: &#39;2022&#39;, &#39;RetMax&#39;: &#39;20&#39;, &#39;RetStart&#39;: &#39;0&#39;, &#39;IdList&#39;: [&#39;2196471109&#39;, &#39;2196471107&#39;, &#39;2196471105&#39;, &#39;2196471103&#39;, &#39;2196471101&#39;, &#39;2196471099&#39;, &#39;2196471097&#39;, &#39;2196471095&#39;, &#39;2196471093&#39;, &#39;2196471091&#39;, &#39;2196471089&#39;, &#39;2196471087&#39;, &#39;2196471085&#39;, &#39;2196471083&#39;, &#39;2196471081&#39;, &#39;2196471079&#39;, &#39;2196471077&#39;, &#39;2196471075&#39;, &#39;2196471073&#39;, &#39;2196471071&#39;], &#39;TranslationSet&#39;: [{&#39;From&#39;: &#39;&#34;Plasmodium falciparum&#34;[Organism]&#39;, &#39;To&#39;: &#39;&#34;Plasmodium falciparum&#34;[Organism]&#39;}], &#39;TranslationStack&#39;: [{&#39;Term&#39;: &#39;CRT[Gene Name]&#39;, &#39;Field&#39;: &#39;Gene Name&#39;, &#39;Count&#39;: &#39;4778&#39;, &#39;Explode&#39;: &#39;N&#39;}, {&#39;Term&#39;: &#39;&#34;Plasmodium falciparum&#34;[Organism]&#39;, &#39;Field&#39;: &#39;Organism&#39;, &#39;Count&#39;: &#39;258609&#39;, &#39;Explode&#39;: &#39;Y&#39;}, &#39;AND&#39;], &#39;QueryTranslation&#39;: &#39;CRT[Gene Name] AND &#34;Plasmodium falciparum&#34;[Organism]&#39;} . Returning all records . The standard search will limit the number of record references to 20, so if you have more, you may want to repeat the query with an increased maximum limit. In this case, we will actually override the default limit with retmax. The Entrez system provides quite a few sophisticated ways to retrieve large number of results. . Be careful with this technique, because you will retrieve a large amount of complete records, and some of them will have fairly large sequences inside. . if rec_list[&#39;RetMax&#39;] &lt; rec_list[&#39;Count&#39;]: handle = Entrez.esearch(db=&quot;nucleotide&quot;, term=&#39;CRT[Gene Name] AND &quot;Plasmodium falciparum&quot;[Organism]&#39;, retmax=rec_list[&#39;Count&#39;]) rec_list = Entrez.read(handle) . Downloading nucleotide sequences . Now that we have the IDs of all of the records, you still need to retrieve the records properly. . This will retrieve a list of records in the GenBank format (including sequences and metadata) . id_list = rec_list[&#39;IdList&#39;] handle_2 = Entrez.efetch(db=&#39;nucleotide&#39;, id=id_list, rettype=&#39;gb&#39;, retmax=rec_list[&#39;Count&#39;]) . The result of SeqIO.parse is an iterator and is converted to a list. The advantage of doing this is that we can use the result as many times as we want(for example, iterate many times over), without repeating the query on the server.This saves time, bandwidth, and server usage if you plan to iterate many timesover. . The disadvantage is that it will allocate memory for all records. This will not work for very large datasets . recs = list(SeqIO.parse(handle_2, &#39;gb&#39;)) . Reading a record . The rec variable now contains the record of interest. The rec.description will contain its human-readable description. . for rec in recs: if rec.name == &#39;KM288867&#39;: break print(rec.name) print(rec.description) . KM288867 Plasmodium falciparum clone PF3D7_0709000 chloroquine resistance transporter (CRT) gene, complete cds . Extracting sequences features . Extract sequence features which contain information such as gene products and exon positions on the sequence: . for feature in rec.features: if feature.type == &#39;gene&#39;: # gene name will be in the feature.qualifiers dictionary. print(feature.qualifiers[&#39;gene&#39;]) # print the start, end, and originating strand of the exon elif feature.type == &#39;exon&#39;: loc = feature.location print(&#39;Exon&#39;, loc.start, loc.end, loc.strand) else: print(&#39;not processed: n%s&#39; % feature) . not processed: type: source location: [0:10000](+) qualifiers: Key: clone, Value: [&#39;PF3D7_0709000&#39;] Key: db_xref, Value: [&#39;taxon:5833&#39;] Key: mol_type, Value: [&#39;genomic DNA&#39;] Key: organism, Value: [&#39;Plasmodium falciparum&#39;] [&#39;CRT&#39;] not processed: type: mRNA location: join{[2751:3543](+), [3720:3989](+), [4168:4341](+), [4513:4646](+), [4799:4871](+), [4994:5070](+), [5166:5249](+), [5376:5427](+), [5564:5621](+), [5769:5862](+), [6055:6100](+), [6247:6302](+), [6471:7598](+)} qualifiers: Key: gene, Value: [&#39;CRT&#39;] Key: product, Value: [&#39;chloroquine resistance transporter&#39;] not processed: type: 5&#39;UTR location: [2751:3452](+) qualifiers: Key: gene, Value: [&#39;CRT&#39;] not processed: type: primer_bind location: [2935:2958](+) qualifiers: not processed: type: primer_bind location: [3094:3121](+) qualifiers: not processed: type: CDS location: join{[3452:3543](+), [3720:3989](+), [4168:4341](+), [4513:4646](+), [4799:4871](+), [4994:5070](+), [5166:5249](+), [5376:5427](+), [5564:5621](+), [5769:5862](+), [6055:6100](+), [6247:6302](+), [6471:6548](+)} qualifiers: Key: codon_start, Value: [&#39;1&#39;] Key: gene, Value: [&#39;CRT&#39;] Key: product, Value: [&#39;chloroquine resistance transporter&#39;] Key: protein_id, Value: [&#39;AIW62921.1&#39;] Key: translation, Value: [&#39;MKFASKKNNQKNSSKNDERYRELDNLVQEGNGSRLGGGSCLGKCAHVFKLIFKEIKDNIFIYILSIIYLSVCVMNKIFAKRTLNKIGNYSFVTSETHNFICMIMFFIVYSLFGNKKGNSKERHRSFNLQFFAISMLDACSVILAFIGLTRTTGNIQSFVLQLSIPINMFFCFLILRYRYHLYNYLGAVIIVVTIALVEMKLSFETQEENSIIFNLVLISALIPVCFSNMTREIVFKKYKIDILRLNAMVSFFQLFTSCLILPVYTLPFLKQLHLPYNEIWTNIKNGFACLFLGRNTVVENCGLGMAKLCDDCDGAWKTFALFSFFNICDNLITSYIIDKFSTMTYTIVSCIQGPAIAIAYYFKFLAGDVVREPRLLDFVTLFGYLFGSIIYRVGNIILERKKMRNEENEDSEGELTNVDSIITQ&#39;] Exon 3452 3543 1 Exon 3720 3989 1 Exon 4168 4341 1 not processed: type: primer_bind location: [4288:4323](-) qualifiers: Exon 4513 4646 1 Exon 4799 4871 1 Exon 4994 5070 1 Exon 5166 5249 1 Exon 5376 5427 1 Exon 5564 5621 1 Exon 5769 5862 1 Exon 6055 6100 1 Exon 6247 6302 1 Exon 6471 6548 1 not processed: type: 3&#39;UTR location: [6548:7598](+) qualifiers: Key: gene, Value: [&#39;CRT&#39;] not processed: type: primer_bind location: [7833:7856](-) qualifiers: . Record annotations . We will now look at the annotations on the record, which are mostly metadata that is not related to the sequence position. Note that some values are not strings; they can be numbers or even lists (for example, the taxonomy annotation is a list) . for name, value in rec.annotations.items(): print(&#39;%s=%s&#39; % (name, value)) . molecule_type=DNA topology=linear data_file_division=INV date=12-NOV-2014 accessions=[&#39;KM288867&#39;] sequence_version=1 keywords=[&#39;&#39;] source=Plasmodium falciparum (malaria parasite P. falciparum) organism=Plasmodium falciparum taxonomy=[&#39;Eukaryota&#39;, &#39;Sar&#39;, &#39;Alveolata&#39;, &#39;Apicomplexa&#39;, &#39;Aconoidasida&#39;, &#39;Haemosporida&#39;, &#39;Plasmodiidae&#39;, &#39;Plasmodium&#39;, &#39;Plasmodium (Laverania)&#39;] references=[Reference(title=&#39;Versatile control of Plasmodium falciparum gene expression with an inducible protein-RNA interaction&#39;, ...), Reference(title=&#39;Direct Submission&#39;, ...)] . Accessing the sequence data . Last but not least, you can access the fundamental piece of information, the sequence . print(rec.seq[0:100]) . ATATGTAAAACCAAAATAAATTAAACAGAATTTATTTTTAAAAGATTTATTTGTAACAATATTACCATGATGATTTATTAAAGTAAAATCACCACCTATT .",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/genetics/2022/05/14/accessing-genbank.html",
            "relUrl": "/python/bioinformatics/datasets/genetics/2022/05/14/accessing-genbank.html",
            "date": " • May 14, 2022"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "Scaffold splitting and initial model training using chemprop",
            "content": "Chemprop . I found the chemprop library in the paper A Deep Learning Approach to Antibiotic Discovery30102-1&quot;), it uses the directed message passing neural network (D-MPNN) described in Analyzing Learned Molecular Representations for Property Prediction. . The drug discovery workflow described in the paper uses a training dataset of 2335 molecules with binary e.coli growth inhibition labels to train a classification model. . The D-MPNN architecture as described in the paper: . The D-MPNN architecture translates the graph representation of a molecule into a continuous vector via a directed bond-based message-passing approach. This builds a molecular representation by iteratively aggregating the features of individual atoms and bonds. The model operates by passing “messages” along bonds that encode information about neighboring atoms and bonds. By applying this message passing operation multiple times, the model constructs higher-level bond messages that contain information about larger chemical substructures. The highest-level bond messages are then combined into a single continuous vector representing the entire molecule. . The researchers generated molecular features using RDKit to tackle overfitting, increased algorithm robustness by using an ensemble of classifiers, and optimized hyperparameters with bayesian optimization. The model achieved a ROC-AUC of 0.896 on test data. . This notebook will apply the neural network to the dataset of blood-brain barrier molecules gathered in the previous notebooks. . Scaffold splitting . A distributional shift is a change in the data distribution between an algorithm&#39;s training dataset, and a dataset it encounters when deployed. Distributional shifts are common in the field of molecular property prediction where chemical space is huge and different areas of it exhibit high structural heterogeneity. The shifts tend to be large and difficult to handle for machine learning models. . Scaffold splitting is one solution to this distributional shift. It&#39;s first described in &quot;The properties of known drugs. 1. Molecular frameworks: . A molecular scaffold reduces the chemical structure of a compound to its core components, essentially by removing all side chains and only keeping ring systems and parts that link together ring systems. An additional option for making molecular scaffolds even more general is to “forget” the identities of the bonds and atoms by replacing all atoms with carbons and all bonds with single bonds. . A Murcko scaffold essentially extracts the molecular backbone by extracting the ring structures and the linkers that connect them. These scaffolds collapse molecules into bins based on their generated Murcko scaffold, reducing the number of highly similar structures. When splitting the data by scaffold, molecules sharing a scaffold are in the same split, meaning no similar molecules will be found across different splits. Any bins larger than half of the desired test set size are placed into the training set, to guarantee the scaffold diversity of the validation and test sets. All remaining bins are placed randomly into the training, validation, and test sets until each set has reached its desired size. Clustering molecules by scaffold ensures that the model will be trained on structurally diverse folds when cross-validating. . Compared to a random split, a scaffold split is a more challenging and realistic evaluation setting as it more closely approximates the split present in real-world property prediction data. . A molecule compared to its scaffold below: . Molecular compound and its generic Bemis-Murcko scaffold . from rdkit import Chem from rdkit.Chem.Scaffolds import MurckoScaffold # define compound via its SMILES string smiles = &quot;CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC&quot; # convert SMILES string to RDKit mol object mol = Chem.MolFromSmiles(smiles) # create RDKit mol object corresponding to Bemis-Murcko scaffold of original compound mol_scaffold = MurckoScaffold.GetScaffoldForMol(mol) # make the scaffold generic by replacing all atoms with carbons and all bonds with single bonds mol_scaffold_generic = MurckoScaffold.MakeScaffoldGeneric(mol_scaffold) # convert the generic scaffold mol object back to a SMILES string format smiles_scaffold_generic = Chem.CanonSmiles(Chem.MolToSmiles(mol_scaffold_generic)) # display compound and its generic Bemis-Murcko scaffold display(mol) print(smiles) display(mol_scaffold_generic) print(smiles_scaffold_generic) . CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC . C1CCC(CCC2C3CCCCC3CC3CCCCC32)CC1 . Generating generic Bemis-Murcko scaffolds . Below I&#39;ve written code to generate the scaffolds and a few other related molecular representations. But binning and writing the code to split them myself would take too much time so I use a built-in feature of chemprop to split the data before model training. . # import packages import chemprop import pandas as pd import datamol as dm pd.options.mode.chained_assignment = None # default=&#39;warn&#39; # loading the datasets MolNet = pd.read_csv(&quot;data/MoleculeNet.csv&quot;) B3DB = pd.read_csv(&quot;data/B3DB.csv&quot;) b3_molecules = pd.read_csv(&quot;data/b3_molecules.csv&quot;) . scaffold_split function . This function generates and sanitizes mols, generates a scaffold from the mol object, generalizes the scaffold, and lastly converts the scaffold to a SMILES string, returning a dataframe. . def scaffold_split(df): df[&quot;mol&quot;] = [Chem.MolFromSmiles(x) for x in df[&quot;standard_smiles&quot;]] # generating mols from the standard_smiles column df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=True) for x in df[&#39;mol&#39;]] # sanitize mol objects df = df.dropna() # dropping NA values df[&quot;scaffold&quot;] = [MurckoScaffold.GetScaffoldForMol(x) for x in df[&quot;mol&quot;]] # generating scaffolds from mol object df[&quot;mol_scaffold_generic&quot;] = [MurckoScaffold.MakeScaffoldGeneric(x) for x in df[&quot;scaffold&quot;]] # generalizing scaffolds # convert the generic scaffold mol object back to a SMILES string format df[&quot;smiles_scaffold_generic&quot;] = [Chem.CanonSmiles(Chem.MolToSmiles(x)) for x in df[&quot;mol_scaffold_generic&quot;]] return df . # the data prior to processing MolNet.head(1) . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | . # results of the scaffold generating function data_split = scaffold_split(b3_molecules) data_split.head(1) . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey scaffold mol_scaffold_generic smiles_scaffold_generic . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | C1CCC2CCCCC2C1 | . chemprop_prep function . This function serves to prepare the data to be used by the chemprop library. Specifically, the input data must contain a SMILES string in the first column and the target value - a binary value in this experiment. The resulting dataframe is saved as a CSV file to later be used by chemprop. . # A function to process the data for use with the chemprop library def chemprop_prep(df, filename): df = df.drop([&quot;mol&quot;, &quot;SMILES&quot;, &quot;selfies&quot;, &quot;inchi&quot;, &quot;inchikey&quot;], axis=1) # drop all columns except the smiles and target df[&quot;smiles&quot;] = df[&quot;standard_smiles&quot;] # use standard smiles inplace of smiles df = df.drop([&quot;standard_smiles&quot;], axis=1) # drop this column now df = df[[&quot;smiles&quot;, &quot;BBB+/BBB-&quot;]] # reorder the columns with smiles first and target second df.to_csv(&#39;./data/&#39; + filename + &#39;.csv&#39;, index=False) # save the file return df . # Processing the three different datasets molnet_chemprop = chemprop_prep(MolNet, &#39;molnet_chemprop&#39;) B3DB_chemprop = chemprop_prep(B3DB, &#39;B3DB_chemprep&#39;) b3_mol_chemprop = chemprop_prep(b3_molecules, &#39;b3_mol_chemprop&#39;) . # results of processing molnet_chemprop.head(1) . smiles BBB+/BBB- . 0 CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | 1 | . Model training with chemprop . I ran three experiments with slight differences to test out the predictive ability on the blood-brain barrier molecules: . The split type is set to scaffold balanced; the number of folds is 10, class balancing is set to true, and 200 features are generated with RDKits 2d descriptor generator. The features are scaled when calculated and thus no further scaling is specified. . | This run sets the number of training epochs within each fold at 100. The rest of the settings mentioned above are kept the same. . | The last run has the number of folds increased from 10 to 30 while epochs per fold are set to 20. . | These models are trained using the MoleculeNet data (~2035 molecules), further tests will use the combined dataset with ~8100 molecules. . Model training 1 . arguments = [ &#39;--data_path&#39;, &#39;./data/molnet_chemprop.csv&#39;, &#39;--dataset_type&#39;, &#39;classification&#39;, &#39;--save_dir&#39;, &#39;./data/chemprop_checkpoints/&#39;, &#39;--split_type&#39;, &#39;scaffold_balanced&#39;, # &#39;--separate_val_path&#39;, &#39;./data/chemprop_B3DB.csv&#39;, &#39;--num_folds&#39;, &#39;10&#39;, &#39;--class_balance&#39;, &#39;--features_generator&#39;, &#39;rdkit_2d_normalized&#39;, &#39;--no_features_scaling&#39;, &#39;--quiet&#39; ] args = chemprop.args.TrainArgs().parse_args(arguments) mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training) . Model training 2 . arguments = [ &#39;--data_path&#39;, &#39;./data/molnet_chemprop.csv&#39;, &#39;--dataset_type&#39;, &#39;classification&#39;, &#39;--save_dir&#39;, &#39;./data/chemprop_checkpoints/&#39;, &#39;--split_type&#39;, &#39;scaffold_balanced&#39;, # &#39;--separate_val_path&#39;, &#39;./data/chemprop_B3DB.csv&#39;, &#39;--num_folds&#39;, &#39;10&#39;, &#39;--class_balance&#39;, &#39;--features_generator&#39;, &#39;rdkit_2d_normalized&#39;, &#39;--no_features_scaling&#39;, &#39;--quiet&#39;, &#39;--epochs&#39;, &#39;100&#39; ] args = chemprop.args.TrainArgs().parse_args(arguments) mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training) . Model training 3 . arguments = [ &#39;--data_path&#39;, &#39;./data/molnet_chemprop.csv&#39;, &#39;--dataset_type&#39;, &#39;classification&#39;, &#39;--save_dir&#39;, &#39;./data/chemprop_checkpoints/&#39;, &#39;--split_type&#39;, &#39;scaffold_balanced&#39;, # &#39;--separate_val_path&#39;, &#39;./data/chemprop_B3DB.csv&#39;, &#39;--num_folds&#39;, &#39;30&#39;, &#39;--class_balance&#39;, &#39;--features_generator&#39;, &#39;rdkit_2d_normalized&#39;, &#39;--no_features_scaling&#39;, &#39;--quiet&#39;, &#39;--epochs&#39;, &#39;20&#39;, ] args = chemprop.args.TrainArgs().parse_args(arguments) mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training) . Results . Experiment 1: . 10-fold cross validation . Seed 0 ==&gt; test auc = 0.928767 Seed 1 ==&gt; test auc = 0.903879 Seed 2 ==&gt; test auc = 0.955724 Seed 3 ==&gt; test auc = 0.862450 Seed 4 ==&gt; test auc = 0.890246 Seed 5 ==&gt; test auc = 0.892326 Seed 6 ==&gt; test auc = 0.886606 Seed 7 ==&gt; test auc = 0.951849 Seed 8 ==&gt; test auc = 0.906486 Seed 9 ==&gt; test auc = 0.937866 . Overall test auc = 0.911620 +/- 0.029164 Elapsed time = 0:20:54 . This run was the fastest and used the default number of epochs and fold. The overall AUC for the test data was 0.911620 +/- 0.029164. . Experiment 2: . 10-fold cross validation . Seed 0 ==&gt; test auc = 0.929177 Seed 1 ==&gt; test auc = 0.852848 Seed 2 ==&gt; test auc = 0.944112 Seed 3 ==&gt; test auc = 0.862913 Seed 4 ==&gt; test auc = 0.890246 Seed 5 ==&gt; test auc = 0.893487 Seed 6 ==&gt; test auc = 0.881697 Seed 7 ==&gt; test auc = 0.941933 Seed 8 ==&gt; test auc = 0.920437 Seed 9 ==&gt; test auc = 0.917494 . Overall test auc = 0.903435 +/- 0.030385 Elapsed time = 16:49:39 . This run took the most time, nearing almost 17 hours with 100 epochs for each fold. The overall AUC for the test data was 0.903435 +/- 0.030385 which underperformed compared to the model in the first experiment that ran using default parameters. . Experiment 3: . 30-fold cross validation . Seed 0 ==&gt; test auc = 0.928630 Seed 1 ==&gt; test auc = 0.907636 Seed 2 ==&gt; test auc = 0.943526 Seed 3 ==&gt; test auc = 0.882607 Seed 4 ==&gt; test auc = 0.890246 Seed 5 ==&gt; test auc = 0.892559 Seed 6 ==&gt; test auc = 0.870606 Seed 7 ==&gt; test auc = 0.947479 Seed 8 ==&gt; test auc = 0.903720 Seed 9 ==&gt; test auc = 0.930991 Seed 10 ==&gt; test auc = 0.795291 Seed 11 ==&gt; test auc = 0.894284 Seed 12 ==&gt; test auc = 0.936275 Seed 13 ==&gt; test auc = 0.930272 Seed 14 ==&gt; test auc = 0.950706 Seed 15 ==&gt; test auc = 0.919849 Seed 16 ==&gt; test auc = 0.877632 Seed 17 ==&gt; test auc = 0.871467 Seed 18 ==&gt; test auc = 0.888110 Seed 19 ==&gt; test auc = 0.905920 Seed 20 ==&gt; test auc = 0.921946 Seed 21 ==&gt; test auc = 0.928851 Seed 22 ==&gt; test auc = 0.870864 Seed 23 ==&gt; test auc = 0.865779 Seed 24 ==&gt; test auc = 0.902960 Seed 25 ==&gt; test auc = 0.946100 Seed 26 ==&gt; test auc = 0.900540 Seed 27 ==&gt; test auc = 0.923984 Seed 28 ==&gt; test auc = 0.900345 Seed 29 ==&gt; test auc = 0.905261 . Overall test auc = 0.904481 +/- 0.031870 Elapsed time = 0:38:51 . This run took less than an hour (~40 minutes), with 20 training epochs per fold. Increasing the number of folds and epochs seems to bring down the model&#39;s accuracy compared to the default settings. The overall AUC for the test data was AUC= 0.904481 +/- 0.031870, which still marginally outperformed the model in experiment 2 that took nearly 17 hours. .",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/12/data-splitting-and-training.ipynb.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/12/data-splitting-and-training.ipynb.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Processing the B3DB brain-blood barrier dataset",
            "content": "Gathering data on blood brain barrier permeability . This post is a part of a fullstack machine learning web app project and this notebook contains the data needed to build and train the models. The goal in this post is to clean and preprocess the B3DB dataset and merge it with the blood brain permeability data from MoleculeNet. The merged dataset should contain nearly 10,000 molecules labeled with their ability to pass through the blood brain barrier. The notebook provided in the B3DB repository also contains an interesting PCA plot, which is a good starting place for EDA when the data is merged. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from rdkit import Chem from rdkit.Chem import AllChem from sklearn.decomposition import PCA import datamol as dm %matplotlib inline . Removing, remapping, and creating features . This dataset has features that can be dropped since they won&#39;t contribute to model training. Some features are then calculated from the SMILES data. . # reading in the data bbb_df = pd.read_csv(&quot;data/B3DB_classification.tsv&quot;, sep=&quot; t&quot;) # dropping columns bbb_df = bbb_df.drop([&quot;CID&quot;, &quot;logBB&quot;, &quot;Inchi&quot;, &quot;threshold&quot;, &quot;reference&quot;, &quot;group&quot;, &quot;comments&quot;, &quot;NO.&quot;, &quot;IUPAC_name&quot;, &quot;compound_name&quot;], axis=1) # mapping given labels to binary bbb_df[&#39;BBB+/BBB-&#39;] = bbb_df[&#39;BBB+/BBB-&#39;].map({&#39;BBB+&#39;: 1, &#39;BBB-&#39;: 0}) . Feature generation . The function below processes and generates features such as mol objects, selfies, inchi, and inchikeys for each molecule using the datamol library. . # preprocessing function for molecules def preprocess_smiles(df): df[&quot;mol&quot;] = [dm.to_mol(x) for x in df[&#39;SMILES&#39;]] # generating mols from SMILES df[&quot;mol&quot;] = [dm.fix_mol(x) for x in df[&#39;mol&#39;]] # Fixing mols df = df.dropna() # dropping NA values df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=False) for x in df[&#39;mol&#39;]] # sanitize mol objects df[&quot;mol&quot;] = [dm.standardize_mol(x, disconnect_metals=False, normalize=True, reionize=True, uncharge=False, stereo=True) for x in df[&#39;mol&#39;]] # standardize mol objects df[&quot;standard_smiles&quot;] = [dm.standardize_smiles(x) for x in df[&#39;SMILES&#39;]] # standardize SMILES df[&quot;selfies&quot;] = [dm.to_selfies(x) for x in df[&#39;mol&#39;]] # generate SELFIES df[&quot;inchi&quot;] = [dm.to_inchi(x) for x in df[&#39;mol&#39;]] # Generating InChi df[&quot;inchikey&quot;] = [dm.to_inchikey(x) for x in df[&#39;mol&#39;]] # Generating InChIKey return df data_clean = preprocess_smiles(bbb_df) # Making a copy of the dataframe for later B3DB = data_clean data_clean.head() # Saving the data as B3DB; after where its found data_clean.to_csv(&#39;./data/B3DB.csv&#39;, index=False) . Merging the two datasets . The datasets would have ideally added up to 9846 molecules but that doesnt account for duplicates. Since the B3DB consists of data from across 50 studies, I assumed most of the molecules from MoleculeNet would appear in B3DB. . Counting the unique inchikey values confirmed this, leaving the final dataset with 8091 molecules. This number is lower than the sum of both dataframes but it adds 284 novel compounds to the final dataset. . # Loading the MoleculeNet dataset MolNet = pd.read_csv(&quot;data/MoleculeNet.csv&quot;) # concatenating both dataframes final_df = pd.concat([MolNet, B3DB]) . # Number of molecules before filtering for duplicates final_df.shape . (9846, 7) . # inchikey values should be unique to each molecule final_df.inchikey.value_counts() . FXHJGPDCPMCUKW-UHFFFAOYSA-N 4 UHSKFQJFRQCDBE-UHFFFAOYSA-N 4 CSIVCTHRYRVJCI-UHFFFAOYSA-N 4 UUQMNUMQCIQDMZ-UHFFFAOYSA-N 4 XHMYQXZLVLGNKX-UHFFFAOYSA-N 3 .. XYGVIBXOJOOCFR-BTJKTKAUSA-N 1 NINYZUDVKTUKIA-UHFFFAOYSA-N 1 BVCKFLJARNKCSS-ZJLJEUSSSA-N 1 HOCWPKXKMNXINF-CJIHYQBJSA-N 1 UWHAHBDBSBVMIY-VUXXLBMGSA-N 1 Name: inchikey, Length: 8091, dtype: int64 . # Dropping duplicates based on unique inchikey values final_df = final_df.drop_duplicates(subset=&#39;inchikey&#39;, keep=&quot;first&quot;) # Saving the final dataframe as b3_molecules final_df.to_csv(&#39;./data/b3_molecules.csv&#39;, index=False) . PCA analysis . PCA does not discard any variables and instead it reduces the number of dimensions by constructing principal components. Principal components describe variation and account for the varied influences of the original features. . Each SMILES string will have a morgan fingerprint generated. These fingerprints are binary 2048 bit arrays which describe molecular structure. . Three plots were made, the first using the MolNet dataset, the B3DB data next, and the combined data last. . # MolNet # compute ECFP6 Morgan fingerprints with radius 3 fps_molnet = [] for idx, row in MolNet.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_molnet.append(fp.ToBitString()) # Computing ECFP6 fingerprints for B3DB fps_B3DB = [] for idx, row in B3DB.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_B3DB.append(fp.ToBitString()) # Computing ECFP6 fingerprints for B3DB fps_final = [] for idx, row in final_df.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_final.append(fp.ToBitString()) # Create a numpy array and use the u1 datatype (uint8 8-bit unsigned integer) fps_arr_molnet = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_molnet]) fps_arr_B3DB = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_B3DB]) fps_arr_final = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_final]) # PCA on MolNet molecules molnet_fps = pd.DataFrame(fps_arr_molnet, index=MolNet.index) molnet_fps = pd.concat([MolNet, molnet_fps], axis=1) pca_molnet = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_molnet) molnet_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] molnet_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] # PCA on B3DB molecules B3DB_fps = pd.DataFrame(fps_arr_B3DB, index=B3DB.index) B3DB_fps = pd.concat([B3DB, B3DB_fps], axis=1) pca_B3DB = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_B3DB) B3DB_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] B3DB_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] # PCA on final set of molecules final_fps = pd.DataFrame(fps_arr_final, index=final_df.index) final_fps = pd.concat([final_df, final_fps], axis=1) pca_final = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_final) final_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] final_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] . PCA Visualizations . PCA of molecules in the MoleculeNet dataset . fig_molnet = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=molnet_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the MolecularNet dataset&#39;) plt.show() . PCA of molecules in the B3DB dataset . fig_B3DB = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=B3DB_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the B3DB dataset&#39;) plt.show() . PCA of molecules in the combined and filtered dataset . fig_final = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=final_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the combined and filtered dataset&#39;) plt.show() .",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/09/using-the-B3DB-dataset.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/09/using-the-B3DB-dataset.html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Preprocessing a dataset of blood brain barrier molecules with Datamol",
            "content": "SMILES (Simplified Molecular Input Line Entry System) is a standard notation representing the molecular structure of a compound as a string representation that can be understood by a computer. The SMILES notation consists of a handful of rules which allow for converting the string to an image or graph. SMILES can then be easily used for generating further representations to train machine learning models with. . import datamol as dm import pandas as pd pd.options.mode.chained_assignment = None # default=&#39;warn&#39; . BBBP_df = pd.read_csv(&quot;data/BBBP.csv&quot;) BBBP_df.head() . num name p_np smiles . 0 1 | Propanolol | 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | . 1 2 | Terbutylchlorambucil | 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | . 2 3 | 40730 | 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | . 3 4 | 24 | 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | . 4 5 | cloxacillin | 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | . The dataframe shows 4 named columns, including the &quot;num&quot; of the molecule, the name, a binary label for blood brain barrier permeability status &quot;p_np&quot;, and the SMILES string. . # The name and number can be dropped BBBP_df = BBBP_df.drop([&quot;num&quot;, &quot;name&quot;], axis=1) # Checking the data for null values BBBP_df[&quot;smiles&quot;].isnull().values.any() # Renaming the binary label to &quot;BBB+/BBB-&quot; for clarity BBBP_df.columns = [&#39;BBB+/BBB-&#39;, &#39;SMILES&#39;] . BBBP_df . BBB+/BBB- SMILES . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | . 1 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | . 2 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | . 3 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | . 4 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | . ... ... | ... | . 2045 1 | C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl | . 2046 1 | [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](... | . 2047 1 | [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=... | . 2048 1 | C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC... | . 2049 1 | [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]... | . 2050 rows × 2 columns . Mols and smiles need to be sanitized as it will leave us with SMILES that are complete nonesense, for example, errors resulting from kekulization. . . RDkit generates the alternate position of double bonds, and then (in a second step they call &quot;aromatization&quot;) labels the ring as aromatic. In panel (2), there are three possible Lewis structures contributing to the actual structure (i.e. there is resonance), so the software would have to generate all three to be able to search for identical structures. [1] . Below is a function using datamol to preprocess the dataset, including steps to generate mol objects, SELFIES, inchi, and inchikeys for each molecule. The function also standardizes mols and SMILES, drops NA values, and returns a dataframe. . def preprocess_smiles(df): df[&quot;mol&quot;] = [dm.to_mol(x) for x in df[&#39;SMILES&#39;]] # generating mols from SMILES df[&quot;mol&quot;] = [dm.fix_mol(x) for x in df[&#39;mol&#39;]] # Fixing mols df = df.dropna() # dropping NA values df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=False) for x in df[&#39;mol&#39;]] # sanitize mol objects df[&quot;mol&quot;] = [dm.standardize_mol(x, disconnect_metals=False, normalize=True, reionize=True, uncharge=False, stereo=True) for x in df[&#39;mol&#39;]] # standardize mol objects df[&quot;standard_smiles&quot;] = [dm.standardize_smiles(x) for x in df[&#39;SMILES&#39;]] # standardize SMILES df[&quot;selfies&quot;] = [dm.to_selfies(x) for x in df[&#39;mol&#39;]] # generate SELFIES df[&quot;inchi&quot;] = [dm.to_inchi(x) for x in df[&#39;mol&#39;]] # Generating InChi df[&quot;inchikey&quot;] = [dm.to_inchikey(x) for x in df[&#39;mol&#39;]] # Generating InChIKey return df . Running the function and taking a look at the outputs . data_clean = preprocess_smiles(BBBP_df) . data_clean.shape . (2039, 7) . data_clean.head() . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | . 1 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)(C)OC(=O)CCCc1ccc(N(CCCl)CCCl)cc1 | [C][C][Branch1][C][C][Branch1][C][C][O][C][=Br... | InChI=1S/C18H27Cl2NO2/c1-18(2,3)23-17(22)6-4-5... | SZXDOYFHSIIZCF-UHFFFAOYSA-N | . 2 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC1COc2c(N3CCN(C)CC3)c(F)cc3c(=O)c(C(=O)O)cn1c23 | [C][C][C][O][C][=C][Branch1][N][N][C][C][N][Br... | InChI=1S/C18H20FN3O4/c1-10-9-26-17-14-11(16(23... | GSDSWSVVBLHKDQ-UHFFFAOYSA-N | . 3 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(=O)NCCCOc1cccc(CN2CCCCC2)c1 | [C][C][=Branch1][C][=O][N][C][C][C][O][C][=C][... | InChI=1S/C17H26N2O2/c1-15(20)18-9-6-12-21-17-8... | FAXLXLJWHQJMPK-UHFFFAOYSA-N | . 4 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | Cc1onc(-c2ccccc2Cl)c1C(=O)N[C@@H]1C(=O)N2[C@@H... | [C][C][O][N][=C][Branch1][#Branch2][C][=C][C][... | InChI=1S/C19H18ClN3O5S/c1-8-11(12(22-28-8)9-6-... | LQOLIRLGBULYKD-JKIFEVAISA-N | . The data contains a 3:1 ratio of positive to negative labels, which creates a bias towards molecules with blood brain permeability properties. This may need to be addressed when training models. The next steps are to save the cleaned data for further analysis. . counts = data_clean[&#39;BBB+/BBB-&#39;].value_counts().to_dict() print(counts) . {1: 1560, 0: 479} . data_clean.to_csv(&#39;./data/MoleculeNet.csv&#39;, index=False) . References . Urbaczek, Sascha. A consistent cheminformatics framework for automated virtual screening. Ph.D. Thesis, Universität Hamburg, August 2014. URL: http://ediss.sub.uni-hamburg.de/volltexte/2015/7349/; URN: urn:nbn:de:gbv:18-73491; PDF via Semantic Scholar |",
            "url": "https://www.gurkamal.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/10/22/working-with-SMILES.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/10/22/working-with-SMILES.html",
            "date": " • Oct 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Writing a function in python to perform a restriction enzyme digest",
            "content": "Creating a restriction enzyme dictionary . Restriction enzymes are proteins produced by bacteria that cleave DNA at specific sites along the molecule. The enzyme functions on a specific, short nucleotide sequence and cuts the DNA only at that specific site, which is known as restriction site or target sequence. In the bacterial cell, restriction enzymes cleave foreign DNA, thus eliminating infecting organisms. The activity of a restriction enzyme can be defined by its recognition site on the DNA sequence and the position relative to the recognition site, at which it cuts the DNA. . # create enzyme dictionary restrictionEnzymes = {} # add &quot;bamH1&quot; and &quot;sma1&quot; enzymes, their target sequence, and position releative to the recognition site restrictionEnzymes[&#39;bamH1&#39;] = [&#39;ggatcc&#39;,0] restrictionEnzymes[&#39;sma1&#39;] = [&#39;cccggg&#39;,2] # a function to calculate the molecular weight of dna sequences def oligoMolecularWeight(sequence): # create a dictionairy of DNA basepair molecular weights dnaMolecularWeight = {&#39;a&#39;:313.2,&#39;c&#39;:289.2,&#39;t&#39;:304.2,&#39;g&#39;:329.2} # initialize molecular weight molecularWeight = 0.0 # iterate through DNA sequnce and update weight of sequence for base in sequence: molecularWeight += dnaMolecularWeight[base] return molecularWeight # the primary function for restriction digest def digest(sequence, enzyme): # set target sequence target = restrictionEnzymes[enzyme][0] # enzyme cut position relative to recognition site cutPosition = restrictionEnzymes[enzyme][1] # a list to collect DNA fragments fragments = [] # counter for the position of the last restriction site; beginning of sequence found = 0 # a variable to store the position of the last cut; end of sequence lastCut = found # variable to set where to search for the next site from searchFrom = lastCut while found != -1: found = sequence.find(target, searchFrom) if found != -1: fragment = sequence[lastCut:found+cutPosition] mwt = oligoMolecularWeight(fragment) fragments.append((fragment,mwt)) else: fragment = sequence[lastCut:] mwt = oligoMolecularWeight(fragment) fragments.append((fragment,mwt)) lastCut = found + cutPosition searchFrom = lastCut + 1 return fragments . Running the function on a test sequence results in the following: . digestSequence = &quot;gcgatgctaggatccgcgatcgcgtacgatcgtacgcggtacggacggatccttctc&quot; . digested_dna = digest(digestSequence,&#39;bamH1&#39;) . print(digested_dna) . [(&#39;gcgatgcta&#39;, 2800.7999999999997), (&#39;ggatccgcgatcgcgtacgatcgtacgcggtacggac&#39;, 11478.400000000005), (&#39;ggatccttctc&#39;, 3345.1999999999994)] .",
            "url": "https://www.gurkamal.com//python/bioinformatics/restriction%20enzyme/function/2021/10/18/restriction-enzyme-digest.html",
            "relUrl": "/python/bioinformatics/restriction%20enzyme/function/2021/10/18/restriction-enzyme-digest.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "A primer on Apache Airflow",
            "content": "What is airflow? . Airflow is a platform used to author, schedule, and monitor workflows. It’s essentially a queuing system that runs on a metadata database and a scheduler that runs tasks. Workflows are written as Directed Acyclic Graphs (DAGs). A workflow and DAG are interchangeable. . What are DAGs? . A DAG is a collection of tasks you want to run and are organized in a way that illustrates dependencies and relationships between tasks. . The image below shows how a DAG is a unidirectional, acyclic graph, where each node in the graph is a task and edges define dependencies among tasks. There is no case where you should be able to go backwards from a forward node to one that&#39;s already been executed. . . A DAG can be broken up into smaller and smaller jobs and gives the user full control by generating dynamic pipelines written in code. Airflow DAGs are also extensible and can scale. DAGs are powerful because they allow for collaborative, manageable, and testable workflows. A bonus is that Airflow is developed in python and can interface with any python API. . . The image above shows how Airflow divides the tasks into branches so that if one fails, there is still output from the other. Also, the processing time is reduced as parallel computing occurs. The chances of failure should decrease overall as each task is independent. . How are tasks executed? . An operator represents a single task in a workflow that helps carry out your task (running a python function for example). Operators determine what actually gets to be done when your dag runs. A task is an operator when instantiated. It is something on which the worker works upon. . Airflow Architecture . Metadata — is a relational database with info on task state, such as the top ten tasks consuming the most memory, it contains all data pertaining to jobs currently running as well as historical data. . | Scheduler — decides which task to run, when, and in what order. . | Web server— the UI which is essentially a flask app that talks to the metadata. . | Executor — performs the task at ground level. The executor is a message queuing process that figures out which workers will execute which tasks. The default is the sequential executor — which cannot run tasks in parallel — meaning it can’t be used for production level code. The local executor can be used too which will run tasks till all resources on the server are at capacity. This is good for a moderate amount of DAGs. Both of these are used in single node clusters and therefore cannot be used to scaled. . | Multi node clusters — have the same components and only the scheduler and web server are placed in the same node (master), the workers are placed in a separate instance. This set up works well because it allows for scaling by letting you add more multi-node clusters (celery is the executor of choice here for python). . | . If you&#39;re not dealing with terabytes of data then it&#39;s better to have the scheduler, web server, and executor together in the master node/cluster. The downside is that this single cluster approach runs everything on the same machine, so if you make a change to a DAG/scheduler, then you need to restart the entire workflow — even tasks that were in the process of executing. Celery avoids this. . . If you do build a distributed workflow with celery then a queuing system component is needed (like Redis). For local workflows, the queuing is handled by the system. . The life cycle of a task . The scheduler periodically checks the DAG folder to see if there are any DAGS that need to be run. . | If any DAGS are found pending execution, the scheduler creates a diagram for it, which is an instantiation of a DAG in real time. . | The scheduler will update the DAG state to running in the metadata and the tasks will execute. . | The scheduler then reads the DAG and puts the tasks in order of execution into the queuing system in the form of a message. Each message contains info like DAG ID, TASK ID, and function to be executed. . | The status of these tasks changes to queued at that point. . | The executor then begins to execute tasks and sends fail/success messages for the tasks to the metadata. . | The scheduler finally updates the status of the diagram when all tasks have run to success or failure. . |",
            "url": "https://www.gurkamal.com//data-engineering/python/airflow/dag/2021/04/20/airflow-primer.html",
            "relUrl": "/data-engineering/python/airflow/dag/2021/04/20/airflow-primer.html",
            "date": " • Apr 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.gurkamal.com//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.gurkamal.com//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}