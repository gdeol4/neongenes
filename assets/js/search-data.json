{
  
    
        "post0": {
            "title": "Scaffold splitting and initial model training using chemprop",
            "content": "import chemprop import pandas as pd import datamol as dm from rdkit import Chem from rdkit.Chem.Scaffolds import MurckoScaffold pd.options.mode.chained_assignment = None # default=&#39;warn&#39; # loading the datasets MolNet = pd.read_csv(&quot;data/MoleculeNet.csv&quot;) B3DB = pd.read_csv(&quot;data/B3DB.csv&quot;) b3_molecules = pd.read_csv(&quot;data/b3_molecules.csv&quot;) . def scaffold_split(df): df[&quot;mol&quot;] = [Chem.MolFromSmiles(x) for x in df[&quot;standard_smiles&quot;]] # generating moles from the standard_smiles column df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=True) for x in df[&#39;mol&#39;]] # sanitize mol objects df = df.dropna() # dropping NA values df[&quot;scaffold&quot;] = [MurckoScaffold.GetScaffoldForMol(x) for x in df[&quot;mol&quot;]] # generating scaffolds from mol object df[&quot;mol_scaffold_generic&quot;] = [MurckoScaffold.MakeScaffoldGeneric(x) for x in df[&quot;scaffold&quot;]] # generalizing scaffolds # convert the generic scaffold mol object back to a SMILES string format df[&quot;smiles_scaffold_generic&quot;] = [Chem.CanonSmiles(Chem.MolToSmiles(x)) for x in df[&quot;mol_scaffold_generic&quot;]] return df . # results of the scaffold generating function data_split = scaffold_split(b3_molecules) data_split.head(1) . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey scaffold mol_scaffold_generic smiles_scaffold_generic . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | C1CCC2CCCCC2C1 | . # the data prior to processing MolNet.head(1) . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | . # A function to process the data for use with the chemprop library def chemprop_prep(df, filename): df = df.drop([&quot;mol&quot;, &quot;SMILES&quot;, &quot;selfies&quot;, &quot;inchi&quot;, &quot;inchikey&quot;], axis=1) # drop all columns except the smiles and target df[&quot;smiles&quot;] = df[&quot;standard_smiles&quot;] # use standard smiles inplace of smiles df = df.drop([&quot;standard_smiles&quot;], axis=1) # drop this column now df = df[[&quot;smiles&quot;, &quot;BBB+/BBB-&quot;]] # reorder the columns with smiles first and target second df.to_csv(&#39;./data/&#39; + filename, index=False) # save the file return df . # Processing the three different datasets molnet_chemprop = chemprop_prep(MolNet, &#39;molnet_chemprop&#39;) B3DB_chemprop = chemprop_prep(B3DB, &#39;B3DB_chemprep&#39;) b3_mol_chemprop = chemprop_prep(b3_molecules, &#39;B3DB_chemprep&#39;) . # results of processing molnet_chemprop.head(1) . smiles BBB+/BBB- . 0 CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | 1 | . smiles = &quot;CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC&quot; # convert SMILES string to RDKit mol object mol = Chem.MolFromSmiles(smiles) # create RDKit mol object corresponding to Bemis-Murcko scaffold of original compound mol_scaffold = MurckoScaffold.GetScaffoldForMol(mol) # make the scaffold generic by replacing all atoms with carbons and all bonds with single bonds mol_scaffold_generic = MurckoScaffold.MakeScaffoldGeneric(mol_scaffold) # convert the generic scaffold mol object back to a SMILES string format smiles_scaffold_generic = Chem.CanonSmiles(Chem.MolToSmiles(mol_scaffold_generic)) # display compound and its generic Bemis-Murcko scaffold display(mol) print(smiles) display(mol_scaffold_generic) print(smiles_scaffold_generic) . CN1CCCCC1CCN2C3=CC=CC=C3SC4=C2C=C(C=C4)SC . C1CCC(CCC2C3CCCCC3CC3CCCCC32)CC1 . arguments = [ &#39;--data_path&#39;, &#39;./data/chemprop_data.csv&#39;, &#39;--dataset_type&#39;, &#39;classification&#39;, &#39;--save_dir&#39;, &#39;./data/chemprop_checkpoints/&#39;, &#39;--split_type&#39;, &#39;scaffold_balanced&#39;, &#39;--separate_val_path&#39;, &#39;./data/chemprop_B3DB.csv&#39;, &#39;--num_folds&#39;, &#39;10&#39;, ] args = chemprop.args.TrainArgs().parse_args(arguments) mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training) . Command line python C: Users gurka miniconda3 envs chemprop lib site-packages ipykernel_launcher.py --ip=127.0.0.1 --stdin=9023 --control=9016 --hb=9015 --Session.signature_scheme=&#34;hmac-sha256&#34; --Session.key=b&#34;d428538a-db2b-40c8-b710-95eb7a929307&#34; --shell=9017 --transport=&#34;tcp&#34; --iopub=9024 --f=C: Users gurka AppData Local Temp tmp-9036lrc2sZ0NEBA3.json Args {&#39;activation&#39;: &#39;ReLU&#39;, &#39;aggregation&#39;: &#39;mean&#39;, &#39;aggregation_norm&#39;: 100, &#39;atom_descriptor_scaling&#39;: True, &#39;atom_descriptors&#39;: None, &#39;atom_descriptors_path&#39;: None, &#39;atom_descriptors_size&#39;: 0, &#39;atom_features_size&#39;: 0, &#39;atom_messages&#39;: False, &#39;batch_size&#39;: 50, &#39;bias&#39;: False, &#39;bond_feature_scaling&#39;: True, &#39;bond_features_path&#39;: None, &#39;bond_features_size&#39;: 0, &#39;cache_cutoff&#39;: 10000, &#39;checkpoint_dir&#39;: None, &#39;checkpoint_frzn&#39;: None, &#39;checkpoint_path&#39;: None, &#39;checkpoint_paths&#39;: None, &#39;class_balance&#39;: False, &#39;config_path&#39;: None, &#39;crossval_index_dir&#39;: None, &#39;crossval_index_file&#39;: None, &#39;crossval_index_sets&#39;: None, &#39;cuda&#39;: False, &#39;data_path&#39;: &#39;./data/chemprop_data.csv&#39;, &#39;data_weights_path&#39;: None, &#39;dataset_type&#39;: &#39;classification&#39;, &#39;depth&#39;: 3, &#39;device&#39;: device(type=&#39;cpu&#39;), &#39;dropout&#39;: 0.0, &#39;empty_cache&#39;: False, &#39;ensemble_size&#39;: 1, &#39;epochs&#39;: 30, &#39;explicit_h&#39;: False, &#39;extra_metrics&#39;: [], &#39;features_generator&#39;: None, &#39;features_only&#39;: False, &#39;features_path&#39;: None, &#39;features_scaling&#39;: True, &#39;features_size&#39;: None, &#39;ffn_hidden_size&#39;: 300, &#39;ffn_num_layers&#39;: 2, &#39;final_lr&#39;: 0.0001, &#39;folds_file&#39;: None, &#39;freeze_first_only&#39;: False, &#39;frzn_ffn_layers&#39;: 0, &#39;gpu&#39;: None, &#39;grad_clip&#39;: None, &#39;hidden_size&#39;: 300, &#39;ignore_columns&#39;: None, &#39;init_lr&#39;: 0.0001, &#39;log_frequency&#39;: 10, &#39;max_data_size&#39;: None, &#39;max_lr&#39;: 0.001, &#39;metric&#39;: &#39;auc&#39;, &#39;metrics&#39;: [&#39;auc&#39;], &#39;minimize_score&#39;: False, &#39;mpn_shared&#39;: False, &#39;multiclass_num_classes&#39;: 3, &#39;no_atom_descriptor_scaling&#39;: False, &#39;no_bond_features_scaling&#39;: False, &#39;no_cache_mol&#39;: False, &#39;no_cuda&#39;: False, &#39;no_features_scaling&#39;: False, &#39;num_folds&#39;: 10, &#39;num_lrs&#39;: 1, &#39;num_tasks&#39;: 1, &#39;num_workers&#39;: 8, &#39;number_of_molecules&#39;: 1, &#39;overwrite_default_atom_features&#39;: False, &#39;overwrite_default_bond_features&#39;: False, &#39;pytorch_seed&#39;: 0, &#39;quiet&#39;: False, &#39;reaction&#39;: False, &#39;reaction_mode&#39;: &#39;reac_diff&#39;, &#39;resume_experiment&#39;: False, &#39;save_dir&#39;: &#39;./data/chemprop_checkpoints/&#39;, &#39;save_preds&#39;: False, &#39;save_smiles_splits&#39;: False, &#39;seed&#39;: 0, &#39;separate_test_atom_descriptors_path&#39;: None, &#39;separate_test_bond_features_path&#39;: None, &#39;separate_test_features_path&#39;: None, &#39;separate_test_path&#39;: None, &#39;separate_val_atom_descriptors_path&#39;: None, &#39;separate_val_bond_features_path&#39;: None, &#39;separate_val_features_path&#39;: None, &#39;separate_val_path&#39;: &#39;./data/chemprop_B3DB.csv&#39;, &#39;show_individual_scores&#39;: False, &#39;smiles_columns&#39;: [&#39;smiles&#39;], &#39;split_sizes&#39;: (0.8, 0.1, 0.1), &#39;split_type&#39;: &#39;scaffold_balanced&#39;, &#39;target_columns&#39;: None, &#39;target_weights&#39;: None, &#39;task_names&#39;: [&#39;BBB+/BBB-&#39;], &#39;test&#39;: False, &#39;test_fold_index&#39;: None, &#39;train_data_size&#39;: None, &#39;undirected&#39;: False, &#39;use_input_features&#39;: False, &#39;val_fold_index&#39;: None, &#39;warmup_epochs&#39;: 2.0} Loading data 2039it [00:00, 119837.26it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 226314.16it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 78354.75it/s] Number of tasks = 1 Fold 0 Splitting data with seed 0 7807it [00:00, 48749.56it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 162493.77it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 65545.58it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 3347.05it/s] Total scaffolds = 1,025 | train scaffolds = 924 | val scaffolds = 0 | test scaffolds = 101 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.7921e-01, PNorm = 34.0133, GNorm = 0.5342, lr_0 = 2.5469e-04 Loss = 5.3200e-01, PNorm = 34.0237, GNorm = 0.6107, lr_0 = 3.9531e-04 Loss = 5.1853e-01, PNorm = 34.0464, GNorm = 0.5553, lr_0 = 5.3594e-04 Validation auc = 0.731898 3%|▎ | 1/30 [00:38&lt;18:43, 38.74s/it]Epoch 1 Loss = 4.7001e-01, PNorm = 34.0900, GNorm = 1.9152, lr_0 = 6.9063e-04 Loss = 4.3204e-01, PNorm = 34.1271, GNorm = 0.6978, lr_0 = 8.3125e-04 Loss = 4.5837e-01, PNorm = 34.1749, GNorm = 2.8353, lr_0 = 9.7187e-04 Validation auc = 0.739573 7%|▋ | 2/30 [01:01&lt;13:34, 29.08s/it]Epoch 2 Loss = 4.1592e-01, PNorm = 34.2418, GNorm = 0.8874, lr_0 = 9.7965e-04 Loss = 4.0019e-01, PNorm = 34.3068, GNorm = 0.3414, lr_0 = 9.5480e-04 Loss = 4.1145e-01, PNorm = 34.3543, GNorm = 0.6370, lr_0 = 9.3057e-04 Validation auc = 0.790529 10%|█ | 3/30 [01:23&lt;11:49, 26.26s/it]Epoch 3 Loss = 3.7677e-01, PNorm = 34.4208, GNorm = 0.9437, lr_0 = 9.0463e-04 Loss = 3.4925e-01, PNorm = 34.4773, GNorm = 0.5762, lr_0 = 8.8168e-04 Loss = 3.9518e-01, PNorm = 34.5250, GNorm = 0.4361, lr_0 = 8.5931e-04 Loss = 3.4804e-01, PNorm = 34.5879, GNorm = 0.7934, lr_0 = 8.3751e-04 Validation auc = 0.815596 13%|█▎ | 4/30 [01:46&lt;10:46, 24.88s/it]Epoch 4 Loss = 3.5069e-01, PNorm = 34.6495, GNorm = 0.6996, lr_0 = 8.1626e-04 Loss = 3.1209e-01, PNorm = 34.7035, GNorm = 0.6323, lr_0 = 7.9555e-04 Loss = 3.6705e-01, PNorm = 34.7594, GNorm = 0.7795, lr_0 = 7.7537e-04 Validation auc = 0.849116 17%|█▋ | 5/30 [02:09&lt;09:59, 23.97s/it]Epoch 5 Loss = 3.7468e-01, PNorm = 34.8100, GNorm = 0.7911, lr_0 = 7.5570e-04 Loss = 3.0905e-01, PNorm = 34.8686, GNorm = 1.4186, lr_0 = 7.3653e-04 Loss = 2.6892e-01, PNorm = 34.9266, GNorm = 1.0121, lr_0 = 7.1784e-04 Validation auc = 0.853062 20%|██ | 6/30 [02:33&lt;09:36, 24.01s/it]Epoch 6 Loss = 3.0230e-01, PNorm = 34.9655, GNorm = 0.8728, lr_0 = 6.9783e-04 Loss = 3.1416e-01, PNorm = 35.0221, GNorm = 1.2014, lr_0 = 6.8013e-04 Loss = 2.7962e-01, PNorm = 35.0705, GNorm = 0.6963, lr_0 = 6.6287e-04 Validation auc = 0.840569 23%|██▎ | 7/30 [02:55&lt;08:57, 23.38s/it]Epoch 7 Loss = 1.8715e-01, PNorm = 35.1038, GNorm = 1.2228, lr_0 = 6.4605e-04 Loss = 3.1151e-01, PNorm = 35.1414, GNorm = 0.7543, lr_0 = 6.2966e-04 Loss = 2.9637e-01, PNorm = 35.1967, GNorm = 0.8723, lr_0 = 6.1369e-04 Loss = 3.1616e-01, PNorm = 35.2427, GNorm = 2.5857, lr_0 = 5.9812e-04 Loss = 5.2971e-01, PNorm = 35.2462, GNorm = 3.8233, lr_0 = 5.9658e-04 Validation auc = 0.854273 27%|██▋ | 8/30 [03:16&lt;08:16, 22.56s/it]Epoch 8 Loss = 3.1306e-01, PNorm = 35.2873, GNorm = 1.1070, lr_0 = 5.8145e-04 Loss = 2.8194e-01, PNorm = 35.3365, GNorm = 1.4047, lr_0 = 5.6669e-04 Loss = 2.7593e-01, PNorm = 35.3866, GNorm = 0.9889, lr_0 = 5.5232e-04 Validation auc = 0.855899 30%|███ | 9/30 [03:39&lt;08:01, 22.92s/it]Epoch 9 Loss = 2.8666e-01, PNorm = 35.4314, GNorm = 1.3941, lr_0 = 5.3830e-04 Loss = 2.5622e-01, PNorm = 35.4844, GNorm = 0.4744, lr_0 = 5.2465e-04 Loss = 3.1872e-01, PNorm = 35.5168, GNorm = 0.5933, lr_0 = 5.1133e-04 Validation auc = 0.871488 33%|███▎ | 10/30 [04:02&lt;07:38, 22.92s/it]Epoch 10 Loss = 2.9573e-01, PNorm = 35.5591, GNorm = 1.1211, lr_0 = 4.9836e-04 Loss = 2.6221e-01, PNorm = 35.6001, GNorm = 0.9222, lr_0 = 4.8572e-04 Loss = 2.6929e-01, PNorm = 35.6405, GNorm = 1.2863, lr_0 = 4.7339e-04 Validation auc = 0.867729 37%|███▋ | 11/30 [04:26&lt;07:20, 23.17s/it]Epoch 11 Loss = 2.7630e-01, PNorm = 35.6886, GNorm = 0.8642, lr_0 = 4.6020e-04 Loss = 2.6721e-01, PNorm = 35.7368, GNorm = 1.3450, lr_0 = 4.4852e-04 Loss = 3.1437e-01, PNorm = 35.7796, GNorm = 0.9707, lr_0 = 4.3714e-04 Loss = 2.7745e-01, PNorm = 35.8168, GNorm = 1.8431, lr_0 = 4.2605e-04 Validation auc = 0.873370 40%|████ | 12/30 [04:50&lt;06:59, 23.32s/it]Epoch 12 Loss = 2.3011e-01, PNorm = 35.8567, GNorm = 0.9368, lr_0 = 4.1524e-04 Loss = 3.0085e-01, PNorm = 35.8913, GNorm = 2.3247, lr_0 = 4.0471e-04 Loss = 2.4651e-01, PNorm = 35.9194, GNorm = 2.9509, lr_0 = 3.9444e-04 Validation auc = 0.867464 43%|████▎ | 13/30 [05:14&lt;06:39, 23.50s/it]Epoch 13 Loss = 2.3372e-01, PNorm = 35.9452, GNorm = 0.9986, lr_0 = 3.8443e-04 Loss = 2.2999e-01, PNorm = 35.9764, GNorm = 0.8525, lr_0 = 3.7468e-04 Loss = 2.2414e-01, PNorm = 36.0128, GNorm = 0.7044, lr_0 = 3.6517e-04 Validation auc = 0.875426 47%|████▋ | 14/30 [05:37&lt;06:14, 23.39s/it]Epoch 14 Loss = 2.0713e-01, PNorm = 36.0453, GNorm = 0.4335, lr_0 = 3.5500e-04 Loss = 2.3194e-01, PNorm = 36.0770, GNorm = 2.1658, lr_0 = 3.4599e-04 Loss = 2.5871e-01, PNorm = 36.1087, GNorm = 0.9426, lr_0 = 3.3721e-04 Validation auc = 0.876673 50%|█████ | 15/30 [06:00&lt;05:51, 23.43s/it]Epoch 15 Loss = 2.8820e-01, PNorm = 36.1377, GNorm = 1.4118, lr_0 = 3.2866e-04 Loss = 2.2613e-01, PNorm = 36.1684, GNorm = 1.3319, lr_0 = 3.2032e-04 Loss = 2.0750e-01, PNorm = 36.1953, GNorm = 2.1855, lr_0 = 3.1219e-04 Loss = 2.7105e-01, PNorm = 36.2226, GNorm = 1.6245, lr_0 = 3.0427e-04 Validation auc = 0.865978 53%|█████▎ | 16/30 [06:24&lt;05:30, 23.60s/it]Epoch 16 Loss = 2.4408e-01, PNorm = 36.2519, GNorm = 1.5706, lr_0 = 2.9579e-04 Loss = 2.2506e-01, PNorm = 36.2784, GNorm = 0.9227, lr_0 = 2.8828e-04 Loss = 2.0077e-01, PNorm = 36.3028, GNorm = 0.6776, lr_0 = 2.8097e-04 Validation auc = 0.878100 57%|█████▋ | 17/30 [06:47&lt;05:05, 23.48s/it]Epoch 17 Loss = 2.0379e-01, PNorm = 36.3313, GNorm = 0.5450, lr_0 = 2.7384e-04 Loss = 2.1564e-01, PNorm = 36.3560, GNorm = 0.7152, lr_0 = 2.6689e-04 Loss = 1.8612e-01, PNorm = 36.3825, GNorm = 1.7715, lr_0 = 2.6012e-04 Validation auc = 0.879983 60%|██████ | 18/30 [07:10&lt;04:39, 23.32s/it]Epoch 18 Loss = 2.1749e-01, PNorm = 36.4032, GNorm = 2.1023, lr_0 = 2.5352e-04 Loss = 2.0186e-01, PNorm = 36.4244, GNorm = 1.0254, lr_0 = 2.4709e-04 Loss = 2.2377e-01, PNorm = 36.4476, GNorm = 1.5030, lr_0 = 2.4082e-04 Validation auc = 0.877642 63%|██████▎ | 19/30 [07:35&lt;04:20, 23.64s/it]Epoch 19 Loss = 2.1661e-01, PNorm = 36.4699, GNorm = 2.0885, lr_0 = 2.3411e-04 Loss = 1.9739e-01, PNorm = 36.4885, GNorm = 1.5452, lr_0 = 2.2817e-04 Loss = 1.8020e-01, PNorm = 36.5104, GNorm = 0.7588, lr_0 = 2.2238e-04 Loss = 2.1539e-01, PNorm = 36.5302, GNorm = 1.9140, lr_0 = 2.1674e-04 Validation auc = 0.874604 67%|██████▋ | 20/30 [07:58&lt;03:55, 23.53s/it]Epoch 20 Loss = 2.0466e-01, PNorm = 36.5521, GNorm = 0.9277, lr_0 = 2.1124e-04 Loss = 1.9847e-01, PNorm = 36.5712, GNorm = 1.9312, lr_0 = 2.0588e-04 Loss = 1.7716e-01, PNorm = 36.5888, GNorm = 0.9575, lr_0 = 2.0066e-04 Validation auc = 0.881012 70%|███████ | 21/30 [08:21&lt;03:31, 23.50s/it]Epoch 21 Loss = 1.2171e-01, PNorm = 36.6069, GNorm = 1.4518, lr_0 = 1.9557e-04 Loss = 2.1633e-01, PNorm = 36.6252, GNorm = 3.4335, lr_0 = 1.9060e-04 Loss = 1.9789e-01, PNorm = 36.6406, GNorm = 0.8608, lr_0 = 1.8577e-04 Validation auc = 0.880775 73%|███████▎ | 22/30 [08:46&lt;03:10, 23.84s/it]Epoch 22 Loss = 1.7108e-01, PNorm = 36.6599, GNorm = 1.3346, lr_0 = 1.8059e-04 Loss = 2.0594e-01, PNorm = 36.6781, GNorm = 1.2820, lr_0 = 1.7601e-04 Loss = 2.1049e-01, PNorm = 36.6936, GNorm = 1.2921, lr_0 = 1.7154e-04 Loss = 1.4001e-01, PNorm = 36.7102, GNorm = 1.3220, lr_0 = 1.6719e-04 Validation auc = 0.878249 77%|███████▋ | 23/30 [09:09&lt;02:45, 23.68s/it]Epoch 23 Loss = 1.8390e-01, PNorm = 36.7259, GNorm = 1.5580, lr_0 = 1.6295e-04 Loss = 1.5908e-01, PNorm = 36.7417, GNorm = 0.5529, lr_0 = 1.5882e-04 Loss = 1.9535e-01, PNorm = 36.7555, GNorm = 1.9520, lr_0 = 1.5479e-04 Validation auc = 0.879262 80%|████████ | 24/30 [09:33&lt;02:22, 23.70s/it]Epoch 24 Loss = 1.9363e-01, PNorm = 36.7711, GNorm = 2.5496, lr_0 = 1.5047e-04 Loss = 1.7139e-01, PNorm = 36.7848, GNorm = 0.9105, lr_0 = 1.4665e-04 Loss = 1.9078e-01, PNorm = 36.7979, GNorm = 2.2030, lr_0 = 1.4293e-04 Validation auc = 0.881092 83%|████████▎ | 25/30 [09:58&lt;02:00, 24.04s/it]Epoch 25 Loss = 2.0122e-01, PNorm = 36.8112, GNorm = 2.0350, lr_0 = 1.3931e-04 Loss = 1.7755e-01, PNorm = 36.8237, GNorm = 1.9924, lr_0 = 1.3577e-04 Loss = 1.6288e-01, PNorm = 36.8345, GNorm = 1.0033, lr_0 = 1.3233e-04 Validation auc = 0.879667 87%|████████▋ | 26/30 [10:21&lt;01:35, 23.83s/it]Epoch 26 Loss = 1.7851e-01, PNorm = 36.8477, GNorm = 1.3995, lr_0 = 1.2897e-04 Loss = 1.6410e-01, PNorm = 36.8601, GNorm = 2.0577, lr_0 = 1.2570e-04 Loss = 1.6417e-01, PNorm = 36.8716, GNorm = 1.4548, lr_0 = 1.2251e-04 Loss = 1.9043e-01, PNorm = 36.8844, GNorm = 0.5458, lr_0 = 1.1940e-04 Loss = 1.3765e-01, PNorm = 36.8858, GNorm = 1.8756, lr_0 = 1.1909e-04 Validation auc = 0.878424 90%|█████████ | 27/30 [10:43&lt;01:09, 23.06s/it]Epoch 27 Loss = 1.7380e-01, PNorm = 36.8967, GNorm = 0.6299, lr_0 = 1.1607e-04 Loss = 1.7133e-01, PNorm = 36.9085, GNorm = 0.9839, lr_0 = 1.1313e-04 Loss = 1.5482e-01, PNorm = 36.9189, GNorm = 0.6951, lr_0 = 1.1026e-04 Validation auc = 0.878794 93%|█████████▎| 28/30 [11:04&lt;00:45, 22.59s/it]Epoch 28 Loss = 1.7905e-01, PNorm = 36.9294, GNorm = 0.9441, lr_0 = 1.0746e-04 Loss = 1.6842e-01, PNorm = 36.9389, GNorm = 2.0891, lr_0 = 1.0473e-04 Loss = 1.6954e-01, PNorm = 36.9487, GNorm = 1.1413, lr_0 = 1.0208e-04 Validation auc = 0.877537 97%|█████████▋| 29/30 [11:27&lt;00:22, 22.59s/it]Epoch 29 Loss = 1.2676e-01, PNorm = 36.9588, GNorm = 1.3964, lr_0 = 1.0000e-04 Loss = 1.6358e-01, PNorm = 36.9683, GNorm = 0.8904, lr_0 = 1.0000e-04 Loss = 1.7523e-01, PNorm = 36.9776, GNorm = 1.2408, lr_0 = 1.0000e-04 Validation auc = 0.877406 100%|██████████| 30/30 [11:48&lt;00:00, 23.63s/it] Model 0 best validation auc = 0.881092 on epoch 24 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.844660 Ensemble test auc = 0.844660 Fold 1 Splitting data with seed 1 7807it [00:00, 106844.77it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 169563.58it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 69328.88it/s] 100%|██████████| 2039/2039 [00:01&lt;00:00, 1967.29it/s] Total scaffolds = 1,025 | train scaffolds = 899 | val scaffolds = 0 | test scaffolds = 126 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.8283e-01, PNorm = 34.0132, GNorm = 0.9347, lr_0 = 2.5469e-04 Loss = 5.3997e-01, PNorm = 34.0187, GNorm = 0.2686, lr_0 = 3.9531e-04 Loss = 5.4015e-01, PNorm = 34.0372, GNorm = 0.8418, lr_0 = 5.3594e-04 Validation auc = 0.723354 3%|▎ | 1/30 [00:23&lt;11:32, 23.88s/it]Epoch 1 Loss = 5.0507e-01, PNorm = 34.0716, GNorm = 2.6406, lr_0 = 6.9063e-04 Loss = 4.3937e-01, PNorm = 34.1089, GNorm = 0.4836, lr_0 = 8.3125e-04 Loss = 4.9619e-01, PNorm = 34.1618, GNorm = 1.7031, lr_0 = 9.7187e-04 Validation auc = 0.790985 7%|▋ | 2/30 [00:45&lt;10:38, 22.82s/it]Epoch 2 Loss = 4.5610e-01, PNorm = 34.2304, GNorm = 0.6527, lr_0 = 9.7965e-04 Loss = 4.6261e-01, PNorm = 34.3044, GNorm = 1.3506, lr_0 = 9.5480e-04 Loss = 4.0275e-01, PNorm = 34.3694, GNorm = 1.7302, lr_0 = 9.3057e-04 Validation auc = 0.806004 10%|█ | 3/30 [01:07&lt;09:56, 22.11s/it]Epoch 3 Loss = 4.1068e-01, PNorm = 34.4422, GNorm = 0.8234, lr_0 = 9.0463e-04 Loss = 4.4421e-01, PNorm = 34.4953, GNorm = 0.5707, lr_0 = 8.8168e-04 Loss = 3.5599e-01, PNorm = 34.5530, GNorm = 0.2221, lr_0 = 8.5931e-04 Loss = 3.7585e-01, PNorm = 34.5886, GNorm = 0.8107, lr_0 = 8.3751e-04 Validation auc = 0.816117 13%|█▎ | 4/30 [01:28&lt;09:29, 21.89s/it]Epoch 4 Loss = 3.2671e-01, PNorm = 34.6467, GNorm = 1.4990, lr_0 = 8.1626e-04 Loss = 4.1754e-01, PNorm = 34.6924, GNorm = 1.5698, lr_0 = 7.9555e-04 Loss = 3.7652e-01, PNorm = 34.7473, GNorm = 0.7108, lr_0 = 7.7537e-04 Validation auc = 0.834371 17%|█▋ | 5/30 [01:51&lt;09:12, 22.12s/it]Epoch 5 Loss = 3.5562e-01, PNorm = 34.8102, GNorm = 0.7067, lr_0 = 7.5570e-04 Loss = 3.8079e-01, PNorm = 34.8607, GNorm = 0.6355, lr_0 = 7.3653e-04 Loss = 3.5995e-01, PNorm = 34.9105, GNorm = 1.0636, lr_0 = 7.1784e-04 Validation auc = 0.834212 20%|██ | 6/30 [02:12&lt;08:43, 21.80s/it]Epoch 6 Loss = 3.4494e-01, PNorm = 34.9751, GNorm = 0.5044, lr_0 = 6.9783e-04 Loss = 3.5540e-01, PNorm = 35.0362, GNorm = 2.7041, lr_0 = 6.8013e-04 Loss = 3.7456e-01, PNorm = 35.0753, GNorm = 1.2072, lr_0 = 6.6287e-04 Validation auc = 0.845199 23%|██▎ | 7/30 [02:35&lt;08:31, 22.26s/it]Epoch 7 Loss = 3.6567e-01, PNorm = 35.1313, GNorm = 0.5053, lr_0 = 6.4605e-04 Loss = 2.9606e-01, PNorm = 35.1913, GNorm = 2.0932, lr_0 = 6.2966e-04 Loss = 3.1622e-01, PNorm = 35.2301, GNorm = 2.0060, lr_0 = 6.1369e-04 Loss = 3.5755e-01, PNorm = 35.2781, GNorm = 1.0133, lr_0 = 5.9812e-04 Loss = 3.8792e-01, PNorm = 35.2826, GNorm = 0.7208, lr_0 = 5.9658e-04 Validation auc = 0.853250 27%|██▋ | 8/30 [03:00&lt;08:28, 23.11s/it]Epoch 8 Loss = 3.2643e-01, PNorm = 35.3344, GNorm = 0.6999, lr_0 = 5.8145e-04 Loss = 3.0346e-01, PNorm = 35.3856, GNorm = 0.6837, lr_0 = 5.6669e-04 Loss = 2.8790e-01, PNorm = 35.4334, GNorm = 0.6533, lr_0 = 5.5232e-04 Validation auc = 0.855157 30%|███ | 9/30 [03:24&lt;08:12, 23.45s/it]Epoch 9 Loss = 2.6723e-01, PNorm = 35.4871, GNorm = 0.8949, lr_0 = 5.3830e-04 Loss = 2.7581e-01, PNorm = 35.5388, GNorm = 0.7371, lr_0 = 5.2465e-04 Loss = 3.0116e-01, PNorm = 35.5795, GNorm = 0.8037, lr_0 = 5.1133e-04 Validation auc = 0.860529 33%|███▎ | 10/30 [03:48&lt;07:53, 23.67s/it]Epoch 10 Loss = 3.1910e-01, PNorm = 35.6263, GNorm = 1.7916, lr_0 = 4.9836e-04 Loss = 2.7589e-01, PNorm = 35.6775, GNorm = 1.0073, lr_0 = 4.8572e-04 Loss = 2.6230e-01, PNorm = 35.7196, GNorm = 1.6867, lr_0 = 4.7339e-04 Validation auc = 0.855913 37%|███▋ | 11/30 [04:13&lt;07:34, 23.94s/it]Epoch 11 Loss = 2.2269e-01, PNorm = 35.7685, GNorm = 0.5647, lr_0 = 4.6020e-04 Loss = 2.8032e-01, PNorm = 35.8202, GNorm = 0.9101, lr_0 = 4.4852e-04 Loss = 2.6989e-01, PNorm = 35.8575, GNorm = 0.7661, lr_0 = 4.3714e-04 Loss = 2.5243e-01, PNorm = 35.8924, GNorm = 1.8814, lr_0 = 4.2605e-04 Validation auc = 0.866222 40%|████ | 12/30 [04:35&lt;07:01, 23.39s/it]Epoch 12 Loss = 3.0864e-01, PNorm = 35.9291, GNorm = 2.4162, lr_0 = 4.1524e-04 Loss = 2.6324e-01, PNorm = 35.9657, GNorm = 1.0833, lr_0 = 4.0471e-04 Loss = 2.5254e-01, PNorm = 36.0018, GNorm = 0.8596, lr_0 = 3.9444e-04 Validation auc = 0.864480 43%|████▎ | 13/30 [04:58&lt;06:35, 23.29s/it]Epoch 13 Loss = 2.7847e-01, PNorm = 36.0386, GNorm = 1.0885, lr_0 = 3.8443e-04 Loss = 2.1106e-01, PNorm = 36.0755, GNorm = 0.7475, lr_0 = 3.7468e-04 Loss = 2.8272e-01, PNorm = 36.1032, GNorm = 1.4107, lr_0 = 3.6517e-04 Validation auc = 0.868644 47%|████▋ | 14/30 [05:23&lt;06:20, 23.79s/it]Epoch 14 Loss = 2.6149e-01, PNorm = 36.1385, GNorm = 0.9653, lr_0 = 3.5500e-04 Loss = 2.7012e-01, PNorm = 36.1748, GNorm = 0.9300, lr_0 = 3.4599e-04 Loss = 2.3500e-01, PNorm = 36.2065, GNorm = 0.9834, lr_0 = 3.3721e-04 Validation auc = 0.872554 50%|█████ | 15/30 [05:48&lt;06:03, 24.25s/it]Epoch 15 Loss = 1.3722e-01, PNorm = 36.2363, GNorm = 0.7371, lr_0 = 3.2866e-04 Loss = 2.6957e-01, PNorm = 36.2676, GNorm = 1.2163, lr_0 = 3.2032e-04 Loss = 2.0775e-01, PNorm = 36.2973, GNorm = 0.7404, lr_0 = 3.1219e-04 Loss = 2.2550e-01, PNorm = 36.3213, GNorm = 1.2833, lr_0 = 3.0427e-04 Validation auc = 0.869664 53%|█████▎ | 16/30 [06:14&lt;05:44, 24.57s/it]Epoch 16 Loss = 2.4413e-01, PNorm = 36.3483, GNorm = 0.9222, lr_0 = 2.9579e-04 Loss = 2.3373e-01, PNorm = 36.3762, GNorm = 0.6782, lr_0 = 2.8828e-04 Loss = 1.9903e-01, PNorm = 36.4009, GNorm = 1.7978, lr_0 = 2.8097e-04 Validation auc = 0.871752 57%|█████▋ | 17/30 [06:39&lt;05:20, 24.68s/it]Epoch 17 Loss = 2.1133e-01, PNorm = 36.4263, GNorm = 0.9462, lr_0 = 2.7384e-04 Loss = 2.1816e-01, PNorm = 36.4528, GNorm = 0.9474, lr_0 = 2.6689e-04 Loss = 2.3951e-01, PNorm = 36.4767, GNorm = 0.9969, lr_0 = 2.6012e-04 Validation auc = 0.869897 60%|██████ | 18/30 [07:03&lt;04:54, 24.57s/it]Epoch 18 Loss = 2.5544e-01, PNorm = 36.4969, GNorm = 1.2004, lr_0 = 2.5352e-04 Loss = 2.0932e-01, PNorm = 36.5184, GNorm = 0.7141, lr_0 = 2.4709e-04 Loss = 2.5558e-01, PNorm = 36.5406, GNorm = 0.8702, lr_0 = 2.4082e-04 Validation auc = 0.869473 63%|██████▎ | 19/30 [07:30&lt;04:36, 25.18s/it]Epoch 19 Loss = 2.9814e-01, PNorm = 36.5635, GNorm = 1.9294, lr_0 = 2.3411e-04 Loss = 2.0657e-01, PNorm = 36.5883, GNorm = 0.8910, lr_0 = 2.2817e-04 Loss = 1.7860e-01, PNorm = 36.6074, GNorm = 1.3545, lr_0 = 2.2238e-04 Loss = 2.3013e-01, PNorm = 36.6244, GNorm = 1.1076, lr_0 = 2.1674e-04 Validation auc = 0.869637 67%|██████▋ | 20/30 [07:53&lt;04:06, 24.66s/it]Epoch 20 Loss = 1.9790e-01, PNorm = 36.6436, GNorm = 0.4017, lr_0 = 2.1124e-04 Loss = 1.9631e-01, PNorm = 36.6651, GNorm = 1.2600, lr_0 = 2.0588e-04 Loss = 2.0615e-01, PNorm = 36.6825, GNorm = 1.9261, lr_0 = 2.0066e-04 Validation auc = 0.868412 70%|███████ | 21/30 [08:17&lt;03:40, 24.48s/it]Epoch 21 Loss = 2.0785e-01, PNorm = 36.6980, GNorm = 0.9904, lr_0 = 1.9557e-04 Loss = 1.8450e-01, PNorm = 36.7161, GNorm = 2.2254, lr_0 = 1.9060e-04 Loss = 2.0003e-01, PNorm = 36.7311, GNorm = 0.5961, lr_0 = 1.8577e-04 Validation auc = 0.872807 73%|███████▎ | 22/30 [08:42&lt;03:15, 24.46s/it]Epoch 22 Loss = 1.9961e-01, PNorm = 36.7495, GNorm = 0.7913, lr_0 = 1.8059e-04 Loss = 1.8202e-01, PNorm = 36.7663, GNorm = 1.5576, lr_0 = 1.7601e-04 Loss = 2.0640e-01, PNorm = 36.7824, GNorm = 2.6012, lr_0 = 1.7154e-04 Loss = 1.9832e-01, PNorm = 36.7937, GNorm = 0.7935, lr_0 = 1.6719e-04 Validation auc = 0.870136 77%|███████▋ | 23/30 [09:04&lt;02:47, 23.89s/it]Epoch 23 Loss = 1.7012e-01, PNorm = 36.8093, GNorm = 0.9299, lr_0 = 1.6295e-04 Loss = 1.6535e-01, PNorm = 36.8235, GNorm = 0.9022, lr_0 = 1.5882e-04 Loss = 2.2565e-01, PNorm = 36.8362, GNorm = 0.6081, lr_0 = 1.5479e-04 Validation auc = 0.871655 80%|████████ | 24/30 [09:28&lt;02:22, 23.75s/it]Epoch 24 Loss = 1.7154e-01, PNorm = 36.8516, GNorm = 1.2007, lr_0 = 1.5047e-04 Loss = 2.0130e-01, PNorm = 36.8641, GNorm = 0.8403, lr_0 = 1.4665e-04 Loss = 1.9262e-01, PNorm = 36.8771, GNorm = 1.6096, lr_0 = 1.4293e-04 Validation auc = 0.869651 83%|████████▎ | 25/30 [09:50&lt;01:56, 23.25s/it]Epoch 25 Loss = 1.7977e-01, PNorm = 36.8901, GNorm = 0.5687, lr_0 = 1.3931e-04 Loss = 2.0113e-01, PNorm = 36.9016, GNorm = 0.8200, lr_0 = 1.3577e-04 Loss = 1.5470e-01, PNorm = 36.9153, GNorm = 1.3182, lr_0 = 1.3233e-04 Validation auc = 0.875535 87%|████████▋ | 26/30 [10:12&lt;01:31, 22.85s/it]Epoch 26 Loss = 1.1032e-01, PNorm = 36.9278, GNorm = 0.7398, lr_0 = 1.2897e-04 Loss = 1.7223e-01, PNorm = 36.9396, GNorm = 1.5111, lr_0 = 1.2570e-04 Loss = 2.3340e-01, PNorm = 36.9518, GNorm = 1.4119, lr_0 = 1.2251e-04 Loss = 1.5973e-01, PNorm = 36.9620, GNorm = 0.6217, lr_0 = 1.1940e-04 Loss = 1.5186e-01, PNorm = 36.9632, GNorm = 2.1749, lr_0 = 1.1909e-04 Validation auc = 0.872548 90%|█████████ | 27/30 [10:33&lt;01:07, 22.49s/it]Epoch 27 Loss = 1.7789e-01, PNorm = 36.9714, GNorm = 1.5165, lr_0 = 1.1607e-04 Loss = 2.1012e-01, PNorm = 36.9808, GNorm = 1.7794, lr_0 = 1.1313e-04 Loss = 1.7228e-01, PNorm = 36.9913, GNorm = 1.9834, lr_0 = 1.1026e-04 Validation auc = 0.870228 93%|█████████▎| 28/30 [10:57&lt;00:45, 22.78s/it]Epoch 28 Loss = 1.4317e-01, PNorm = 37.0015, GNorm = 0.5428, lr_0 = 1.0746e-04 Loss = 2.0754e-01, PNorm = 37.0132, GNorm = 1.9797, lr_0 = 1.0473e-04 Loss = 1.7099e-01, PNorm = 37.0217, GNorm = 1.8418, lr_0 = 1.0208e-04 Validation auc = 0.874042 97%|█████████▋| 29/30 [11:18&lt;00:22, 22.42s/it]Epoch 29 Loss = 1.9134e-01, PNorm = 37.0333, GNorm = 2.0929, lr_0 = 1.0000e-04 Loss = 1.4536e-01, PNorm = 37.0410, GNorm = 1.1186, lr_0 = 1.0000e-04 Loss = 1.9219e-01, PNorm = 37.0507, GNorm = 1.4713, lr_0 = 1.0000e-04 Validation auc = 0.871245 100%|██████████| 30/30 [11:39&lt;00:00, 23.33s/it] Model 0 best validation auc = 0.875535 on epoch 25 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.859605 Ensemble test auc = 0.859605 Fold 2 Splitting data with seed 2 7807it [00:00, 93976.35it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 165957.62it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 66665.98it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 2241.03it/s] Total scaffolds = 1,025 | train scaffolds = 871 | val scaffolds = 0 | test scaffolds = 154 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 6.0362e-01, PNorm = 34.0126, GNorm = 0.6707, lr_0 = 2.5469e-04 Loss = 5.0584e-01, PNorm = 34.0185, GNorm = 0.3407, lr_0 = 3.9531e-04 Loss = 4.9781e-01, PNorm = 34.0381, GNorm = 0.3298, lr_0 = 5.3594e-04 Validation auc = 0.760552 3%|▎ | 1/30 [00:22&lt;11:05, 22.96s/it]Epoch 1 Loss = 4.6252e-01, PNorm = 34.0779, GNorm = 0.8323, lr_0 = 6.9063e-04 Loss = 4.4080e-01, PNorm = 34.1227, GNorm = 2.8627, lr_0 = 8.3125e-04 Loss = 4.6250e-01, PNorm = 34.1646, GNorm = 0.5142, lr_0 = 9.7187e-04 Validation auc = 0.790695 7%|▋ | 2/30 [00:44&lt;10:22, 22.24s/it]Epoch 2 Loss = 4.0534e-01, PNorm = 34.2277, GNorm = 0.5855, lr_0 = 9.7965e-04 Loss = 4.1850e-01, PNorm = 34.2869, GNorm = 1.5344, lr_0 = 9.5480e-04 Loss = 3.9848e-01, PNorm = 34.3544, GNorm = 0.6003, lr_0 = 9.3057e-04 Validation auc = 0.824694 10%|█ | 3/30 [01:05&lt;09:44, 21.63s/it]Epoch 3 Loss = 3.3840e-01, PNorm = 34.4211, GNorm = 1.0208, lr_0 = 9.0463e-04 Loss = 4.2338e-01, PNorm = 34.4658, GNorm = 2.6432, lr_0 = 8.8168e-04 Loss = 4.6957e-01, PNorm = 34.5164, GNorm = 2.4411, lr_0 = 8.5931e-04 Loss = 4.1367e-01, PNorm = 34.5805, GNorm = 0.9025, lr_0 = 8.3751e-04 Validation auc = 0.825354 13%|█▎ | 4/30 [01:27&lt;09:25, 21.74s/it]Epoch 4 Loss = 3.9201e-01, PNorm = 34.6501, GNorm = 0.6751, lr_0 = 8.1626e-04 Loss = 3.5608e-01, PNorm = 34.7110, GNorm = 0.2464, lr_0 = 7.9555e-04 Loss = 3.7925e-01, PNorm = 34.7609, GNorm = 1.6704, lr_0 = 7.7537e-04 Validation auc = 0.838017 17%|█▋ | 5/30 [01:49&lt;09:01, 21.66s/it]Epoch 5 Loss = 2.9262e-01, PNorm = 34.8032, GNorm = 0.8056, lr_0 = 7.5570e-04 Loss = 3.4921e-01, PNorm = 34.8425, GNorm = 0.6386, lr_0 = 7.3653e-04 Loss = 3.5358e-01, PNorm = 34.8934, GNorm = 0.9398, lr_0 = 7.1784e-04 Validation auc = 0.841108 20%|██ | 6/30 [02:09&lt;08:34, 21.42s/it]Epoch 6 Loss = 3.7898e-01, PNorm = 34.9461, GNorm = 1.7352, lr_0 = 6.9783e-04 Loss = 2.8961e-01, PNorm = 34.9995, GNorm = 0.7246, lr_0 = 6.8013e-04 Loss = 3.8121e-01, PNorm = 35.0442, GNorm = 1.4410, lr_0 = 6.6287e-04 Validation auc = 0.855532 23%|██▎ | 7/30 [02:32&lt;08:17, 21.63s/it]Epoch 7 Loss = 3.7778e-01, PNorm = 35.0863, GNorm = 0.9807, lr_0 = 6.4605e-04 Loss = 3.0422e-01, PNorm = 35.1288, GNorm = 1.0329, lr_0 = 6.2966e-04 Loss = 3.3546e-01, PNorm = 35.1729, GNorm = 1.1070, lr_0 = 6.1369e-04 Loss = 3.0829e-01, PNorm = 35.2123, GNorm = 0.6419, lr_0 = 5.9812e-04 Loss = 3.3615e-01, PNorm = 35.2163, GNorm = 2.7590, lr_0 = 5.9658e-04 Validation auc = 0.858447 27%|██▋ | 8/30 [02:53&lt;07:55, 21.60s/it]Epoch 8 Loss = 2.8547e-01, PNorm = 35.2628, GNorm = 0.4471, lr_0 = 5.8145e-04 Loss = 3.1165e-01, PNorm = 35.3067, GNorm = 0.9458, lr_0 = 5.6669e-04 Loss = 3.1942e-01, PNorm = 35.3441, GNorm = 0.7806, lr_0 = 5.5232e-04 Validation auc = 0.865120 30%|███ | 9/30 [03:14&lt;07:31, 21.51s/it]Epoch 9 Loss = 2.5204e-01, PNorm = 35.3841, GNorm = 0.8710, lr_0 = 5.3830e-04 Loss = 3.2551e-01, PNorm = 35.4262, GNorm = 0.5684, lr_0 = 5.2465e-04 Loss = 2.6458e-01, PNorm = 35.4691, GNorm = 2.5230, lr_0 = 5.1133e-04 Validation auc = 0.867362 33%|███▎ | 10/30 [03:36&lt;07:11, 21.57s/it]Epoch 10 Loss = 2.5710e-01, PNorm = 35.5074, GNorm = 0.7284, lr_0 = 4.9836e-04 Loss = 2.7672e-01, PNorm = 35.5509, GNorm = 0.8951, lr_0 = 4.8572e-04 Loss = 2.9099e-01, PNorm = 35.5904, GNorm = 1.2288, lr_0 = 4.7339e-04 Validation auc = 0.860585 37%|███▋ | 11/30 [03:57&lt;06:43, 21.22s/it]Epoch 11 Loss = 2.3208e-01, PNorm = 35.6297, GNorm = 0.7105, lr_0 = 4.6020e-04 Loss = 2.5835e-01, PNorm = 35.6634, GNorm = 1.8572, lr_0 = 4.4852e-04 Loss = 2.6463e-01, PNorm = 35.7016, GNorm = 0.9017, lr_0 = 4.3714e-04 Loss = 2.5494e-01, PNorm = 35.7439, GNorm = 2.5274, lr_0 = 4.2605e-04 Validation auc = 0.862331 40%|████ | 12/30 [04:17&lt;06:17, 20.96s/it]Epoch 12 Loss = 2.7021e-01, PNorm = 35.7715, GNorm = 0.6333, lr_0 = 4.1524e-04 Loss = 2.5902e-01, PNorm = 35.8035, GNorm = 1.9498, lr_0 = 4.0471e-04 Loss = 2.8274e-01, PNorm = 35.8342, GNorm = 1.3376, lr_0 = 3.9444e-04 Validation auc = 0.871743 43%|████▎ | 13/30 [04:37&lt;05:53, 20.79s/it]Epoch 13 Loss = 2.3532e-01, PNorm = 35.8605, GNorm = 2.3923, lr_0 = 3.8443e-04 Loss = 2.5791e-01, PNorm = 35.8861, GNorm = 2.1943, lr_0 = 3.7468e-04 Loss = 2.7460e-01, PNorm = 35.9150, GNorm = 1.5343, lr_0 = 3.6517e-04 Validation auc = 0.873620 47%|████▋ | 14/30 [04:58&lt;05:34, 20.89s/it]Epoch 14 Loss = 2.0399e-01, PNorm = 35.9476, GNorm = 1.4632, lr_0 = 3.5500e-04 Loss = 2.7089e-01, PNorm = 35.9726, GNorm = 1.6253, lr_0 = 3.4599e-04 Loss = 2.3401e-01, PNorm = 35.9997, GNorm = 0.7775, lr_0 = 3.3721e-04 Validation auc = 0.876425 50%|█████ | 15/30 [05:19&lt;05:11, 20.78s/it]Epoch 15 Loss = 2.6768e-01, PNorm = 36.0240, GNorm = 0.9954, lr_0 = 3.2866e-04 Loss = 2.0685e-01, PNorm = 36.0548, GNorm = 1.8900, lr_0 = 3.2032e-04 Loss = 2.2412e-01, PNorm = 36.0833, GNorm = 0.5792, lr_0 = 3.1219e-04 Loss = 2.4476e-01, PNorm = 36.1085, GNorm = 1.6445, lr_0 = 3.0427e-04 Validation auc = 0.875596 53%|█████▎ | 16/30 [05:40&lt;04:53, 20.96s/it]Epoch 16 Loss = 2.2949e-01, PNorm = 36.1305, GNorm = 0.9145, lr_0 = 2.9579e-04 Loss = 1.9585e-01, PNorm = 36.1505, GNorm = 0.4529, lr_0 = 2.8828e-04 Loss = 2.7090e-01, PNorm = 36.1746, GNorm = 0.7945, lr_0 = 2.8097e-04 Validation auc = 0.877208 57%|█████▋ | 17/30 [06:04&lt;04:42, 21.73s/it]Epoch 17 Loss = 2.1816e-01, PNorm = 36.1949, GNorm = 2.0708, lr_0 = 2.7384e-04 Loss = 1.8441e-01, PNorm = 36.2158, GNorm = 1.0388, lr_0 = 2.6689e-04 Loss = 2.8637e-01, PNorm = 36.2391, GNorm = 0.9753, lr_0 = 2.6012e-04 Validation auc = 0.870138 60%|██████ | 18/30 [06:25&lt;04:17, 21.45s/it]Epoch 18 Loss = 1.5308e-01, PNorm = 36.2586, GNorm = 2.1514, lr_0 = 2.5352e-04 Loss = 2.2635e-01, PNorm = 36.2779, GNorm = 1.0403, lr_0 = 2.4709e-04 Loss = 2.2665e-01, PNorm = 36.2964, GNorm = 1.3590, lr_0 = 2.4082e-04 Validation auc = 0.869019 63%|██████▎ | 19/30 [06:45&lt;03:53, 21.22s/it]Epoch 19 Loss = 8.9551e-02, PNorm = 36.3192, GNorm = 0.7785, lr_0 = 2.3411e-04 Loss = 2.0486e-01, PNorm = 36.3410, GNorm = 0.7392, lr_0 = 2.2817e-04 Loss = 2.2939e-01, PNorm = 36.3640, GNorm = 1.2424, lr_0 = 2.2238e-04 Loss = 1.9926e-01, PNorm = 36.3786, GNorm = 0.7721, lr_0 = 2.1674e-04 Validation auc = 0.875981 67%|██████▋ | 20/30 [07:08&lt;03:37, 21.72s/it]Epoch 20 Loss = 2.0309e-01, PNorm = 36.3941, GNorm = 2.4386, lr_0 = 2.1124e-04 Loss = 2.0890e-01, PNorm = 36.4104, GNorm = 1.4285, lr_0 = 2.0588e-04 Loss = 1.8153e-01, PNorm = 36.4284, GNorm = 0.5590, lr_0 = 2.0066e-04 Validation auc = 0.870780 70%|███████ | 21/30 [07:30&lt;03:16, 21.84s/it]Epoch 21 Loss = 1.8481e-01, PNorm = 36.4460, GNorm = 2.7512, lr_0 = 1.9557e-04 Loss = 2.1980e-01, PNorm = 36.4644, GNorm = 2.2602, lr_0 = 1.9060e-04 Loss = 1.8008e-01, PNorm = 36.4805, GNorm = 0.9021, lr_0 = 1.8577e-04 Validation auc = 0.875258 73%|███████▎ | 22/30 [07:51&lt;02:51, 21.44s/it]Epoch 22 Loss = 1.8978e-01, PNorm = 36.4971, GNorm = 0.7574, lr_0 = 1.8059e-04 Loss = 1.9064e-01, PNorm = 36.5102, GNorm = 1.5119, lr_0 = 1.7601e-04 Loss = 2.1171e-01, PNorm = 36.5234, GNorm = 0.6926, lr_0 = 1.7154e-04 Loss = 1.9472e-01, PNorm = 36.5373, GNorm = 0.9849, lr_0 = 1.6719e-04 Validation auc = 0.881586 77%|███████▋ | 23/30 [08:14&lt;02:33, 21.89s/it]Epoch 23 Loss = 1.8850e-01, PNorm = 36.5525, GNorm = 1.5114, lr_0 = 1.6295e-04 Loss = 2.0301e-01, PNorm = 36.5675, GNorm = 1.0058, lr_0 = 1.5882e-04 Loss = 1.8228e-01, PNorm = 36.5804, GNorm = 1.7929, lr_0 = 1.5479e-04 Validation auc = 0.878990 80%|████████ | 24/30 [08:36&lt;02:11, 22.00s/it]Epoch 24 Loss = 1.9413e-01, PNorm = 36.5934, GNorm = 0.9976, lr_0 = 1.5047e-04 Loss = 2.0365e-01, PNorm = 36.6074, GNorm = 0.7346, lr_0 = 1.4665e-04 Loss = 1.8043e-01, PNorm = 36.6215, GNorm = 0.6577, lr_0 = 1.4293e-04 Validation auc = 0.873295 83%|████████▎ | 25/30 [08:59&lt;01:50, 22.16s/it]Epoch 25 Loss = 1.6548e-01, PNorm = 36.6328, GNorm = 0.7261, lr_0 = 1.3931e-04 Loss = 1.9304e-01, PNorm = 36.6451, GNorm = 1.2106, lr_0 = 1.3577e-04 Loss = 1.9536e-01, PNorm = 36.6578, GNorm = 0.9719, lr_0 = 1.3233e-04 Validation auc = 0.872888 87%|████████▋ | 26/30 [09:21&lt;01:28, 22.22s/it]Epoch 26 Loss = 2.1892e-01, PNorm = 36.6704, GNorm = 1.1834, lr_0 = 1.2897e-04 Loss = 1.8540e-01, PNorm = 36.6827, GNorm = 2.2023, lr_0 = 1.2570e-04 Loss = 1.8935e-01, PNorm = 36.6935, GNorm = 1.6809, lr_0 = 1.2251e-04 Loss = 1.7464e-01, PNorm = 36.7032, GNorm = 1.7633, lr_0 = 1.1940e-04 Loss = 1.8098e-01, PNorm = 36.7042, GNorm = 3.0041, lr_0 = 1.1909e-04 Validation auc = 0.881022 90%|█████████ | 27/30 [09:42&lt;01:05, 21.91s/it]Epoch 27 Loss = 1.7639e-01, PNorm = 36.7141, GNorm = 1.2014, lr_0 = 1.1607e-04 Loss = 1.8164e-01, PNorm = 36.7256, GNorm = 1.2400, lr_0 = 1.1313e-04 Loss = 1.7504e-01, PNorm = 36.7352, GNorm = 3.4003, lr_0 = 1.1026e-04 Validation auc = 0.877288 93%|█████████▎| 28/30 [10:03&lt;00:43, 21.74s/it]Epoch 28 Loss = 1.8956e-01, PNorm = 36.7454, GNorm = 2.1527, lr_0 = 1.0746e-04 Loss = 1.6665e-01, PNorm = 36.7553, GNorm = 1.3492, lr_0 = 1.0473e-04 Loss = 1.7506e-01, PNorm = 36.7639, GNorm = 1.7218, lr_0 = 1.0208e-04 Validation auc = 0.876728 97%|█████████▋| 29/30 [10:26&lt;00:21, 21.90s/it]Epoch 29 Loss = 1.2047e-01, PNorm = 36.7740, GNorm = 1.0475, lr_0 = 1.0000e-04 Loss = 2.1471e-01, PNorm = 36.7832, GNorm = 1.2062, lr_0 = 1.0000e-04 Loss = 1.7566e-01, PNorm = 36.7917, GNorm = 2.2096, lr_0 = 1.0000e-04 Validation auc = 0.881241 100%|██████████| 30/30 [10:49&lt;00:00, 21.65s/it] Model 0 best validation auc = 0.881586 on epoch 22 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.884451 Ensemble test auc = 0.884451 Fold 3 Splitting data with seed 3 7807it [00:00, 109858.36it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 190240.36it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 70268.85it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 3095.88it/s] Total scaffolds = 1,025 | train scaffolds = 783 | val scaffolds = 0 | test scaffolds = 242 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.8503e-01, PNorm = 34.0129, GNorm = 0.7183, lr_0 = 2.5469e-04 Loss = 5.3260e-01, PNorm = 34.0194, GNorm = 0.8494, lr_0 = 3.9531e-04 Loss = 5.2468e-01, PNorm = 34.0407, GNorm = 0.3495, lr_0 = 5.3594e-04 Validation auc = 0.753098 3%|▎ | 1/30 [00:23&lt;11:12, 23.20s/it]Epoch 1 Loss = 4.9305e-01, PNorm = 34.0837, GNorm = 1.5001, lr_0 = 6.9063e-04 Loss = 4.5417e-01, PNorm = 34.1229, GNorm = 1.2743, lr_0 = 8.3125e-04 Loss = 4.2560e-01, PNorm = 34.1709, GNorm = 0.3752, lr_0 = 9.7187e-04 Validation auc = 0.801727 7%|▋ | 2/30 [00:43&lt;10:02, 21.52s/it]Epoch 2 Loss = 4.1467e-01, PNorm = 34.2309, GNorm = 2.1363, lr_0 = 9.7965e-04 Loss = 3.7320e-01, PNorm = 34.2967, GNorm = 2.4100, lr_0 = 9.5480e-04 Loss = 4.0981e-01, PNorm = 34.3477, GNorm = 1.3296, lr_0 = 9.3057e-04 Validation auc = 0.817309 10%|█ | 3/30 [01:04&lt;09:32, 21.22s/it]Epoch 3 Loss = 3.8771e-01, PNorm = 34.4124, GNorm = 0.6317, lr_0 = 9.0463e-04 Loss = 3.9085e-01, PNorm = 34.4764, GNorm = 1.6561, lr_0 = 8.8168e-04 Loss = 3.4792e-01, PNorm = 34.5298, GNorm = 0.5630, lr_0 = 8.5931e-04 Loss = 3.5220e-01, PNorm = 34.5699, GNorm = 1.3693, lr_0 = 8.3751e-04 Validation auc = 0.809701 13%|█▎ | 4/30 [01:24&lt;08:57, 20.68s/it]Epoch 4 Loss = 3.3247e-01, PNorm = 34.6296, GNorm = 1.1420, lr_0 = 8.1626e-04 Loss = 3.5306e-01, PNorm = 34.6740, GNorm = 1.1846, lr_0 = 7.9555e-04 Loss = 3.3732e-01, PNorm = 34.7250, GNorm = 0.4723, lr_0 = 7.7537e-04 Validation auc = 0.844284 17%|█▋ | 5/30 [01:44&lt;08:31, 20.45s/it]Epoch 5 Loss = 3.3587e-01, PNorm = 34.7825, GNorm = 1.8932, lr_0 = 7.5570e-04 Loss = 3.0881e-01, PNorm = 34.8456, GNorm = 0.4694, lr_0 = 7.3653e-04 Loss = 3.7637e-01, PNorm = 34.8771, GNorm = 1.4360, lr_0 = 7.1784e-04 Validation auc = 0.850265 20%|██ | 6/30 [02:05&lt;08:13, 20.56s/it]Epoch 6 Loss = 2.9220e-01, PNorm = 34.9294, GNorm = 0.6241, lr_0 = 6.9783e-04 Loss = 3.2549e-01, PNorm = 34.9790, GNorm = 0.6818, lr_0 = 6.8013e-04 Loss = 3.0608e-01, PNorm = 35.0372, GNorm = 0.8883, lr_0 = 6.6287e-04 Validation auc = 0.849036 23%|██▎ | 7/30 [02:24&lt;07:47, 20.31s/it]Epoch 7 Loss = 2.5418e-01, PNorm = 35.0786, GNorm = 1.1208, lr_0 = 6.4605e-04 Loss = 2.9694e-01, PNorm = 35.1282, GNorm = 0.4616, lr_0 = 6.2966e-04 Loss = 3.2139e-01, PNorm = 35.1663, GNorm = 0.8886, lr_0 = 6.1369e-04 Loss = 3.0297e-01, PNorm = 35.2010, GNorm = 0.6191, lr_0 = 5.9812e-04 Loss = 3.7280e-01, PNorm = 35.2063, GNorm = 0.4405, lr_0 = 5.9658e-04 Validation auc = 0.852861 27%|██▋ | 8/30 [02:44&lt;07:24, 20.21s/it]Epoch 8 Loss = 2.8168e-01, PNorm = 35.2603, GNorm = 1.9431, lr_0 = 5.8145e-04 Loss = 3.2319e-01, PNorm = 35.3027, GNorm = 0.6268, lr_0 = 5.6669e-04 Loss = 2.8460e-01, PNorm = 35.3421, GNorm = 1.0085, lr_0 = 5.5232e-04 Validation auc = 0.869803 30%|███ | 9/30 [03:05&lt;07:09, 20.47s/it]Epoch 9 Loss = 2.4875e-01, PNorm = 35.3866, GNorm = 0.7201, lr_0 = 5.3830e-04 Loss = 2.9694e-01, PNorm = 35.4251, GNorm = 1.6812, lr_0 = 5.2465e-04 Loss = 2.7647e-01, PNorm = 35.4672, GNorm = 1.1003, lr_0 = 5.1133e-04 Validation auc = 0.866824 33%|███▎ | 10/30 [03:25&lt;06:45, 20.29s/it]Epoch 10 Loss = 2.1429e-01, PNorm = 35.5067, GNorm = 0.4265, lr_0 = 4.9836e-04 Loss = 2.6924e-01, PNorm = 35.5539, GNorm = 3.2518, lr_0 = 4.8572e-04 Loss = 3.3974e-01, PNorm = 35.5871, GNorm = 1.1372, lr_0 = 4.7339e-04 Validation auc = 0.865611 37%|███▋ | 11/30 [03:45&lt;06:22, 20.14s/it]Epoch 11 Loss = 2.5574e-01, PNorm = 35.6282, GNorm = 0.5900, lr_0 = 4.6020e-04 Loss = 2.7551e-01, PNorm = 35.6684, GNorm = 0.6836, lr_0 = 4.4852e-04 Loss = 2.5548e-01, PNorm = 35.7092, GNorm = 0.7596, lr_0 = 4.3714e-04 Loss = 2.6243e-01, PNorm = 35.7530, GNorm = 0.5971, lr_0 = 4.2605e-04 Validation auc = 0.854902 40%|████ | 12/30 [04:05&lt;06:02, 20.16s/it]Epoch 12 Loss = 2.7059e-01, PNorm = 35.7839, GNorm = 0.8614, lr_0 = 4.1524e-04 Loss = 2.6100e-01, PNorm = 35.8191, GNorm = 0.8910, lr_0 = 4.0471e-04 Loss = 2.3268e-01, PNorm = 35.8580, GNorm = 0.6772, lr_0 = 3.9444e-04 Validation auc = 0.879089 43%|████▎ | 13/30 [04:28&lt;05:53, 20.82s/it]Epoch 13 Loss = 2.4654e-01, PNorm = 35.8996, GNorm = 2.3554, lr_0 = 3.8443e-04 Loss = 2.4650e-01, PNorm = 35.9410, GNorm = 0.6923, lr_0 = 3.7468e-04 Loss = 2.7544e-01, PNorm = 35.9681, GNorm = 0.8501, lr_0 = 3.6517e-04 Validation auc = 0.879367 47%|████▋ | 14/30 [04:49&lt;05:38, 21.13s/it]Epoch 14 Loss = 2.1829e-01, PNorm = 35.9958, GNorm = 1.1294, lr_0 = 3.5500e-04 Loss = 2.2900e-01, PNorm = 36.0258, GNorm = 1.1646, lr_0 = 3.4599e-04 Loss = 2.5168e-01, PNorm = 36.0620, GNorm = 0.7692, lr_0 = 3.3721e-04 Validation auc = 0.880691 50%|█████ | 15/30 [05:13&lt;05:26, 21.79s/it]Epoch 15 Loss = 1.7709e-01, PNorm = 36.0969, GNorm = 1.0523, lr_0 = 3.2866e-04 Loss = 2.4527e-01, PNorm = 36.1308, GNorm = 2.1124, lr_0 = 3.2032e-04 Loss = 2.2482e-01, PNorm = 36.1610, GNorm = 1.7217, lr_0 = 3.1219e-04 Loss = 2.3390e-01, PNorm = 36.1893, GNorm = 0.9631, lr_0 = 3.0427e-04 Validation auc = 0.880331 53%|█████▎ | 16/30 [05:37&lt;05:13, 22.39s/it]Epoch 16 Loss = 2.1933e-01, PNorm = 36.2226, GNorm = 1.0130, lr_0 = 2.9579e-04 Loss = 2.1466e-01, PNorm = 36.2504, GNorm = 1.1862, lr_0 = 2.8828e-04 Loss = 2.1801e-01, PNorm = 36.2805, GNorm = 1.3128, lr_0 = 2.8097e-04 Validation auc = 0.883562 57%|█████▋ | 17/30 [05:59&lt;04:52, 22.53s/it]Epoch 17 Loss = 1.9419e-01, PNorm = 36.3101, GNorm = 1.3975, lr_0 = 2.7384e-04 Loss = 2.0557e-01, PNorm = 36.3417, GNorm = 0.8939, lr_0 = 2.6689e-04 Loss = 2.3192e-01, PNorm = 36.3687, GNorm = 2.0555, lr_0 = 2.6012e-04 Validation auc = 0.886957 60%|██████ | 18/30 [06:21&lt;04:25, 22.12s/it]Epoch 18 Loss = 2.3539e-01, PNorm = 36.3916, GNorm = 0.8720, lr_0 = 2.5352e-04 Loss = 2.1929e-01, PNorm = 36.4164, GNorm = 0.7058, lr_0 = 2.4709e-04 Loss = 2.4068e-01, PNorm = 36.4369, GNorm = 3.0150, lr_0 = 2.4082e-04 Validation auc = 0.881972 63%|██████▎ | 19/30 [06:42&lt;04:02, 22.01s/it]Epoch 19 Loss = 2.8110e-01, PNorm = 36.4620, GNorm = 0.9777, lr_0 = 2.3411e-04 Loss = 1.8928e-01, PNorm = 36.4869, GNorm = 0.7610, lr_0 = 2.2817e-04 Loss = 1.8999e-01, PNorm = 36.5149, GNorm = 1.1548, lr_0 = 2.2238e-04 Loss = 2.0293e-01, PNorm = 36.5373, GNorm = 0.7328, lr_0 = 2.1674e-04 Validation auc = 0.882238 67%|██████▋ | 20/30 [07:03&lt;03:37, 21.71s/it]Epoch 20 Loss = 1.4837e-01, PNorm = 36.5575, GNorm = 0.9761, lr_0 = 2.1124e-04 Loss = 2.4855e-01, PNorm = 36.5775, GNorm = 1.1418, lr_0 = 2.0588e-04 Loss = 1.9571e-01, PNorm = 36.5983, GNorm = 1.6342, lr_0 = 2.0066e-04 Validation auc = 0.885651 70%|███████ | 21/30 [07:25&lt;03:15, 21.76s/it]Epoch 21 Loss = 1.8447e-01, PNorm = 36.6182, GNorm = 1.4701, lr_0 = 1.9557e-04 Loss = 2.0830e-01, PNorm = 36.6364, GNorm = 1.7001, lr_0 = 1.9060e-04 Loss = 1.9511e-01, PNorm = 36.6552, GNorm = 0.7673, lr_0 = 1.8577e-04 Validation auc = 0.881236 73%|███████▎ | 22/30 [07:48&lt;02:57, 22.19s/it]Epoch 22 Loss = 1.6973e-01, PNorm = 36.6757, GNorm = 0.9187, lr_0 = 1.8059e-04 Loss = 1.8659e-01, PNorm = 36.6936, GNorm = 1.1550, lr_0 = 1.7601e-04 Loss = 1.7931e-01, PNorm = 36.7113, GNorm = 1.1469, lr_0 = 1.7154e-04 Loss = 1.9458e-01, PNorm = 36.7288, GNorm = 0.9962, lr_0 = 1.6719e-04 Validation auc = 0.886035 77%|███████▋ | 23/30 [08:13&lt;02:41, 23.02s/it]Epoch 23 Loss = 1.6233e-01, PNorm = 36.7440, GNorm = 2.0711, lr_0 = 1.6295e-04 Loss = 1.8848e-01, PNorm = 36.7586, GNorm = 1.0342, lr_0 = 1.5882e-04 Loss = 2.2215e-01, PNorm = 36.7735, GNorm = 1.5878, lr_0 = 1.5479e-04 Validation auc = 0.884020 80%|████████ | 24/30 [08:37&lt;02:18, 23.16s/it]Epoch 24 Loss = 1.8076e-01, PNorm = 36.7899, GNorm = 1.7198, lr_0 = 1.5047e-04 Loss = 1.8257e-01, PNorm = 36.8053, GNorm = 1.0520, lr_0 = 1.4665e-04 Loss = 1.9706e-01, PNorm = 36.8194, GNorm = 1.0626, lr_0 = 1.4293e-04 Validation auc = 0.890895 83%|████████▎ | 25/30 [09:01&lt;01:56, 23.35s/it]Epoch 25 Loss = 2.0510e-01, PNorm = 36.8340, GNorm = 1.5704, lr_0 = 1.3931e-04 Loss = 1.6853e-01, PNorm = 36.8474, GNorm = 0.8417, lr_0 = 1.3577e-04 Loss = 1.5834e-01, PNorm = 36.8612, GNorm = 0.9323, lr_0 = 1.3233e-04 Validation auc = 0.891019 87%|████████▋ | 26/30 [09:25&lt;01:34, 23.74s/it]Epoch 26 Loss = 2.0799e-01, PNorm = 36.8742, GNorm = 0.9327, lr_0 = 1.2897e-04 Loss = 1.7609e-01, PNorm = 36.8876, GNorm = 0.8374, lr_0 = 1.2570e-04 Loss = 1.6518e-01, PNorm = 36.9010, GNorm = 1.6749, lr_0 = 1.2251e-04 Loss = 1.7412e-01, PNorm = 36.9125, GNorm = 1.5180, lr_0 = 1.1940e-04 Loss = 1.5249e-01, PNorm = 36.9135, GNorm = 1.7315, lr_0 = 1.1909e-04 Validation auc = 0.891827 90%|█████████ | 27/30 [09:48&lt;01:10, 23.50s/it]Epoch 27 Loss = 1.6977e-01, PNorm = 36.9245, GNorm = 2.7031, lr_0 = 1.1607e-04 Loss = 1.6511e-01, PNorm = 36.9350, GNorm = 1.1789, lr_0 = 1.1313e-04 Loss = 1.6014e-01, PNorm = 36.9464, GNorm = 1.8058, lr_0 = 1.1026e-04 Validation auc = 0.886515 93%|█████████▎| 28/30 [10:09&lt;00:45, 22.78s/it]Epoch 28 Loss = 1.6906e-01, PNorm = 36.9574, GNorm = 2.0566, lr_0 = 1.0746e-04 Loss = 1.5989e-01, PNorm = 36.9693, GNorm = 1.4375, lr_0 = 1.0473e-04 Loss = 1.5272e-01, PNorm = 36.9798, GNorm = 0.9655, lr_0 = 1.0208e-04 Validation auc = 0.889167 97%|█████████▋| 29/30 [10:31&lt;00:22, 22.55s/it]Epoch 29 Loss = 1.8119e-01, PNorm = 36.9902, GNorm = 2.0362, lr_0 = 1.0000e-04 Loss = 1.5775e-01, PNorm = 36.9985, GNorm = 0.7633, lr_0 = 1.0000e-04 Loss = 1.8003e-01, PNorm = 37.0084, GNorm = 1.5576, lr_0 = 1.0000e-04 Validation auc = 0.878503 100%|██████████| 30/30 [10:52&lt;00:00, 21.75s/it] Model 0 best validation auc = 0.891827 on epoch 26 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.893417 Ensemble test auc = 0.893417 Fold 4 Splitting data with seed 4 7807it [00:00, 100326.71it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 173326.97it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 70270.96it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 2503.17it/s] Total scaffolds = 1,025 | train scaffolds = 793 | val scaffolds = 0 | test scaffolds = 232 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.8904e-01, PNorm = 34.0130, GNorm = 0.4582, lr_0 = 2.5469e-04 Loss = 5.4910e-01, PNorm = 34.0167, GNorm = 0.3335, lr_0 = 3.9531e-04 Loss = 4.7079e-01, PNorm = 34.0347, GNorm = 0.4258, lr_0 = 5.3594e-04 Validation auc = 0.714421 3%|▎ | 1/30 [00:24&lt;11:44, 24.30s/it]Epoch 1 Loss = 4.4932e-01, PNorm = 34.0689, GNorm = 0.7341, lr_0 = 6.9063e-04 Loss = 4.5592e-01, PNorm = 34.1133, GNorm = 1.1783, lr_0 = 8.3125e-04 Loss = 4.6461e-01, PNorm = 34.1674, GNorm = 0.9487, lr_0 = 9.7187e-04 Validation auc = 0.802569 7%|▋ | 2/30 [00:47&lt;10:54, 23.37s/it]Epoch 2 Loss = 4.4770e-01, PNorm = 34.2328, GNorm = 0.9698, lr_0 = 9.7965e-04 Loss = 3.7590e-01, PNorm = 34.3134, GNorm = 1.4789, lr_0 = 9.5480e-04 Loss = 3.8262e-01, PNorm = 34.3785, GNorm = 0.3217, lr_0 = 9.3057e-04 Validation auc = 0.816338 10%|█ | 3/30 [01:07&lt;09:58, 22.15s/it]Epoch 3 Loss = 4.1084e-01, PNorm = 34.4491, GNorm = 1.3773, lr_0 = 9.0463e-04 Loss = 4.2045e-01, PNorm = 34.5011, GNorm = 2.9319, lr_0 = 8.8168e-04 Loss = 4.0727e-01, PNorm = 34.5599, GNorm = 0.2623, lr_0 = 8.5931e-04 Loss = 3.7995e-01, PNorm = 34.6398, GNorm = 0.6466, lr_0 = 8.3751e-04 Validation auc = 0.816479 13%|█▎ | 4/30 [01:28&lt;09:23, 21.66s/it]Epoch 4 Loss = 3.3837e-01, PNorm = 34.7114, GNorm = 0.8128, lr_0 = 8.1626e-04 Loss = 3.8992e-01, PNorm = 34.7621, GNorm = 1.0438, lr_0 = 7.9555e-04 Loss = 3.5696e-01, PNorm = 34.8062, GNorm = 1.3761, lr_0 = 7.7537e-04 Validation auc = 0.843436 17%|█▋ | 5/30 [01:50&lt;09:02, 21.68s/it]Epoch 5 Loss = 3.4266e-01, PNorm = 34.8671, GNorm = 0.6989, lr_0 = 7.5570e-04 Loss = 3.3413e-01, PNorm = 34.9265, GNorm = 0.7118, lr_0 = 7.3653e-04 Loss = 3.6780e-01, PNorm = 34.9783, GNorm = 0.8837, lr_0 = 7.1784e-04 Validation auc = 0.852989 20%|██ | 6/30 [02:11&lt;08:32, 21.36s/it]Epoch 6 Loss = 3.3458e-01, PNorm = 35.0243, GNorm = 1.1508, lr_0 = 6.9783e-04 Loss = 3.1381e-01, PNorm = 35.0788, GNorm = 0.8429, lr_0 = 6.8013e-04 Loss = 3.2376e-01, PNorm = 35.1350, GNorm = 0.7356, lr_0 = 6.6287e-04 Validation auc = 0.857785 23%|██▎ | 7/30 [02:32&lt;08:11, 21.37s/it]Epoch 7 Loss = 3.9145e-01, PNorm = 35.1828, GNorm = 0.7814, lr_0 = 6.4605e-04 Loss = 2.7528e-01, PNorm = 35.2190, GNorm = 0.8323, lr_0 = 6.2966e-04 Loss = 3.2363e-01, PNorm = 35.2726, GNorm = 1.8237, lr_0 = 6.1369e-04 Loss = 3.0749e-01, PNorm = 35.3255, GNorm = 2.0515, lr_0 = 5.9812e-04 Loss = 2.8228e-01, PNorm = 35.3293, GNorm = 0.7213, lr_0 = 5.9658e-04 Validation auc = 0.854700 27%|██▋ | 8/30 [02:56&lt;08:07, 22.17s/it]Epoch 8 Loss = 2.9411e-01, PNorm = 35.3831, GNorm = 1.5271, lr_0 = 5.8145e-04 Loss = 2.8510e-01, PNorm = 35.4336, GNorm = 0.5846, lr_0 = 5.6669e-04 Loss = 3.2167e-01, PNorm = 35.4799, GNorm = 0.7057, lr_0 = 5.5232e-04 Validation auc = 0.851165 30%|███ | 9/30 [03:19&lt;07:54, 22.59s/it]Epoch 9 Loss = 2.6575e-01, PNorm = 35.5124, GNorm = 0.7704, lr_0 = 5.3830e-04 Loss = 2.6473e-01, PNorm = 35.5572, GNorm = 0.5866, lr_0 = 5.2465e-04 Loss = 2.9093e-01, PNorm = 35.6069, GNorm = 0.7128, lr_0 = 5.1133e-04 Validation auc = 0.857636 33%|███▎ | 10/30 [03:43&lt;07:39, 22.98s/it]Epoch 10 Loss = 3.5403e-01, PNorm = 35.6530, GNorm = 0.7657, lr_0 = 4.9836e-04 Loss = 2.5887e-01, PNorm = 35.6959, GNorm = 0.6837, lr_0 = 4.8572e-04 Loss = 2.5052e-01, PNorm = 35.7403, GNorm = 0.5641, lr_0 = 4.7339e-04 Validation auc = 0.863423 37%|███▋ | 11/30 [04:09&lt;07:33, 23.88s/it]Epoch 11 Loss = 2.7768e-01, PNorm = 35.7913, GNorm = 1.3297, lr_0 = 4.6020e-04 Loss = 2.3540e-01, PNorm = 35.8298, GNorm = 0.5965, lr_0 = 4.4852e-04 Loss = 2.5308e-01, PNorm = 35.8713, GNorm = 1.3339, lr_0 = 4.3714e-04 Loss = 2.8692e-01, PNorm = 35.9113, GNorm = 0.5342, lr_0 = 4.2605e-04 Validation auc = 0.871476 40%|████ | 12/30 [04:32&lt;07:03, 23.50s/it]Epoch 12 Loss = 2.5221e-01, PNorm = 35.9515, GNorm = 0.6847, lr_0 = 4.1524e-04 Loss = 2.4157e-01, PNorm = 35.9993, GNorm = 0.9547, lr_0 = 4.0471e-04 Loss = 2.6724e-01, PNorm = 36.0337, GNorm = 0.9547, lr_0 = 3.9444e-04 Validation auc = 0.876039 43%|████▎ | 13/30 [04:55&lt;06:40, 23.55s/it]Epoch 13 Loss = 2.1849e-01, PNorm = 36.0666, GNorm = 1.0429, lr_0 = 3.8443e-04 Loss = 2.6876e-01, PNorm = 36.1041, GNorm = 0.6180, lr_0 = 3.7468e-04 Loss = 2.5427e-01, PNorm = 36.1407, GNorm = 1.7197, lr_0 = 3.6517e-04 Validation auc = 0.873085 47%|████▋ | 14/30 [05:20&lt;06:22, 23.91s/it]Epoch 14 Loss = 2.1584e-01, PNorm = 36.1751, GNorm = 0.7762, lr_0 = 3.5500e-04 Loss = 2.4688e-01, PNorm = 36.2082, GNorm = 1.1125, lr_0 = 3.4599e-04 Loss = 2.1242e-01, PNorm = 36.2445, GNorm = 1.2601, lr_0 = 3.3721e-04 Validation auc = 0.864683 50%|█████ | 15/30 [05:44&lt;05:59, 23.95s/it]Epoch 15 Loss = 1.7947e-01, PNorm = 36.2788, GNorm = 0.4194, lr_0 = 3.2866e-04 Loss = 2.2289e-01, PNorm = 36.3048, GNorm = 0.6260, lr_0 = 3.2032e-04 Loss = 2.6165e-01, PNorm = 36.3329, GNorm = 0.6240, lr_0 = 3.1219e-04 Loss = 2.2531e-01, PNorm = 36.3659, GNorm = 0.7357, lr_0 = 3.0427e-04 Validation auc = 0.881949 53%|█████▎ | 16/30 [06:06&lt;05:25, 23.25s/it]Epoch 16 Loss = 2.4698e-01, PNorm = 36.4025, GNorm = 0.8394, lr_0 = 2.9579e-04 Loss = 2.1054e-01, PNorm = 36.4361, GNorm = 0.5998, lr_0 = 2.8828e-04 Loss = 2.1337e-01, PNorm = 36.4674, GNorm = 1.2679, lr_0 = 2.8097e-04 Validation auc = 0.875559 57%|█████▋ | 17/30 [06:27&lt;04:54, 22.65s/it]Epoch 17 Loss = 2.0708e-01, PNorm = 36.4913, GNorm = 0.9110, lr_0 = 2.7384e-04 Loss = 1.9125e-01, PNorm = 36.5212, GNorm = 0.8134, lr_0 = 2.6689e-04 Loss = 2.1184e-01, PNorm = 36.5505, GNorm = 0.6213, lr_0 = 2.6012e-04 Validation auc = 0.881293 60%|██████ | 18/30 [06:50&lt;04:31, 22.64s/it]Epoch 18 Loss = 1.7150e-01, PNorm = 36.5733, GNorm = 1.0104, lr_0 = 2.5352e-04 Loss = 2.0804e-01, PNorm = 36.5983, GNorm = 0.9529, lr_0 = 2.4709e-04 Loss = 2.2783e-01, PNorm = 36.6194, GNorm = 1.5317, lr_0 = 2.4082e-04 Validation auc = 0.883125 63%|██████▎ | 19/30 [07:13&lt;04:12, 22.98s/it]Epoch 19 Loss = 2.8598e-01, PNorm = 36.6457, GNorm = 2.1299, lr_0 = 2.3411e-04 Loss = 2.0476e-01, PNorm = 36.6710, GNorm = 0.8491, lr_0 = 2.2817e-04 Loss = 1.9721e-01, PNorm = 36.6963, GNorm = 2.5710, lr_0 = 2.2238e-04 Loss = 2.0929e-01, PNorm = 36.7201, GNorm = 0.7773, lr_0 = 2.1674e-04 Validation auc = 0.881723 67%|██████▋ | 20/30 [07:35&lt;03:46, 22.61s/it]Epoch 20 Loss = 2.0260e-01, PNorm = 36.7383, GNorm = 1.5371, lr_0 = 2.1124e-04 Loss = 2.1149e-01, PNorm = 36.7575, GNorm = 2.3478, lr_0 = 2.0588e-04 Loss = 2.0423e-01, PNorm = 36.7785, GNorm = 1.1346, lr_0 = 2.0066e-04 Validation auc = 0.883188 70%|███████ | 21/30 [07:57&lt;03:20, 22.26s/it]Epoch 21 Loss = 1.5742e-01, PNorm = 36.8023, GNorm = 1.2067, lr_0 = 1.9557e-04 Loss = 1.9617e-01, PNorm = 36.8247, GNorm = 1.3102, lr_0 = 1.9060e-04 Loss = 2.2532e-01, PNorm = 36.8430, GNorm = 2.1962, lr_0 = 1.8577e-04 Validation auc = 0.885361 73%|███████▎ | 22/30 [08:18&lt;02:55, 21.97s/it]Epoch 22 Loss = 1.3227e-01, PNorm = 36.8661, GNorm = 0.5254, lr_0 = 1.8059e-04 Loss = 2.1334e-01, PNorm = 36.8861, GNorm = 0.9261, lr_0 = 1.7601e-04 Loss = 2.0413e-01, PNorm = 36.9028, GNorm = 1.7435, lr_0 = 1.7154e-04 Loss = 1.6772e-01, PNorm = 36.9223, GNorm = 0.8143, lr_0 = 1.6719e-04 Validation auc = 0.884290 77%|███████▋ | 23/30 [08:39&lt;02:32, 21.77s/it]Epoch 23 Loss = 1.9666e-01, PNorm = 36.9405, GNorm = 0.9696, lr_0 = 1.6295e-04 Loss = 1.7601e-01, PNorm = 36.9563, GNorm = 1.4962, lr_0 = 1.5882e-04 Loss = 2.0059e-01, PNorm = 36.9728, GNorm = 1.8332, lr_0 = 1.5479e-04 Validation auc = 0.882020 80%|████████ | 24/30 [09:01&lt;02:10, 21.68s/it]Epoch 24 Loss = 1.9019e-01, PNorm = 36.9908, GNorm = 1.6233, lr_0 = 1.5047e-04 Loss = 1.5421e-01, PNorm = 37.0075, GNorm = 0.9650, lr_0 = 1.4665e-04 Loss = 1.8616e-01, PNorm = 37.0246, GNorm = 0.6858, lr_0 = 1.4293e-04 Validation auc = 0.884285 83%|████████▎ | 25/30 [09:25&lt;01:52, 22.59s/it]Epoch 25 Loss = 1.7820e-01, PNorm = 37.0395, GNorm = 1.5652, lr_0 = 1.3931e-04 Loss = 1.6335e-01, PNorm = 37.0534, GNorm = 1.0124, lr_0 = 1.3577e-04 Loss = 1.9755e-01, PNorm = 37.0685, GNorm = 0.6697, lr_0 = 1.3233e-04 Validation auc = 0.881236 87%|████████▋ | 26/30 [09:46&lt;01:28, 22.05s/it]Epoch 26 Loss = 2.5348e-01, PNorm = 37.0818, GNorm = 1.0460, lr_0 = 1.2897e-04 Loss = 1.5218e-01, PNorm = 37.0944, GNorm = 1.1583, lr_0 = 1.2570e-04 Loss = 1.8730e-01, PNorm = 37.1069, GNorm = 1.1171, lr_0 = 1.2251e-04 Loss = 1.9544e-01, PNorm = 37.1206, GNorm = 3.5098, lr_0 = 1.1940e-04 Loss = 2.2225e-01, PNorm = 37.1214, GNorm = 2.6071, lr_0 = 1.1909e-04 Validation auc = 0.883871 90%|█████████ | 27/30 [10:09&lt;01:06, 22.21s/it]Epoch 27 Loss = 1.7266e-01, PNorm = 37.1319, GNorm = 1.5730, lr_0 = 1.1607e-04 Loss = 1.9641e-01, PNorm = 37.1425, GNorm = 1.2776, lr_0 = 1.1313e-04 Loss = 1.6487e-01, PNorm = 37.1540, GNorm = 1.2483, lr_0 = 1.1026e-04 Validation auc = 0.882866 93%|█████████▎| 28/30 [10:31&lt;00:44, 22.31s/it]Epoch 28 Loss = 1.7875e-01, PNorm = 37.1672, GNorm = 1.7522, lr_0 = 1.0746e-04 Loss = 1.7473e-01, PNorm = 37.1794, GNorm = 0.7878, lr_0 = 1.0473e-04 Loss = 1.6981e-01, PNorm = 37.1904, GNorm = 1.3535, lr_0 = 1.0208e-04 Validation auc = 0.887590 97%|█████████▋| 29/30 [10:54&lt;00:22, 22.54s/it]Epoch 29 Loss = 1.6088e-01, PNorm = 37.2021, GNorm = 1.2300, lr_0 = 1.0000e-04 Loss = 1.4213e-01, PNorm = 37.2127, GNorm = 1.5068, lr_0 = 1.0000e-04 Loss = 1.8074e-01, PNorm = 37.2235, GNorm = 1.3147, lr_0 = 1.0000e-04 Validation auc = 0.884358 100%|██████████| 30/30 [11:18&lt;00:00, 22.61s/it] Model 0 best validation auc = 0.887590 on epoch 28 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.938451 Ensemble test auc = 0.938451 Fold 5 Splitting data with seed 5 7807it [00:00, 104000.37it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 162498.60it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 69025.63it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 2255.84it/s] Total scaffolds = 1,025 | train scaffolds = 906 | val scaffolds = 0 | test scaffolds = 119 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.8033e-01, PNorm = 34.0138, GNorm = 1.1787, lr_0 = 2.5469e-04 Loss = 5.4678e-01, PNorm = 34.0184, GNorm = 1.1154, lr_0 = 3.9531e-04 Loss = 5.5692e-01, PNorm = 34.0360, GNorm = 0.9338, lr_0 = 5.3594e-04 Validation auc = 0.735027 3%|▎ | 1/30 [00:25&lt;12:33, 25.97s/it]Epoch 1 Loss = 4.5970e-01, PNorm = 34.0777, GNorm = 2.2193, lr_0 = 6.9063e-04 Loss = 4.5579e-01, PNorm = 34.1152, GNorm = 2.7257, lr_0 = 8.3125e-04 Loss = 4.9135e-01, PNorm = 34.1634, GNorm = 0.7147, lr_0 = 9.7187e-04 Validation auc = 0.766746 7%|▋ | 2/30 [00:48&lt;11:14, 24.07s/it]Epoch 2 Loss = 4.1898e-01, PNorm = 34.2314, GNorm = 0.5118, lr_0 = 9.7965e-04 Loss = 4.3117e-01, PNorm = 34.2876, GNorm = 1.2363, lr_0 = 9.5480e-04 Loss = 3.9515e-01, PNorm = 34.3500, GNorm = 0.7560, lr_0 = 9.3057e-04 Validation auc = 0.804363 10%|█ | 3/30 [01:10&lt;10:16, 22.85s/it]Epoch 3 Loss = 4.2600e-01, PNorm = 34.4209, GNorm = 1.2319, lr_0 = 9.0463e-04 Loss = 3.3194e-01, PNorm = 34.4872, GNorm = 1.0340, lr_0 = 8.8168e-04 Loss = 3.8497e-01, PNorm = 34.5392, GNorm = 2.0303, lr_0 = 8.5931e-04 Loss = 4.1747e-01, PNorm = 34.5918, GNorm = 0.6238, lr_0 = 8.3751e-04 Validation auc = 0.794418 13%|█▎ | 4/30 [01:32&lt;09:51, 22.74s/it]Epoch 4 Loss = 3.7177e-01, PNorm = 34.6527, GNorm = 0.3393, lr_0 = 8.1626e-04 Loss = 3.2056e-01, PNorm = 34.7074, GNorm = 0.4336, lr_0 = 7.9555e-04 Loss = 3.7145e-01, PNorm = 34.7541, GNorm = 0.9259, lr_0 = 7.7537e-04 Validation auc = 0.834360 17%|█▋ | 5/30 [01:54&lt;09:19, 22.38s/it]Epoch 5 Loss = 3.1982e-01, PNorm = 34.8056, GNorm = 0.6751, lr_0 = 7.5570e-04 Loss = 3.2307e-01, PNorm = 34.8663, GNorm = 2.1552, lr_0 = 7.3653e-04 Loss = 3.6569e-01, PNorm = 34.9066, GNorm = 1.9811, lr_0 = 7.1784e-04 Validation auc = 0.837094 20%|██ | 6/30 [02:15&lt;08:47, 21.97s/it]Epoch 6 Loss = 3.7645e-01, PNorm = 34.9541, GNorm = 1.1249, lr_0 = 6.9783e-04 Loss = 2.9699e-01, PNorm = 35.0068, GNorm = 0.6845, lr_0 = 6.8013e-04 Loss = 3.3965e-01, PNorm = 35.0486, GNorm = 0.4158, lr_0 = 6.6287e-04 Validation auc = 0.842136 23%|██▎ | 7/30 [02:37&lt;08:22, 21.84s/it]Epoch 7 Loss = 3.6432e-01, PNorm = 35.0918, GNorm = 1.0273, lr_0 = 6.4605e-04 Loss = 2.7651e-01, PNorm = 35.1428, GNorm = 0.3162, lr_0 = 6.2966e-04 Loss = 3.2439e-01, PNorm = 35.1988, GNorm = 1.1205, lr_0 = 6.1369e-04 Loss = 3.3336e-01, PNorm = 35.2405, GNorm = 1.3062, lr_0 = 5.9812e-04 Loss = 3.3097e-01, PNorm = 35.2434, GNorm = 1.5771, lr_0 = 5.9658e-04 Validation auc = 0.849479 27%|██▋ | 8/30 [03:00&lt;08:07, 22.16s/it]Epoch 8 Loss = 3.1454e-01, PNorm = 35.2786, GNorm = 0.5499, lr_0 = 5.8145e-04 Loss = 2.8740e-01, PNorm = 35.3219, GNorm = 1.2451, lr_0 = 5.6669e-04 Loss = 3.2447e-01, PNorm = 35.3711, GNorm = 0.4027, lr_0 = 5.5232e-04 Validation auc = 0.853164 30%|███ | 9/30 [03:22&lt;07:47, 22.25s/it]Epoch 9 Loss = 2.6941e-01, PNorm = 35.4252, GNorm = 0.6284, lr_0 = 5.3830e-04 Loss = 2.5306e-01, PNorm = 35.4747, GNorm = 1.4621, lr_0 = 5.2465e-04 Loss = 2.8457e-01, PNorm = 35.5056, GNorm = 0.9449, lr_0 = 5.1133e-04 Validation auc = 0.852232 33%|███▎ | 10/30 [03:45&lt;07:32, 22.62s/it]Epoch 10 Loss = 3.5473e-01, PNorm = 35.5534, GNorm = 0.9740, lr_0 = 4.9836e-04 Loss = 2.5513e-01, PNorm = 35.5959, GNorm = 1.0464, lr_0 = 4.8572e-04 Loss = 2.3035e-01, PNorm = 35.6407, GNorm = 0.5048, lr_0 = 4.7339e-04 Validation auc = 0.873011 37%|███▋ | 11/30 [04:08&lt;07:11, 22.70s/it]Epoch 11 Loss = 2.1947e-01, PNorm = 35.6786, GNorm = 1.5859, lr_0 = 4.6020e-04 Loss = 2.9966e-01, PNorm = 35.7161, GNorm = 1.6161, lr_0 = 4.4852e-04 Loss = 2.3920e-01, PNorm = 35.7500, GNorm = 0.8140, lr_0 = 4.3714e-04 Loss = 2.5769e-01, PNorm = 35.7887, GNorm = 2.0964, lr_0 = 4.2605e-04 Validation auc = 0.876054 40%|████ | 12/30 [04:32&lt;06:54, 23.01s/it]Epoch 12 Loss = 2.6451e-01, PNorm = 35.8290, GNorm = 1.9923, lr_0 = 4.1524e-04 Loss = 2.9541e-01, PNorm = 35.8587, GNorm = 0.7680, lr_0 = 4.0471e-04 Loss = 2.3624e-01, PNorm = 35.8921, GNorm = 0.6564, lr_0 = 3.9444e-04 Validation auc = 0.872729 43%|████▎ | 13/30 [04:55&lt;06:33, 23.14s/it]Epoch 13 Loss = 2.4811e-01, PNorm = 35.9334, GNorm = 2.9048, lr_0 = 3.8443e-04 Loss = 2.4003e-01, PNorm = 35.9732, GNorm = 0.8903, lr_0 = 3.7468e-04 Loss = 3.1994e-01, PNorm = 36.0036, GNorm = 0.4857, lr_0 = 3.6517e-04 Validation auc = 0.873944 47%|████▋ | 14/30 [05:19&lt;06:11, 23.19s/it]Epoch 14 Loss = 2.3606e-01, PNorm = 36.0327, GNorm = 0.9651, lr_0 = 3.5500e-04 Loss = 2.1234e-01, PNorm = 36.0662, GNorm = 0.8828, lr_0 = 3.4599e-04 Loss = 2.6915e-01, PNorm = 36.0970, GNorm = 1.4575, lr_0 = 3.3721e-04 Validation auc = 0.868701 50%|█████ | 15/30 [05:40&lt;05:38, 22.59s/it]Epoch 15 Loss = 3.0209e-01, PNorm = 36.1242, GNorm = 1.8489, lr_0 = 3.2866e-04 Loss = 2.2004e-01, PNorm = 36.1516, GNorm = 0.9104, lr_0 = 3.2032e-04 Loss = 2.0303e-01, PNorm = 36.1804, GNorm = 0.7067, lr_0 = 3.1219e-04 Loss = 2.5464e-01, PNorm = 36.2061, GNorm = 0.9119, lr_0 = 3.0427e-04 Validation auc = 0.876898 53%|█████▎ | 16/30 [06:03&lt;05:19, 22.79s/it]Epoch 16 Loss = 2.1173e-01, PNorm = 36.2318, GNorm = 1.0047, lr_0 = 2.9579e-04 Loss = 2.3335e-01, PNorm = 36.2596, GNorm = 1.0415, lr_0 = 2.8828e-04 Loss = 2.5902e-01, PNorm = 36.2869, GNorm = 0.9950, lr_0 = 2.8097e-04 Validation auc = 0.875209 57%|█████▋ | 17/30 [06:25&lt;04:53, 22.54s/it]Epoch 17 Loss = 1.9664e-01, PNorm = 36.3125, GNorm = 1.1060, lr_0 = 2.7384e-04 Loss = 2.0317e-01, PNorm = 36.3333, GNorm = 2.0615, lr_0 = 2.6689e-04 Loss = 2.3142e-01, PNorm = 36.3524, GNorm = 1.6721, lr_0 = 2.6012e-04 Validation auc = 0.876019 60%|██████ | 18/30 [06:46&lt;04:25, 22.09s/it]Epoch 18 Loss = 1.6227e-01, PNorm = 36.3788, GNorm = 0.7457, lr_0 = 2.5352e-04 Loss = 2.0263e-01, PNorm = 36.4048, GNorm = 1.6899, lr_0 = 2.4709e-04 Loss = 1.5962e-01, PNorm = 36.4262, GNorm = 1.3443, lr_0 = 2.4082e-04 Validation auc = 0.874317 63%|██████▎ | 19/30 [07:08&lt;04:01, 21.98s/it]Epoch 19 Loss = 2.8802e-01, PNorm = 36.4475, GNorm = 1.4766, lr_0 = 2.3411e-04 Loss = 2.0611e-01, PNorm = 36.4682, GNorm = 0.6141, lr_0 = 2.2817e-04 Loss = 1.7577e-01, PNorm = 36.4882, GNorm = 1.0868, lr_0 = 2.2238e-04 Loss = 2.1099e-01, PNorm = 36.5076, GNorm = 1.9374, lr_0 = 2.1674e-04 Validation auc = 0.872743 67%|██████▋ | 20/30 [07:30&lt;03:39, 21.90s/it]Epoch 20 Loss = 2.2023e-01, PNorm = 36.5294, GNorm = 0.7959, lr_0 = 2.1124e-04 Loss = 1.7023e-01, PNorm = 36.5468, GNorm = 1.8680, lr_0 = 2.0588e-04 Loss = 2.1346e-01, PNorm = 36.5662, GNorm = 1.5853, lr_0 = 2.0066e-04 Validation auc = 0.876977 70%|███████ | 21/30 [07:53&lt;03:21, 22.43s/it]Epoch 21 Loss = 1.6756e-01, PNorm = 36.5847, GNorm = 0.5861, lr_0 = 1.9557e-04 Loss = 2.1823e-01, PNorm = 36.6040, GNorm = 1.5526, lr_0 = 1.9060e-04 Loss = 1.6360e-01, PNorm = 36.6209, GNorm = 1.1944, lr_0 = 1.8577e-04 Validation auc = 0.879484 73%|███████▎ | 22/30 [08:20&lt;03:08, 23.59s/it]Epoch 22 Loss = 1.9874e-01, PNorm = 36.6384, GNorm = 1.3396, lr_0 = 1.8059e-04 Loss = 1.5375e-01, PNorm = 36.6547, GNorm = 0.9037, lr_0 = 1.7601e-04 Loss = 1.8212e-01, PNorm = 36.6717, GNorm = 0.6833, lr_0 = 1.7154e-04 Loss = 2.2151e-01, PNorm = 36.6873, GNorm = 1.0785, lr_0 = 1.6719e-04 Validation auc = 0.880958 77%|███████▋ | 23/30 [08:46&lt;02:51, 24.47s/it]Epoch 23 Loss = 1.9193e-01, PNorm = 36.7019, GNorm = 1.5677, lr_0 = 1.6295e-04 Loss = 1.9108e-01, PNorm = 36.7153, GNorm = 1.1084, lr_0 = 1.5882e-04 Loss = 1.5103e-01, PNorm = 36.7265, GNorm = 1.1219, lr_0 = 1.5479e-04 Validation auc = 0.882930 80%|████████ | 24/30 [09:14&lt;02:32, 25.36s/it]Epoch 24 Loss = 1.3201e-01, PNorm = 36.7415, GNorm = 0.7428, lr_0 = 1.5047e-04 Loss = 1.8692e-01, PNorm = 36.7554, GNorm = 2.0888, lr_0 = 1.4665e-04 Loss = 1.9702e-01, PNorm = 36.7685, GNorm = 1.5603, lr_0 = 1.4293e-04 Validation auc = 0.880481 83%|████████▎ | 25/30 [09:39&lt;02:06, 25.25s/it]Epoch 25 Loss = 1.1132e-01, PNorm = 36.7826, GNorm = 1.9829, lr_0 = 1.3931e-04 Loss = 1.5786e-01, PNorm = 36.7952, GNorm = 0.5368, lr_0 = 1.3577e-04 Loss = 1.6230e-01, PNorm = 36.8090, GNorm = 0.6102, lr_0 = 1.3233e-04 Validation auc = 0.879268 87%|████████▋ | 26/30 [10:00&lt;01:36, 24.15s/it]Epoch 26 Loss = 2.0914e-01, PNorm = 36.8220, GNorm = 0.9017, lr_0 = 1.2897e-04 Loss = 2.0112e-01, PNorm = 36.8320, GNorm = 2.0825, lr_0 = 1.2570e-04 Loss = 1.5139e-01, PNorm = 36.8427, GNorm = 2.4840, lr_0 = 1.2251e-04 Loss = 1.6615e-01, PNorm = 36.8539, GNorm = 2.2876, lr_0 = 1.1940e-04 Loss = 1.4737e-01, PNorm = 36.8552, GNorm = 4.3809, lr_0 = 1.1909e-04 Validation auc = 0.880099 90%|█████████ | 27/30 [10:23&lt;01:10, 23.64s/it]Epoch 27 Loss = 1.8118e-01, PNorm = 36.8667, GNorm = 0.7281, lr_0 = 1.1607e-04 Loss = 1.6857e-01, PNorm = 36.8781, GNorm = 1.6661, lr_0 = 1.1313e-04 Loss = 1.3214e-01, PNorm = 36.8884, GNorm = 1.2085, lr_0 = 1.1026e-04 Validation auc = 0.878571 93%|█████████▎| 28/30 [10:44&lt;00:45, 22.87s/it]Epoch 28 Loss = 1.7943e-01, PNorm = 36.8972, GNorm = 1.5619, lr_0 = 1.0746e-04 Loss = 1.6334e-01, PNorm = 36.9072, GNorm = 0.7111, lr_0 = 1.0473e-04 Loss = 1.4617e-01, PNorm = 36.9182, GNorm = 1.1304, lr_0 = 1.0208e-04 Validation auc = 0.880136 97%|█████████▋| 29/30 [11:05&lt;00:22, 22.26s/it]Epoch 29 Loss = 1.4805e-01, PNorm = 36.9280, GNorm = 1.3807, lr_0 = 1.0000e-04 Loss = 1.6974e-01, PNorm = 36.9364, GNorm = 0.6806, lr_0 = 1.0000e-04 Loss = 1.7505e-01, PNorm = 36.9447, GNorm = 1.3101, lr_0 = 1.0000e-04 Validation auc = 0.878255 100%|██████████| 30/30 [11:26&lt;00:00, 22.87s/it] Model 0 best validation auc = 0.882930 on epoch 23 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.876687 Ensemble test auc = 0.876687 Fold 6 Splitting data with seed 6 7807it [00:00, 105404.74it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 199995.92it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 66009.49it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 2258.47it/s] Total scaffolds = 1,025 | train scaffolds = 833 | val scaffolds = 0 | test scaffolds = 192 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.6182e-01, PNorm = 34.0132, GNorm = 0.6161, lr_0 = 2.5469e-04 Loss = 5.4864e-01, PNorm = 34.0188, GNorm = 0.4329, lr_0 = 3.9531e-04 Loss = 5.0555e-01, PNorm = 34.0376, GNorm = 0.5773, lr_0 = 5.3594e-04 Validation auc = 0.720839 3%|▎ | 1/30 [00:21&lt;10:13, 21.16s/it]Epoch 1 Loss = 4.5161e-01, PNorm = 34.0805, GNorm = 1.8178, lr_0 = 6.9063e-04 Loss = 4.9292e-01, PNorm = 34.1068, GNorm = 3.3809, lr_0 = 8.3125e-04 Loss = 4.5214e-01, PNorm = 34.1450, GNorm = 0.6624, lr_0 = 9.7187e-04 Validation auc = 0.795613 7%|▋ | 2/30 [00:41&lt;09:43, 20.84s/it]Epoch 2 Loss = 4.6454e-01, PNorm = 34.1995, GNorm = 0.4109, lr_0 = 9.7965e-04 Loss = 3.9795e-01, PNorm = 34.2431, GNorm = 2.3008, lr_0 = 9.5480e-04 Loss = 3.5796e-01, PNorm = 34.2947, GNorm = 1.4976, lr_0 = 9.3057e-04 Validation auc = 0.817488 10%|█ | 3/30 [01:03&lt;09:29, 21.10s/it]Epoch 3 Loss = 4.2133e-01, PNorm = 34.3367, GNorm = 2.0550, lr_0 = 9.0463e-04 Loss = 4.6422e-01, PNorm = 34.3847, GNorm = 0.4307, lr_0 = 8.8168e-04 Loss = 4.3600e-01, PNorm = 34.4464, GNorm = 1.3578, lr_0 = 8.5931e-04 Loss = 3.7713e-01, PNorm = 34.5002, GNorm = 0.3976, lr_0 = 8.3751e-04 Validation auc = 0.814387 13%|█▎ | 4/30 [01:23&lt;08:59, 20.74s/it]Epoch 4 Loss = 3.5375e-01, PNorm = 34.5522, GNorm = 1.5074, lr_0 = 8.1626e-04 Loss = 3.5896e-01, PNorm = 34.5938, GNorm = 0.5127, lr_0 = 7.9555e-04 Loss = 3.8334e-01, PNorm = 34.6308, GNorm = 1.0827, lr_0 = 7.7537e-04 Validation auc = 0.827489 17%|█▋ | 5/30 [01:43&lt;08:35, 20.62s/it]Epoch 5 Loss = 3.1088e-01, PNorm = 34.6880, GNorm = 0.9628, lr_0 = 7.5570e-04 Loss = 4.0120e-01, PNorm = 34.7423, GNorm = 0.4730, lr_0 = 7.3653e-04 Loss = 3.4849e-01, PNorm = 34.7869, GNorm = 0.4235, lr_0 = 7.1784e-04 Validation auc = 0.848541 20%|██ | 6/30 [02:04&lt;08:12, 20.52s/it]Epoch 6 Loss = 3.2776e-01, PNorm = 34.8429, GNorm = 2.2564, lr_0 = 6.9783e-04 Loss = 3.5925e-01, PNorm = 34.8844, GNorm = 1.3872, lr_0 = 6.8013e-04 Loss = 3.3250e-01, PNorm = 34.9391, GNorm = 0.3400, lr_0 = 6.6287e-04 Validation auc = 0.846812 23%|██▎ | 7/30 [02:25&lt;07:55, 20.69s/it]Epoch 7 Loss = 3.3242e-01, PNorm = 34.9893, GNorm = 0.5063, lr_0 = 6.4605e-04 Loss = 2.7401e-01, PNorm = 35.0454, GNorm = 0.8332, lr_0 = 6.2966e-04 Loss = 3.7172e-01, PNorm = 35.0911, GNorm = 0.8606, lr_0 = 6.1369e-04 Loss = 3.2611e-01, PNorm = 35.1325, GNorm = 0.6904, lr_0 = 5.9812e-04 Loss = 2.9493e-01, PNorm = 35.1374, GNorm = 0.4848, lr_0 = 5.9658e-04 Validation auc = 0.854277 27%|██▋ | 8/30 [02:45&lt;07:33, 20.61s/it]Epoch 8 Loss = 3.5267e-01, PNorm = 35.1835, GNorm = 0.5040, lr_0 = 5.8145e-04 Loss = 2.9611e-01, PNorm = 35.2405, GNorm = 1.5063, lr_0 = 5.6669e-04 Loss = 2.7431e-01, PNorm = 35.2899, GNorm = 1.9948, lr_0 = 5.5232e-04 Validation auc = 0.848035 30%|███ | 9/30 [03:05&lt;07:09, 20.46s/it]Epoch 9 Loss = 3.3042e-01, PNorm = 35.3289, GNorm = 0.5380, lr_0 = 5.3830e-04 Loss = 3.1613e-01, PNorm = 35.3795, GNorm = 1.5997, lr_0 = 5.2465e-04 Loss = 3.1015e-01, PNorm = 35.4274, GNorm = 0.5534, lr_0 = 5.1133e-04 Validation auc = 0.867727 33%|███▎ | 10/30 [03:27&lt;06:54, 20.75s/it]Epoch 10 Loss = 2.6779e-01, PNorm = 35.4678, GNorm = 0.4061, lr_0 = 4.9836e-04 Loss = 2.7977e-01, PNorm = 35.5126, GNorm = 1.1212, lr_0 = 4.8572e-04 Loss = 3.2023e-01, PNorm = 35.5553, GNorm = 1.8822, lr_0 = 4.7339e-04 Validation auc = 0.864423 37%|███▋ | 11/30 [03:47&lt;06:30, 20.57s/it]Epoch 11 Loss = 3.2623e-01, PNorm = 35.5952, GNorm = 1.3172, lr_0 = 4.6020e-04 Loss = 2.6594e-01, PNorm = 35.6291, GNorm = 0.8160, lr_0 = 4.4852e-04 Loss = 2.7306e-01, PNorm = 35.6642, GNorm = 2.5083, lr_0 = 4.3714e-04 Loss = 2.7640e-01, PNorm = 35.7069, GNorm = 1.2440, lr_0 = 4.2605e-04 Validation auc = 0.867787 40%|████ | 12/30 [04:07&lt;06:10, 20.57s/it]Epoch 12 Loss = 2.8383e-01, PNorm = 35.7498, GNorm = 0.6595, lr_0 = 4.1524e-04 Loss = 2.6669e-01, PNorm = 35.7861, GNorm = 0.6838, lr_0 = 4.0471e-04 Loss = 2.6751e-01, PNorm = 35.8190, GNorm = 1.5112, lr_0 = 3.9444e-04 Validation auc = 0.872715 43%|████▎ | 13/30 [04:29&lt;05:57, 21.04s/it]Epoch 13 Loss = 2.4573e-01, PNorm = 35.8542, GNorm = 0.9518, lr_0 = 3.8443e-04 Loss = 2.5258e-01, PNorm = 35.8918, GNorm = 0.7385, lr_0 = 3.7468e-04 Loss = 2.7146e-01, PNorm = 35.9213, GNorm = 0.7374, lr_0 = 3.6517e-04 Validation auc = 0.869090 47%|████▋ | 14/30 [04:50&lt;05:33, 20.85s/it]Epoch 14 Loss = 2.2748e-01, PNorm = 35.9547, GNorm = 0.9609, lr_0 = 3.5500e-04 Loss = 2.0082e-01, PNorm = 35.9866, GNorm = 0.9581, lr_0 = 3.4599e-04 Loss = 2.7723e-01, PNorm = 36.0187, GNorm = 0.5026, lr_0 = 3.3721e-04 Validation auc = 0.868104 50%|█████ | 15/30 [05:11&lt;05:13, 20.92s/it]Epoch 15 Loss = 3.0154e-01, PNorm = 36.0463, GNorm = 1.4333, lr_0 = 3.2866e-04 Loss = 2.7512e-01, PNorm = 36.0731, GNorm = 1.4761, lr_0 = 3.2032e-04 Loss = 2.2682e-01, PNorm = 36.0994, GNorm = 0.5695, lr_0 = 3.1219e-04 Loss = 2.1509e-01, PNorm = 36.1298, GNorm = 1.5742, lr_0 = 3.0427e-04 Validation auc = 0.875658 53%|█████▎ | 16/30 [05:34&lt;05:03, 21.65s/it]Epoch 16 Loss = 2.4645e-01, PNorm = 36.1587, GNorm = 1.4000, lr_0 = 2.9579e-04 Loss = 2.1079e-01, PNorm = 36.1864, GNorm = 0.7697, lr_0 = 2.8828e-04 Loss = 2.3936e-01, PNorm = 36.2159, GNorm = 1.7381, lr_0 = 2.8097e-04 Validation auc = 0.876264 57%|█████▋ | 17/30 [05:56&lt;04:41, 21.64s/it]Epoch 17 Loss = 1.7706e-01, PNorm = 36.2408, GNorm = 1.4449, lr_0 = 2.7384e-04 Loss = 2.6606e-01, PNorm = 36.2667, GNorm = 1.2493, lr_0 = 2.6689e-04 Loss = 2.3528e-01, PNorm = 36.2863, GNorm = 0.5351, lr_0 = 2.6012e-04 Validation auc = 0.877279 60%|██████ | 18/30 [06:18&lt;04:21, 21.78s/it]Epoch 18 Loss = 2.2911e-01, PNorm = 36.3074, GNorm = 0.8576, lr_0 = 2.5352e-04 Loss = 2.5532e-01, PNorm = 36.3298, GNorm = 1.3049, lr_0 = 2.4709e-04 Loss = 2.0325e-01, PNorm = 36.3527, GNorm = 0.7950, lr_0 = 2.4082e-04 Validation auc = 0.881196 63%|██████▎ | 19/30 [06:40&lt;04:00, 21.90s/it]Epoch 19 Loss = 1.6380e-01, PNorm = 36.3749, GNorm = 1.2393, lr_0 = 2.3411e-04 Loss = 2.4719e-01, PNorm = 36.3915, GNorm = 1.5367, lr_0 = 2.2817e-04 Loss = 1.7361e-01, PNorm = 36.4108, GNorm = 1.9581, lr_0 = 2.2238e-04 Loss = 2.2462e-01, PNorm = 36.4303, GNorm = 1.4962, lr_0 = 2.1674e-04 Validation auc = 0.875450 67%|██████▋ | 20/30 [07:02&lt;03:38, 21.86s/it]Epoch 20 Loss = 1.9001e-01, PNorm = 36.4510, GNorm = 0.9956, lr_0 = 2.1124e-04 Loss = 2.1640e-01, PNorm = 36.4719, GNorm = 1.4993, lr_0 = 2.0588e-04 Loss = 1.8828e-01, PNorm = 36.4908, GNorm = 0.7877, lr_0 = 2.0066e-04 Validation auc = 0.881552 70%|███████ | 21/30 [07:23&lt;03:13, 21.52s/it]Epoch 21 Loss = 2.0541e-01, PNorm = 36.5054, GNorm = 1.2624, lr_0 = 1.9557e-04 Loss = 1.9978e-01, PNorm = 36.5225, GNorm = 0.9034, lr_0 = 1.9060e-04 Loss = 1.7407e-01, PNorm = 36.5379, GNorm = 1.3351, lr_0 = 1.8577e-04 Validation auc = 0.877829 73%|███████▎ | 22/30 [07:44&lt;02:50, 21.37s/it]Epoch 22 Loss = 1.7511e-01, PNorm = 36.5563, GNorm = 0.8774, lr_0 = 1.8059e-04 Loss = 1.5307e-01, PNorm = 36.5738, GNorm = 2.4033, lr_0 = 1.7601e-04 Loss = 2.4618e-01, PNorm = 36.5901, GNorm = 1.7060, lr_0 = 1.7154e-04 Loss = 1.7840e-01, PNorm = 36.6022, GNorm = 1.0139, lr_0 = 1.6719e-04 Validation auc = 0.878885 77%|███████▋ | 23/30 [08:05&lt;02:29, 21.30s/it]Epoch 23 Loss = 1.7602e-01, PNorm = 36.6166, GNorm = 2.6906, lr_0 = 1.6295e-04 Loss = 2.1315e-01, PNorm = 36.6297, GNorm = 3.1348, lr_0 = 1.5882e-04 Loss = 1.9032e-01, PNorm = 36.6446, GNorm = 1.4982, lr_0 = 1.5479e-04 Validation auc = 0.871594 80%|████████ | 24/30 [08:27&lt;02:09, 21.52s/it]Epoch 24 Loss = 2.0369e-01, PNorm = 36.6579, GNorm = 2.8376, lr_0 = 1.5047e-04 Loss = 2.0615e-01, PNorm = 36.6703, GNorm = 1.4416, lr_0 = 1.4665e-04 Loss = 2.0013e-01, PNorm = 36.6799, GNorm = 0.9871, lr_0 = 1.4293e-04 Validation auc = 0.876537 83%|████████▎ | 25/30 [08:47&lt;01:45, 21.13s/it]Epoch 25 Loss = 2.0016e-01, PNorm = 36.6933, GNorm = 2.2319, lr_0 = 1.3931e-04 Loss = 1.6291e-01, PNorm = 36.7061, GNorm = 0.7980, lr_0 = 1.3577e-04 Loss = 2.0881e-01, PNorm = 36.7186, GNorm = 1.1770, lr_0 = 1.3233e-04 Validation auc = 0.878787 87%|████████▋ | 26/30 [09:09&lt;01:25, 21.26s/it]Epoch 26 Loss = 2.2723e-01, PNorm = 36.7291, GNorm = 1.8979, lr_0 = 1.2897e-04 Loss = 1.6891e-01, PNorm = 36.7399, GNorm = 0.9638, lr_0 = 1.2570e-04 Loss = 1.4573e-01, PNorm = 36.7516, GNorm = 1.4638, lr_0 = 1.2251e-04 Loss = 2.0396e-01, PNorm = 36.7623, GNorm = 1.6277, lr_0 = 1.1940e-04 Loss = 1.6013e-01, PNorm = 36.7631, GNorm = 2.0542, lr_0 = 1.1909e-04 Validation auc = 0.880557 90%|█████████ | 27/30 [09:30&lt;01:04, 21.37s/it]Epoch 27 Loss = 1.8082e-01, PNorm = 36.7736, GNorm = 1.5187, lr_0 = 1.1607e-04 Loss = 1.5974e-01, PNorm = 36.7825, GNorm = 0.9081, lr_0 = 1.1313e-04 Loss = 2.1324e-01, PNorm = 36.7908, GNorm = 1.2305, lr_0 = 1.1026e-04 Validation auc = 0.881096 93%|█████████▎| 28/30 [09:51&lt;00:42, 21.08s/it]Epoch 28 Loss = 1.9306e-01, PNorm = 36.7998, GNorm = 1.4380, lr_0 = 1.0746e-04 Loss = 1.7735e-01, PNorm = 36.8071, GNorm = 2.7694, lr_0 = 1.0473e-04 Loss = 1.5423e-01, PNorm = 36.8164, GNorm = 1.1709, lr_0 = 1.0208e-04 Validation auc = 0.881583 97%|█████████▋| 29/30 [10:11&lt;00:20, 20.95s/it]Epoch 29 Loss = 1.5280e-01, PNorm = 36.8263, GNorm = 1.7311, lr_0 = 1.0000e-04 Loss = 1.2902e-01, PNorm = 36.8350, GNorm = 1.4067, lr_0 = 1.0000e-04 Loss = 2.0477e-01, PNorm = 36.8446, GNorm = 1.5457, lr_0 = 1.0000e-04 Validation auc = 0.876813 100%|██████████| 30/30 [10:33&lt;00:00, 21.12s/it] Model 0 best validation auc = 0.881583 on epoch 28 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.906706 Ensemble test auc = 0.906706 Fold 7 Splitting data with seed 7 7807it [00:00, 113042.58it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 177270.81it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 70908.09it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 2455.84it/s] Total scaffolds = 1,025 | train scaffolds = 863 | val scaffolds = 0 | test scaffolds = 162 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.9673e-01, PNorm = 34.0126, GNorm = 1.5348, lr_0 = 2.5469e-04 Loss = 4.9817e-01, PNorm = 34.0187, GNorm = 0.5062, lr_0 = 3.9531e-04 Loss = 4.7747e-01, PNorm = 34.0412, GNorm = 0.5017, lr_0 = 5.3594e-04 Validation auc = 0.755383 3%|▎ | 1/30 [00:21&lt;10:35, 21.92s/it]Epoch 1 Loss = 4.2505e-01, PNorm = 34.0760, GNorm = 0.6224, lr_0 = 6.9063e-04 Loss = 4.4959e-01, PNorm = 34.1135, GNorm = 1.4844, lr_0 = 8.3125e-04 Loss = 4.3547e-01, PNorm = 34.1558, GNorm = 1.4239, lr_0 = 9.7187e-04 Validation auc = 0.795760 7%|▋ | 2/30 [00:43&lt;10:08, 21.75s/it]Epoch 2 Loss = 3.7337e-01, PNorm = 34.2097, GNorm = 3.4275, lr_0 = 9.7965e-04 Loss = 4.2502e-01, PNorm = 34.2627, GNorm = 0.5118, lr_0 = 9.5480e-04 Loss = 3.8550e-01, PNorm = 34.3194, GNorm = 0.9283, lr_0 = 9.3057e-04 Validation auc = 0.815194 10%|█ | 3/30 [01:04&lt;09:33, 21.25s/it]Epoch 3 Loss = 4.2547e-01, PNorm = 34.3732, GNorm = 1.1318, lr_0 = 9.0463e-04 Loss = 3.7796e-01, PNorm = 34.4303, GNorm = 1.2964, lr_0 = 8.8168e-04 Loss = 3.7880e-01, PNorm = 34.4900, GNorm = 0.8707, lr_0 = 8.5931e-04 Loss = 4.1805e-01, PNorm = 34.5363, GNorm = 1.7581, lr_0 = 8.3751e-04 Validation auc = 0.832049 13%|█▎ | 4/30 [01:24&lt;09:05, 20.97s/it]Epoch 4 Loss = 3.8877e-01, PNorm = 34.5793, GNorm = 1.3541, lr_0 = 8.1626e-04 Loss = 3.3638e-01, PNorm = 34.6436, GNorm = 0.3741, lr_0 = 7.9555e-04 Loss = 3.2092e-01, PNorm = 34.6966, GNorm = 0.3562, lr_0 = 7.7537e-04 Validation auc = 0.814380 17%|█▋ | 5/30 [01:45&lt;08:39, 20.79s/it]Epoch 5 Loss = 3.5278e-01, PNorm = 34.7368, GNorm = 0.6656, lr_0 = 7.5570e-04 Loss = 3.2812e-01, PNorm = 34.7910, GNorm = 1.0830, lr_0 = 7.3653e-04 Loss = 3.5939e-01, PNorm = 34.8453, GNorm = 0.6897, lr_0 = 7.1784e-04 Validation auc = 0.846920 20%|██ | 6/30 [02:06&lt;08:22, 20.94s/it]Epoch 6 Loss = 3.1876e-01, PNorm = 34.9135, GNorm = 0.9794, lr_0 = 6.9783e-04 Loss = 3.0256e-01, PNorm = 34.9680, GNorm = 1.3468, lr_0 = 6.8013e-04 Loss = 3.1646e-01, PNorm = 35.0206, GNorm = 1.1073, lr_0 = 6.6287e-04 Validation auc = 0.854253 23%|██▎ | 7/30 [02:26&lt;07:57, 20.75s/it]Epoch 7 Loss = 2.7953e-01, PNorm = 35.0653, GNorm = 2.3032, lr_0 = 6.4605e-04 Loss = 3.2105e-01, PNorm = 35.1171, GNorm = 2.0512, lr_0 = 6.2966e-04 Loss = 3.0641e-01, PNorm = 35.1748, GNorm = 2.5586, lr_0 = 6.1369e-04 Loss = 3.7268e-01, PNorm = 35.2264, GNorm = 3.6927, lr_0 = 5.9812e-04 Loss = 4.5915e-01, PNorm = 35.2303, GNorm = 2.7772, lr_0 = 5.9658e-04 Validation auc = 0.855636 27%|██▋ | 8/30 [02:48&lt;07:39, 20.90s/it]Epoch 8 Loss = 3.4779e-01, PNorm = 35.2787, GNorm = 0.5849, lr_0 = 5.8145e-04 Loss = 3.4322e-01, PNorm = 35.3278, GNorm = 1.4557, lr_0 = 5.6669e-04 Loss = 2.9223e-01, PNorm = 35.3803, GNorm = 0.3259, lr_0 = 5.5232e-04 Validation auc = 0.857801 30%|███ | 9/30 [03:10&lt;07:27, 21.32s/it]Epoch 9 Loss = 3.3487e-01, PNorm = 35.4334, GNorm = 0.9138, lr_0 = 5.3830e-04 Loss = 2.4880e-01, PNorm = 35.4801, GNorm = 0.7187, lr_0 = 5.2465e-04 Loss = 2.8323e-01, PNorm = 35.5223, GNorm = 0.5790, lr_0 = 5.1133e-04 Validation auc = 0.863790 33%|███▎ | 10/30 [03:32&lt;07:09, 21.46s/it]Epoch 10 Loss = 2.5531e-01, PNorm = 35.5586, GNorm = 0.9180, lr_0 = 4.9836e-04 Loss = 2.7229e-01, PNorm = 35.5990, GNorm = 0.8368, lr_0 = 4.8572e-04 Loss = 3.0928e-01, PNorm = 35.6383, GNorm = 0.6118, lr_0 = 4.7339e-04 Validation auc = 0.867484 37%|███▋ | 11/30 [03:54&lt;06:53, 21.76s/it]Epoch 11 Loss = 3.7548e-01, PNorm = 35.6842, GNorm = 1.6515, lr_0 = 4.6020e-04 Loss = 2.4339e-01, PNorm = 35.7264, GNorm = 0.6287, lr_0 = 4.4852e-04 Loss = 3.0924e-01, PNorm = 35.7685, GNorm = 2.4178, lr_0 = 4.3714e-04 Loss = 2.9903e-01, PNorm = 35.8038, GNorm = 1.5291, lr_0 = 4.2605e-04 Validation auc = 0.867790 40%|████ | 12/30 [04:18&lt;06:42, 22.36s/it]Epoch 12 Loss = 2.5767e-01, PNorm = 35.8390, GNorm = 0.5539, lr_0 = 4.1524e-04 Loss = 2.7974e-01, PNorm = 35.8753, GNorm = 1.0584, lr_0 = 4.0471e-04 Loss = 2.9305e-01, PNorm = 35.9113, GNorm = 1.8703, lr_0 = 3.9444e-04 Validation auc = 0.872165 43%|████▎ | 13/30 [04:41&lt;06:25, 22.70s/it]Epoch 13 Loss = 2.6109e-01, PNorm = 35.9422, GNorm = 1.6873, lr_0 = 3.8443e-04 Loss = 2.4154e-01, PNorm = 35.9808, GNorm = 0.8882, lr_0 = 3.7468e-04 Loss = 2.8830e-01, PNorm = 36.0182, GNorm = 0.8957, lr_0 = 3.6517e-04 Validation auc = 0.867917 47%|████▋ | 14/30 [05:05&lt;06:09, 23.11s/it]Epoch 14 Loss = 2.1272e-01, PNorm = 36.0542, GNorm = 1.6273, lr_0 = 3.5500e-04 Loss = 2.5678e-01, PNorm = 36.0900, GNorm = 0.5082, lr_0 = 3.4599e-04 Loss = 2.4599e-01, PNorm = 36.1204, GNorm = 1.5795, lr_0 = 3.3721e-04 Validation auc = 0.869381 50%|█████ | 15/30 [05:31&lt;05:58, 23.87s/it]Epoch 15 Loss = 3.2694e-01, PNorm = 36.1468, GNorm = 0.9686, lr_0 = 3.2866e-04 Loss = 2.5300e-01, PNorm = 36.1784, GNorm = 1.2536, lr_0 = 3.2032e-04 Loss = 2.2225e-01, PNorm = 36.2105, GNorm = 0.8472, lr_0 = 3.1219e-04 Loss = 2.5585e-01, PNorm = 36.2379, GNorm = 0.7422, lr_0 = 3.0427e-04 Validation auc = 0.875142 53%|█████▎ | 16/30 [05:54&lt;05:32, 23.72s/it]Epoch 16 Loss = 1.9838e-01, PNorm = 36.2736, GNorm = 1.8348, lr_0 = 2.9579e-04 Loss = 2.7353e-01, PNorm = 36.2984, GNorm = 3.2385, lr_0 = 2.8828e-04 Loss = 2.2603e-01, PNorm = 36.3175, GNorm = 1.1745, lr_0 = 2.8097e-04 Validation auc = 0.876805 57%|█████▋ | 17/30 [06:19&lt;05:10, 23.90s/it]Epoch 17 Loss = 1.8614e-01, PNorm = 36.3490, GNorm = 0.6584, lr_0 = 2.7384e-04 Loss = 2.1150e-01, PNorm = 36.3826, GNorm = 0.5984, lr_0 = 2.6689e-04 Loss = 2.4958e-01, PNorm = 36.4098, GNorm = 1.3179, lr_0 = 2.6012e-04 Validation auc = 0.876469 60%|██████ | 18/30 [06:43&lt;04:49, 24.11s/it]Epoch 18 Loss = 2.3328e-01, PNorm = 36.4302, GNorm = 1.6945, lr_0 = 2.5352e-04 Loss = 1.9636e-01, PNorm = 36.4565, GNorm = 2.4367, lr_0 = 2.4709e-04 Loss = 2.3666e-01, PNorm = 36.4837, GNorm = 1.1126, lr_0 = 2.4082e-04 Validation auc = 0.874926 63%|██████▎ | 19/30 [07:07&lt;04:25, 24.11s/it]Epoch 19 Loss = 2.0342e-01, PNorm = 36.5117, GNorm = 0.5979, lr_0 = 2.3411e-04 Loss = 2.1825e-01, PNorm = 36.5342, GNorm = 1.6542, lr_0 = 2.2817e-04 Loss = 2.0701e-01, PNorm = 36.5546, GNorm = 0.8762, lr_0 = 2.2238e-04 Loss = 2.1182e-01, PNorm = 36.5790, GNorm = 0.7950, lr_0 = 2.1674e-04 Validation auc = 0.875871 67%|██████▋ | 20/30 [07:30&lt;03:56, 23.63s/it]Epoch 20 Loss = 2.1835e-01, PNorm = 36.6005, GNorm = 1.2816, lr_0 = 2.1124e-04 Loss = 1.8984e-01, PNorm = 36.6232, GNorm = 2.6008, lr_0 = 2.0588e-04 Loss = 2.3389e-01, PNorm = 36.6454, GNorm = 0.9049, lr_0 = 2.0066e-04 Validation auc = 0.877689 70%|███████ | 21/30 [07:53&lt;03:32, 23.63s/it]Epoch 21 Loss = 2.2350e-01, PNorm = 36.6651, GNorm = 1.5218, lr_0 = 1.9557e-04 Loss = 2.0463e-01, PNorm = 36.6865, GNorm = 1.1106, lr_0 = 1.9060e-04 Loss = 2.1103e-01, PNorm = 36.7023, GNorm = 1.3653, lr_0 = 1.8577e-04 Validation auc = 0.879357 73%|███████▎ | 22/30 [08:17&lt;03:07, 23.48s/it]Epoch 22 Loss = 1.7427e-01, PNorm = 36.7236, GNorm = 0.8589, lr_0 = 1.8059e-04 Loss = 2.2984e-01, PNorm = 36.7431, GNorm = 1.3823, lr_0 = 1.7601e-04 Loss = 1.7259e-01, PNorm = 36.7599, GNorm = 1.1141, lr_0 = 1.7154e-04 Loss = 2.1782e-01, PNorm = 36.7752, GNorm = 2.2384, lr_0 = 1.6719e-04 Validation auc = 0.880634 77%|███████▋ | 23/30 [08:40&lt;02:45, 23.58s/it]Epoch 23 Loss = 2.1012e-01, PNorm = 36.7903, GNorm = 1.9463, lr_0 = 1.6295e-04 Loss = 1.9743e-01, PNorm = 36.8055, GNorm = 2.6711, lr_0 = 1.5882e-04 Loss = 1.9569e-01, PNorm = 36.8225, GNorm = 0.8472, lr_0 = 1.5479e-04 Validation auc = 0.876119 80%|████████ | 24/30 [09:06&lt;02:24, 24.07s/it]Epoch 24 Loss = 1.6951e-01, PNorm = 36.8384, GNorm = 1.3965, lr_0 = 1.5047e-04 Loss = 2.0458e-01, PNorm = 36.8530, GNorm = 1.4777, lr_0 = 1.4665e-04 Loss = 2.1188e-01, PNorm = 36.8657, GNorm = 1.2790, lr_0 = 1.4293e-04 Validation auc = 0.880178 83%|████████▎ | 25/30 [09:30&lt;02:01, 24.26s/it]Epoch 25 Loss = 1.9248e-01, PNorm = 36.8803, GNorm = 1.0131, lr_0 = 1.3931e-04 Loss = 1.6660e-01, PNorm = 36.8954, GNorm = 1.3230, lr_0 = 1.3577e-04 Loss = 2.0059e-01, PNorm = 36.9087, GNorm = 0.5384, lr_0 = 1.3233e-04 Validation auc = 0.877855 87%|████████▋ | 26/30 [09:55&lt;01:38, 24.52s/it]Epoch 26 Loss = 8.8095e-02, PNorm = 36.9205, GNorm = 0.8437, lr_0 = 1.2897e-04 Loss = 2.1412e-01, PNorm = 36.9334, GNorm = 1.1261, lr_0 = 1.2570e-04 Loss = 1.9582e-01, PNorm = 36.9453, GNorm = 1.5304, lr_0 = 1.2251e-04 Loss = 1.9293e-01, PNorm = 36.9564, GNorm = 0.7216, lr_0 = 1.1940e-04 Loss = 1.0425e-01, PNorm = 36.9574, GNorm = 0.8109, lr_0 = 1.1909e-04 Validation auc = 0.880129 90%|█████████ | 27/30 [10:19&lt;01:12, 24.25s/it]Epoch 27 Loss = 1.5074e-01, PNorm = 36.9704, GNorm = 1.0898, lr_0 = 1.1607e-04 Loss = 2.0373e-01, PNorm = 36.9809, GNorm = 1.8491, lr_0 = 1.1313e-04 Loss = 1.7781e-01, PNorm = 36.9917, GNorm = 0.7160, lr_0 = 1.1026e-04 Validation auc = 0.880636 93%|█████████▎| 28/30 [10:45&lt;00:49, 24.80s/it]Epoch 28 Loss = 1.6330e-01, PNorm = 37.0031, GNorm = 1.8308, lr_0 = 1.0746e-04 Loss = 1.9872e-01, PNorm = 37.0131, GNorm = 1.0970, lr_0 = 1.0473e-04 Loss = 1.8052e-01, PNorm = 37.0237, GNorm = 2.1910, lr_0 = 1.0208e-04 Validation auc = 0.882866 97%|█████████▋| 29/30 [11:10&lt;00:24, 24.71s/it]Epoch 29 Loss = 1.5836e-01, PNorm = 37.0338, GNorm = 1.0602, lr_0 = 1.0000e-04 Loss = 1.5969e-01, PNorm = 37.0432, GNorm = 0.8004, lr_0 = 1.0000e-04 Loss = 1.7776e-01, PNorm = 37.0536, GNorm = 1.9000, lr_0 = 1.0000e-04 Validation auc = 0.880424 100%|██████████| 30/30 [11:35&lt;00:00, 23.19s/it] Model 0 best validation auc = 0.882866 on epoch 28 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.897564 Ensemble test auc = 0.897564 Fold 8 Splitting data with seed 8 7807it [00:00, 103998.71it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 132200.20it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 65545.84it/s] 100%|██████████| 2039/2039 [00:01&lt;00:00, 1973.98it/s] Total scaffolds = 1,025 | train scaffolds = 840 | val scaffolds = 0 | test scaffolds = 185 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.8649e-01, PNorm = 34.0131, GNorm = 0.7064, lr_0 = 2.5469e-04 Loss = 5.4676e-01, PNorm = 34.0175, GNorm = 0.6914, lr_0 = 3.9531e-04 Loss = 5.1271e-01, PNorm = 34.0337, GNorm = 1.0100, lr_0 = 5.3594e-04 Validation auc = 0.734512 3%|▎ | 1/30 [00:25&lt;12:24, 25.66s/it]Epoch 1 Loss = 4.5022e-01, PNorm = 34.0646, GNorm = 0.2913, lr_0 = 6.9063e-04 Loss = 5.3064e-01, PNorm = 34.1029, GNorm = 0.4107, lr_0 = 8.3125e-04 Loss = 4.2082e-01, PNorm = 34.1675, GNorm = 0.4234, lr_0 = 9.7187e-04 Validation auc = 0.799223 7%|▋ | 2/30 [00:49&lt;11:21, 24.34s/it]Epoch 2 Loss = 4.9131e-01, PNorm = 34.2294, GNorm = 3.6234, lr_0 = 9.7965e-04 Loss = 4.3415e-01, PNorm = 34.2835, GNorm = 1.6113, lr_0 = 9.5480e-04 Loss = 4.2086e-01, PNorm = 34.3419, GNorm = 0.5353, lr_0 = 9.3057e-04 Validation auc = 0.811972 10%|█ | 3/30 [01:11&lt;10:33, 23.46s/it]Epoch 3 Loss = 3.2139e-01, PNorm = 34.4167, GNorm = 1.3411, lr_0 = 9.0463e-04 Loss = 3.5758e-01, PNorm = 34.4715, GNorm = 0.6067, lr_0 = 8.8168e-04 Loss = 4.4127e-01, PNorm = 34.5163, GNorm = 0.3850, lr_0 = 8.5931e-04 Loss = 3.9899e-01, PNorm = 34.5728, GNorm = 1.0117, lr_0 = 8.3751e-04 Validation auc = 0.824736 13%|█▎ | 4/30 [01:34&lt;10:04, 23.25s/it]Epoch 4 Loss = 3.6112e-01, PNorm = 34.6351, GNorm = 0.3311, lr_0 = 8.1626e-04 Loss = 3.8032e-01, PNorm = 34.6835, GNorm = 0.7118, lr_0 = 7.9555e-04 Loss = 3.5960e-01, PNorm = 34.7333, GNorm = 0.5180, lr_0 = 7.7537e-04 Validation auc = 0.814832 17%|█▋ | 5/30 [01:57&lt;09:38, 23.14s/it]Epoch 5 Loss = 3.7811e-01, PNorm = 34.7901, GNorm = 0.4220, lr_0 = 7.5570e-04 Loss = 3.2375e-01, PNorm = 34.8457, GNorm = 0.3401, lr_0 = 7.3653e-04 Loss = 3.4894e-01, PNorm = 34.9037, GNorm = 0.3493, lr_0 = 7.1784e-04 Validation auc = 0.835459 20%|██ | 6/30 [02:20&lt;09:16, 23.20s/it]Epoch 6 Loss = 3.3213e-01, PNorm = 34.9734, GNorm = 0.4705, lr_0 = 6.9783e-04 Loss = 3.0268e-01, PNorm = 35.0354, GNorm = 0.6496, lr_0 = 6.8013e-04 Loss = 3.3222e-01, PNorm = 35.0953, GNorm = 0.4395, lr_0 = 6.6287e-04 Validation auc = 0.846089 23%|██▎ | 7/30 [02:44&lt;08:57, 23.38s/it]Epoch 7 Loss = 3.1684e-01, PNorm = 35.1481, GNorm = 0.3060, lr_0 = 6.4605e-04 Loss = 2.9548e-01, PNorm = 35.2107, GNorm = 0.6182, lr_0 = 6.2966e-04 Loss = 3.2000e-01, PNorm = 35.2648, GNorm = 1.1350, lr_0 = 6.1369e-04 Loss = 3.2855e-01, PNorm = 35.3141, GNorm = 1.7529, lr_0 = 5.9812e-04 Loss = 2.5370e-01, PNorm = 35.3196, GNorm = 0.6291, lr_0 = 5.9658e-04 Validation auc = 0.844342 27%|██▋ | 8/30 [03:06&lt;08:25, 22.97s/it]Epoch 8 Loss = 3.0773e-01, PNorm = 35.3746, GNorm = 0.6402, lr_0 = 5.8145e-04 Loss = 2.9196e-01, PNorm = 35.4290, GNorm = 1.5832, lr_0 = 5.6669e-04 Loss = 3.1052e-01, PNorm = 35.4759, GNorm = 1.6844, lr_0 = 5.5232e-04 Validation auc = 0.855234 30%|███ | 9/30 [03:30&lt;08:09, 23.32s/it]Epoch 9 Loss = 2.7800e-01, PNorm = 35.5202, GNorm = 1.4366, lr_0 = 5.3830e-04 Loss = 2.7946e-01, PNorm = 35.5743, GNorm = 0.9622, lr_0 = 5.2465e-04 Loss = 3.1169e-01, PNorm = 35.6304, GNorm = 1.8932, lr_0 = 5.1133e-04 Validation auc = 0.863928 33%|███▎ | 10/30 [03:54&lt;07:49, 23.46s/it]Epoch 10 Loss = 2.5310e-01, PNorm = 35.6694, GNorm = 0.6531, lr_0 = 4.9836e-04 Loss = 2.6976e-01, PNorm = 35.7218, GNorm = 0.8412, lr_0 = 4.8572e-04 Loss = 2.5824e-01, PNorm = 35.7754, GNorm = 1.0941, lr_0 = 4.7339e-04 Validation auc = 0.859331 37%|███▋ | 11/30 [04:17&lt;07:22, 23.30s/it]Epoch 11 Loss = 2.7343e-01, PNorm = 35.8134, GNorm = 0.7610, lr_0 = 4.6020e-04 Loss = 2.4235e-01, PNorm = 35.8557, GNorm = 0.8548, lr_0 = 4.4852e-04 Loss = 3.1253e-01, PNorm = 35.8929, GNorm = 0.4618, lr_0 = 4.3714e-04 Loss = 2.8434e-01, PNorm = 35.9287, GNorm = 0.6328, lr_0 = 4.2605e-04 Validation auc = 0.857417 40%|████ | 12/30 [04:39&lt;06:52, 22.89s/it]Epoch 12 Loss = 2.6903e-01, PNorm = 35.9740, GNorm = 0.7112, lr_0 = 4.1524e-04 Loss = 2.3436e-01, PNorm = 36.0131, GNorm = 1.8073, lr_0 = 4.0471e-04 Loss = 2.7593e-01, PNorm = 36.0518, GNorm = 1.3064, lr_0 = 3.9444e-04 Validation auc = 0.858900 43%|████▎ | 13/30 [05:01&lt;06:26, 22.73s/it]Epoch 13 Loss = 2.4428e-01, PNorm = 36.0851, GNorm = 2.4298, lr_0 = 3.8443e-04 Loss = 2.6900e-01, PNorm = 36.1198, GNorm = 0.5732, lr_0 = 3.7468e-04 Loss = 2.6997e-01, PNorm = 36.1496, GNorm = 0.9713, lr_0 = 3.6517e-04 Validation auc = 0.859750 47%|████▋ | 14/30 [05:24&lt;06:05, 22.86s/it]Epoch 14 Loss = 2.2789e-01, PNorm = 36.1912, GNorm = 1.6291, lr_0 = 3.5500e-04 Loss = 2.4061e-01, PNorm = 36.2352, GNorm = 0.7153, lr_0 = 3.4599e-04 Loss = 2.3716e-01, PNorm = 36.2668, GNorm = 0.7406, lr_0 = 3.3721e-04 Validation auc = 0.869236 50%|█████ | 15/30 [05:46&lt;05:39, 22.63s/it]Epoch 15 Loss = 2.6546e-01, PNorm = 36.2957, GNorm = 1.5526, lr_0 = 3.2866e-04 Loss = 2.3108e-01, PNorm = 36.3296, GNorm = 0.9983, lr_0 = 3.2032e-04 Loss = 2.2666e-01, PNorm = 36.3640, GNorm = 1.0721, lr_0 = 3.1219e-04 Loss = 2.1325e-01, PNorm = 36.3973, GNorm = 1.2604, lr_0 = 3.0427e-04 Validation auc = 0.867841 53%|█████▎ | 16/30 [06:10&lt;05:20, 22.91s/it]Epoch 16 Loss = 1.8263e-01, PNorm = 36.4279, GNorm = 0.5923, lr_0 = 2.9579e-04 Loss = 2.4053e-01, PNorm = 36.4532, GNorm = 0.9292, lr_0 = 2.8828e-04 Loss = 2.3996e-01, PNorm = 36.4802, GNorm = 1.2285, lr_0 = 2.8097e-04 Validation auc = 0.871301 57%|█████▋ | 17/30 [06:34&lt;05:04, 23.40s/it]Epoch 17 Loss = 2.0343e-01, PNorm = 36.5121, GNorm = 0.7900, lr_0 = 2.7384e-04 Loss = 1.5995e-01, PNorm = 36.5461, GNorm = 0.5904, lr_0 = 2.6689e-04 Loss = 2.3919e-01, PNorm = 36.5722, GNorm = 2.0980, lr_0 = 2.6012e-04 Validation auc = 0.864505 60%|██████ | 18/30 [06:57&lt;04:36, 23.07s/it]Epoch 18 Loss = 1.8148e-01, PNorm = 36.5922, GNorm = 1.5846, lr_0 = 2.5352e-04 Loss = 2.5366e-01, PNorm = 36.6149, GNorm = 0.7508, lr_0 = 2.4709e-04 Loss = 2.2465e-01, PNorm = 36.6443, GNorm = 0.6522, lr_0 = 2.4082e-04 Validation auc = 0.869615 63%|██████▎ | 19/30 [07:20&lt;04:13, 23.02s/it]Epoch 19 Loss = 2.4635e-01, PNorm = 36.6733, GNorm = 1.3489, lr_0 = 2.3411e-04 Loss = 2.3557e-01, PNorm = 36.6984, GNorm = 1.1359, lr_0 = 2.2817e-04 Loss = 1.6692e-01, PNorm = 36.7249, GNorm = 0.8156, lr_0 = 2.2238e-04 Loss = 2.0582e-01, PNorm = 36.7497, GNorm = 0.7859, lr_0 = 2.1674e-04 Validation auc = 0.870811 67%|██████▋ | 20/30 [07:43&lt;03:50, 23.03s/it]Epoch 20 Loss = 2.3595e-01, PNorm = 36.7687, GNorm = 2.1405, lr_0 = 2.1124e-04 Loss = 1.6743e-01, PNorm = 36.7912, GNorm = 1.8271, lr_0 = 2.0588e-04 Loss = 2.0801e-01, PNorm = 36.8151, GNorm = 1.2116, lr_0 = 2.0066e-04 Validation auc = 0.875582 70%|███████ | 21/30 [08:07&lt;03:31, 23.47s/it]Epoch 21 Loss = 1.6087e-01, PNorm = 36.8302, GNorm = 1.2805, lr_0 = 1.9557e-04 Loss = 2.2082e-01, PNorm = 36.8504, GNorm = 0.8873, lr_0 = 1.9060e-04 Loss = 1.8606e-01, PNorm = 36.8717, GNorm = 0.7627, lr_0 = 1.8577e-04 Validation auc = 0.877031 73%|███████▎ | 22/30 [08:30&lt;03:04, 23.11s/it]Epoch 22 Loss = 2.2158e-01, PNorm = 36.8915, GNorm = 2.4453, lr_0 = 1.8059e-04 Loss = 2.0853e-01, PNorm = 36.9117, GNorm = 0.8223, lr_0 = 1.7601e-04 Loss = 1.7528e-01, PNorm = 36.9325, GNorm = 2.4850, lr_0 = 1.7154e-04 Loss = 2.0993e-01, PNorm = 36.9486, GNorm = 2.8569, lr_0 = 1.6719e-04 Validation auc = 0.874757 77%|███████▋ | 23/30 [08:55&lt;02:45, 23.68s/it]Epoch 23 Loss = 1.4639e-01, PNorm = 36.9654, GNorm = 0.7697, lr_0 = 1.6295e-04 Loss = 1.9275e-01, PNorm = 36.9816, GNorm = 1.3974, lr_0 = 1.5882e-04 Loss = 2.1059e-01, PNorm = 36.9961, GNorm = 1.5485, lr_0 = 1.5479e-04 Validation auc = 0.875779 80%|████████ | 24/30 [09:20&lt;02:26, 24.34s/it]Epoch 24 Loss = 2.0678e-01, PNorm = 37.0133, GNorm = 1.3215, lr_0 = 1.5047e-04 Loss = 1.4689e-01, PNorm = 37.0280, GNorm = 1.1888, lr_0 = 1.4665e-04 Loss = 1.9324e-01, PNorm = 37.0431, GNorm = 1.3306, lr_0 = 1.4293e-04 Validation auc = 0.875084 83%|████████▎ | 25/30 [09:46&lt;02:02, 24.57s/it]Epoch 25 Loss = 1.6089e-01, PNorm = 37.0576, GNorm = 1.1166, lr_0 = 1.3931e-04 Loss = 1.6333e-01, PNorm = 37.0707, GNorm = 1.0544, lr_0 = 1.3577e-04 Loss = 1.8117e-01, PNorm = 37.0841, GNorm = 1.3057, lr_0 = 1.3233e-04 Validation auc = 0.874240 87%|████████▋ | 26/30 [10:10&lt;01:38, 24.53s/it]Epoch 26 Loss = 2.0404e-01, PNorm = 37.0970, GNorm = 2.9506, lr_0 = 1.2897e-04 Loss = 1.8924e-01, PNorm = 37.1089, GNorm = 1.3185, lr_0 = 1.2570e-04 Loss = 1.6666e-01, PNorm = 37.1222, GNorm = 0.9292, lr_0 = 1.2251e-04 Loss = 1.9692e-01, PNorm = 37.1330, GNorm = 2.2530, lr_0 = 1.1940e-04 Loss = 8.1838e-02, PNorm = 37.1342, GNorm = 1.3591, lr_0 = 1.1909e-04 Validation auc = 0.876953 90%|█████████ | 27/30 [10:36&lt;01:14, 25.00s/it]Epoch 27 Loss = 1.8200e-01, PNorm = 37.1469, GNorm = 1.7932, lr_0 = 1.1607e-04 Loss = 1.7436e-01, PNorm = 37.1568, GNorm = 1.1590, lr_0 = 1.1313e-04 Loss = 1.4737e-01, PNorm = 37.1697, GNorm = 0.6555, lr_0 = 1.1026e-04 Validation auc = 0.876077 93%|█████████▎| 28/30 [11:00&lt;00:49, 24.64s/it]Epoch 28 Loss = 1.1868e-01, PNorm = 37.1807, GNorm = 0.7642, lr_0 = 1.0746e-04 Loss = 1.6770e-01, PNorm = 37.1898, GNorm = 0.9152, lr_0 = 1.0473e-04 Loss = 1.7687e-01, PNorm = 37.1999, GNorm = 1.1660, lr_0 = 1.0208e-04 Validation auc = 0.875336 97%|█████████▋| 29/30 [11:22&lt;00:23, 23.79s/it]Epoch 29 Loss = 1.6006e-01, PNorm = 37.2104, GNorm = 2.7978, lr_0 = 1.0000e-04 Loss = 1.6464e-01, PNorm = 37.2212, GNorm = 1.4714, lr_0 = 1.0000e-04 Loss = 1.8973e-01, PNorm = 37.2316, GNorm = 2.1347, lr_0 = 1.0000e-04 Validation auc = 0.876871 100%|██████████| 30/30 [11:44&lt;00:00, 23.47s/it] Model 0 best validation auc = 0.877031 on epoch 21 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.899712 Ensemble test auc = 0.899712 Fold 9 Splitting data with seed 9 7807it [00:00, 102630.67it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 152942.94it/s] 100%|██████████| 7807/7807 [00:00&lt;00:00, 68781.90it/s] 100%|██████████| 2039/2039 [00:00&lt;00:00, 2724.10it/s] Total scaffolds = 1,025 | train scaffolds = 822 | val scaffolds = 0 | test scaffolds = 203 Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: Scaffold 0 Task 0: count = 137 | target average = 0.729927 Scaffold 1 Task 0: count = 99 | target average = 0.929293 Scaffold 2 Task 0: count = 76 | target average = 0.947368 Scaffold 3 Task 0: count = 26 | target average = 1.000000 Scaffold 4 Task 0: count = 26 | target average = 1.000000 Scaffold 5 Task 0: count = 24 | target average = 0.666667 Scaffold 6 Task 0: count = 21 | target average = 0.952381 Scaffold 7 Task 0: count = 18 | target average = 1.000000 Scaffold 8 Task 0: count = 17 | target average = 0.294118 Scaffold 9 Task 0: count = 16 | target average = 0.750000 Class sizes BBB+/BBB- 0: 23.49%, 1: 76.51% Total size = 2,039 | train size = 1,631 | val size = 7,807 | test size = 408 Building model 0 MoleculeModel( (sigmoid): Sigmoid() (encoder): MPN( (encoder): ModuleList( (0): MPNEncoder( (dropout_layer): Dropout(p=0.0, inplace=False) (act_func): ReLU() (W_i): Linear(in_features=147, out_features=300, bias=False) (W_h): Linear(in_features=300, out_features=300, bias=False) (W_o): Linear(in_features=433, out_features=300, bias=True) ) ) ) (ffn): Sequential( (0): Dropout(p=0.0, inplace=False) (1): Linear(in_features=300, out_features=300, bias=True) (2): ReLU() (3): Dropout(p=0.0, inplace=False) (4): Linear(in_features=300, out_features=1, bias=True) ) ) Number of parameters = 355,201 0%| | 0/30 [00:00&lt;?, ?it/s]Epoch 0 Loss = 5.7355e-01, PNorm = 34.0135, GNorm = 1.3898, lr_0 = 2.5469e-04 Loss = 5.4916e-01, PNorm = 34.0176, GNorm = 1.3625, lr_0 = 3.9531e-04 Loss = 5.2713e-01, PNorm = 34.0341, GNorm = 0.1727, lr_0 = 5.3594e-04 Validation auc = 0.731387 3%|▎ | 1/30 [00:27&lt;13:22, 27.68s/it]Epoch 1 Loss = 5.6181e-01, PNorm = 34.0599, GNorm = 1.3737, lr_0 = 6.9063e-04 Loss = 5.4009e-01, PNorm = 34.0979, GNorm = 0.8545, lr_0 = 8.3125e-04 Loss = 4.9376e-01, PNorm = 34.1536, GNorm = 0.5686, lr_0 = 9.7187e-04 Validation auc = 0.761204 7%|▋ | 2/30 [00:51&lt;11:45, 25.20s/it]Epoch 2 Loss = 5.3742e-01, PNorm = 34.2259, GNorm = 1.5691, lr_0 = 9.7965e-04 Loss = 3.9386e-01, PNorm = 34.3099, GNorm = 0.8096, lr_0 = 9.5480e-04 Loss = 4.4236e-01, PNorm = 34.3557, GNorm = 0.5507, lr_0 = 9.3057e-04 Validation auc = 0.791568 10%|█ | 3/30 [01:16&lt;11:16, 25.06s/it]Epoch 3 Loss = 4.2308e-01, PNorm = 34.4201, GNorm = 0.4710, lr_0 = 9.0463e-04 Loss = 4.2625e-01, PNorm = 34.4839, GNorm = 2.3323, lr_0 = 8.8168e-04 Loss = 3.4713e-01, PNorm = 34.5529, GNorm = 1.4637, lr_0 = 8.5931e-04 Loss = 4.2077e-01, PNorm = 34.5964, GNorm = 0.6571, lr_0 = 8.3751e-04 Validation auc = 0.827755 13%|█▎ | 4/30 [01:38&lt;10:28, 24.19s/it]Epoch 4 Loss = 3.3712e-01, PNorm = 34.6475, GNorm = 1.0306, lr_0 = 8.1626e-04 Loss = 3.2588e-01, PNorm = 34.7061, GNorm = 1.6170, lr_0 = 7.9555e-04 Loss = 3.8492e-01, PNorm = 34.7481, GNorm = 0.3718, lr_0 = 7.7537e-04 Validation auc = 0.828054 17%|█▋ | 5/30 [02:02&lt;09:56, 23.87s/it]Epoch 5 Loss = 3.4264e-01, PNorm = 34.8038, GNorm = 0.4273, lr_0 = 7.5570e-04 Loss = 3.2844e-01, PNorm = 34.8629, GNorm = 1.2101, lr_0 = 7.3653e-04 Loss = 3.4978e-01, PNorm = 34.9220, GNorm = 1.1866, lr_0 = 7.1784e-04 Validation auc = 0.851047 20%|██ | 6/30 [02:24&lt;09:18, 23.29s/it]Epoch 6 Loss = 3.2186e-01, PNorm = 34.9997, GNorm = 0.7869, lr_0 = 6.9783e-04 Loss = 3.2372e-01, PNorm = 35.0719, GNorm = 0.4749, lr_0 = 6.8013e-04 Loss = 3.1195e-01, PNorm = 35.1244, GNorm = 2.0203, lr_0 = 6.6287e-04 Validation auc = 0.852341 23%|██▎ | 7/30 [02:47&lt;08:58, 23.40s/it]Epoch 7 Loss = 3.0974e-01, PNorm = 35.1811, GNorm = 1.7987, lr_0 = 6.4605e-04 Loss = 3.0368e-01, PNorm = 35.2501, GNorm = 1.4095, lr_0 = 6.2966e-04 Loss = 3.7589e-01, PNorm = 35.2954, GNorm = 2.1109, lr_0 = 6.1369e-04 Loss = 3.6816e-01, PNorm = 35.3463, GNorm = 0.7726, lr_0 = 5.9812e-04 Loss = 3.9080e-01, PNorm = 35.3516, GNorm = 0.4996, lr_0 = 5.9658e-04 Validation auc = 0.845782 27%|██▋ | 8/30 [03:10&lt;08:27, 23.08s/it]Epoch 8 Loss = 3.5167e-01, PNorm = 35.4028, GNorm = 0.4615, lr_0 = 5.8145e-04 Loss = 2.8388e-01, PNorm = 35.4565, GNorm = 0.9019, lr_0 = 5.6669e-04 Loss = 3.5399e-01, PNorm = 35.4984, GNorm = 0.5972, lr_0 = 5.5232e-04 Validation auc = 0.856509 30%|███ | 9/30 [03:33&lt;08:04, 23.07s/it]Epoch 9 Loss = 2.4704e-01, PNorm = 35.5443, GNorm = 0.9954, lr_0 = 5.3830e-04 Loss = 3.2154e-01, PNorm = 35.5865, GNorm = 1.9169, lr_0 = 5.2465e-04 Loss = 2.9224e-01, PNorm = 35.6308, GNorm = 0.8374, lr_0 = 5.1133e-04 Validation auc = 0.865598 33%|███▎ | 10/30 [03:58&lt;07:53, 23.69s/it]Epoch 10 Loss = 3.0247e-01, PNorm = 35.6612, GNorm = 0.8315, lr_0 = 4.9836e-04 Loss = 2.7513e-01, PNorm = 35.6949, GNorm = 1.1238, lr_0 = 4.8572e-04 Loss = 2.6106e-01, PNorm = 35.7404, GNorm = 1.6288, lr_0 = 4.7339e-04 Validation auc = 0.869175 37%|███▋ | 11/30 [04:21&lt;07:27, 23.54s/it]Epoch 11 Loss = 2.2082e-01, PNorm = 35.7909, GNorm = 0.7308, lr_0 = 4.6020e-04 Loss = 2.7329e-01, PNorm = 35.8341, GNorm = 0.6021, lr_0 = 4.4852e-04 Loss = 2.6603e-01, PNorm = 35.8722, GNorm = 0.9407, lr_0 = 4.3714e-04 Loss = 2.4075e-01, PNorm = 35.9113, GNorm = 2.0187, lr_0 = 4.2605e-04 Validation auc = 0.867018 40%|████ | 12/30 [04:45&lt;07:03, 23.55s/it]Epoch 12 Loss = 2.5492e-01, PNorm = 35.9484, GNorm = 1.0856, lr_0 = 4.1524e-04 Loss = 2.4939e-01, PNorm = 35.9858, GNorm = 0.4561, lr_0 = 4.0471e-04 Loss = 2.4741e-01, PNorm = 36.0232, GNorm = 1.5466, lr_0 = 3.9444e-04 Validation auc = 0.875294 43%|████▎ | 13/30 [05:09&lt;06:42, 23.67s/it]Epoch 13 Loss = 2.4982e-01, PNorm = 36.0614, GNorm = 2.4447, lr_0 = 3.8443e-04 Loss = 2.4805e-01, PNorm = 36.0925, GNorm = 1.2654, lr_0 = 3.7468e-04 Loss = 2.5467e-01, PNorm = 36.1294, GNorm = 1.3820, lr_0 = 3.6517e-04 Validation auc = 0.876470 47%|████▋ | 14/30 [05:35&lt;06:31, 24.45s/it]Epoch 14 Loss = 2.3493e-01, PNorm = 36.1666, GNorm = 0.9006, lr_0 = 3.5500e-04 Loss = 2.3090e-01, PNorm = 36.2007, GNorm = 0.9171, lr_0 = 3.4599e-04 Loss = 2.3686e-01, PNorm = 36.2300, GNorm = 2.0575, lr_0 = 3.3721e-04 Validation auc = 0.878013 50%|█████ | 15/30 [06:02&lt;06:18, 25.20s/it]Epoch 15 Loss = 2.9026e-01, PNorm = 36.2573, GNorm = 1.3134, lr_0 = 3.2866e-04 Loss = 1.9981e-01, PNorm = 36.2863, GNorm = 0.7343, lr_0 = 3.2032e-04 Loss = 1.9687e-01, PNorm = 36.3116, GNorm = 1.6622, lr_0 = 3.1219e-04 Loss = 2.5691e-01, PNorm = 36.3379, GNorm = 0.8183, lr_0 = 3.0427e-04 Validation auc = 0.875972 53%|█████▎ | 16/30 [06:28&lt;05:54, 25.36s/it]Epoch 16 Loss = 1.9455e-01, PNorm = 36.3669, GNorm = 1.2843, lr_0 = 2.9579e-04 Loss = 2.2575e-01, PNorm = 36.3932, GNorm = 0.6172, lr_0 = 2.8828e-04 Loss = 2.0250e-01, PNorm = 36.4198, GNorm = 0.9775, lr_0 = 2.8097e-04 Validation auc = 0.869266 57%|█████▋ | 17/30 [06:53&lt;05:28, 25.28s/it]Epoch 17 Loss = 1.9697e-01, PNorm = 36.4458, GNorm = 2.1296, lr_0 = 2.7384e-04 Loss = 2.1166e-01, PNorm = 36.4693, GNorm = 1.2592, lr_0 = 2.6689e-04 Loss = 1.9109e-01, PNorm = 36.4920, GNorm = 1.2560, lr_0 = 2.6012e-04 Validation auc = 0.875670 60%|██████ | 18/30 [07:16&lt;04:57, 24.80s/it]Epoch 18 Loss = 1.8643e-01, PNorm = 36.5195, GNorm = 1.2630, lr_0 = 2.5352e-04 Loss = 1.8821e-01, PNorm = 36.5462, GNorm = 1.1309, lr_0 = 2.4709e-04 Loss = 2.1482e-01, PNorm = 36.5691, GNorm = 2.0538, lr_0 = 2.4082e-04 Validation auc = 0.882305 63%|██████▎ | 19/30 [07:43&lt;04:36, 25.18s/it]Epoch 19 Loss = 3.6615e-01, PNorm = 36.5870, GNorm = 1.2744, lr_0 = 2.3411e-04 Loss = 1.7905e-01, PNorm = 36.6064, GNorm = 1.4056, lr_0 = 2.2817e-04 Loss = 1.6863e-01, PNorm = 36.6280, GNorm = 0.8830, lr_0 = 2.2238e-04 Loss = 2.2525e-01, PNorm = 36.6487, GNorm = 1.1407, lr_0 = 2.1674e-04 Validation auc = 0.878234 67%|██████▋ | 20/30 [08:06&lt;04:06, 24.65s/it]Epoch 20 Loss = 1.8261e-01, PNorm = 36.6677, GNorm = 0.5729, lr_0 = 2.1124e-04 Loss = 1.6636e-01, PNorm = 36.6873, GNorm = 2.5082, lr_0 = 2.0588e-04 Loss = 1.9284e-01, PNorm = 36.7066, GNorm = 0.6993, lr_0 = 2.0066e-04 Validation auc = 0.884596 70%|███████ | 21/30 [08:28&lt;03:34, 23.88s/it]Epoch 21 Loss = 1.7006e-01, PNorm = 36.7234, GNorm = 1.4929, lr_0 = 1.9557e-04 Loss = 1.7874e-01, PNorm = 36.7407, GNorm = 2.4642, lr_0 = 1.9060e-04 Loss = 1.8300e-01, PNorm = 36.7556, GNorm = 1.3921, lr_0 = 1.8577e-04 Validation auc = 0.880451 73%|███████▎ | 22/30 [08:50&lt;03:07, 23.41s/it]Epoch 22 Loss = 2.2415e-01, PNorm = 36.7748, GNorm = 3.0747, lr_0 = 1.8059e-04 Loss = 1.8420e-01, PNorm = 36.7904, GNorm = 1.5966, lr_0 = 1.7601e-04 Loss = 1.8406e-01, PNorm = 36.8065, GNorm = 3.2764, lr_0 = 1.7154e-04 Loss = 1.7007e-01, PNorm = 36.8239, GNorm = 2.2602, lr_0 = 1.6719e-04 Validation auc = 0.880731 77%|███████▋ | 23/30 [09:13&lt;02:42, 23.24s/it]Epoch 23 Loss = 1.8307e-01, PNorm = 36.8388, GNorm = 0.9624, lr_0 = 1.6295e-04 Loss = 1.6942e-01, PNorm = 36.8528, GNorm = 1.5294, lr_0 = 1.5882e-04 Loss = 1.9677e-01, PNorm = 36.8652, GNorm = 2.0170, lr_0 = 1.5479e-04 Validation auc = 0.883153 80%|████████ | 24/30 [09:37&lt;02:19, 23.31s/it]Epoch 24 Loss = 1.9167e-01, PNorm = 36.8796, GNorm = 1.6526, lr_0 = 1.5047e-04 Loss = 1.7906e-01, PNorm = 36.8896, GNorm = 1.4202, lr_0 = 1.4665e-04 Loss = 1.6134e-01, PNorm = 36.9037, GNorm = 1.5846, lr_0 = 1.4293e-04 Validation auc = 0.884527 83%|████████▎ | 25/30 [10:02&lt;01:59, 23.95s/it]Epoch 25 Loss = 2.1820e-01, PNorm = 36.9145, GNorm = 2.8235, lr_0 = 1.3931e-04 Loss = 1.5305e-01, PNorm = 36.9264, GNorm = 1.3115, lr_0 = 1.3577e-04 Loss = 1.4523e-01, PNorm = 36.9400, GNorm = 2.9751, lr_0 = 1.3233e-04 Validation auc = 0.884324 87%|████████▋ | 26/30 [10:28&lt;01:37, 24.48s/it]Epoch 26 Loss = 1.0589e-01, PNorm = 36.9495, GNorm = 0.5398, lr_0 = 1.2897e-04 Loss = 1.7584e-01, PNorm = 36.9614, GNorm = 0.7777, lr_0 = 1.2570e-04 Loss = 1.7186e-01, PNorm = 36.9732, GNorm = 1.1555, lr_0 = 1.2251e-04 Loss = 1.6196e-01, PNorm = 36.9816, GNorm = 0.6087, lr_0 = 1.1940e-04 Loss = 7.2826e-02, PNorm = 36.9825, GNorm = 1.8686, lr_0 = 1.1909e-04 Validation auc = 0.883706 90%|█████████ | 27/30 [10:52&lt;01:13, 24.43s/it]Epoch 27 Loss = 1.8395e-01, PNorm = 36.9932, GNorm = 1.1524, lr_0 = 1.1607e-04 Loss = 1.4853e-01, PNorm = 37.0025, GNorm = 2.9369, lr_0 = 1.1313e-04 Loss = 1.5653e-01, PNorm = 37.0122, GNorm = 1.2032, lr_0 = 1.1026e-04 Validation auc = 0.884723 93%|█████████▎| 28/30 [11:16&lt;00:48, 24.35s/it]Epoch 28 Loss = 1.3687e-01, PNorm = 37.0222, GNorm = 1.2745, lr_0 = 1.0746e-04 Loss = 1.4577e-01, PNorm = 37.0320, GNorm = 1.0341, lr_0 = 1.0473e-04 Loss = 2.0489e-01, PNorm = 37.0415, GNorm = 3.6318, lr_0 = 1.0208e-04 Validation auc = 0.884838 97%|█████████▋| 29/30 [11:41&lt;00:24, 24.34s/it]Epoch 29 Loss = 1.5350e-01, PNorm = 37.0487, GNorm = 1.8702, lr_0 = 1.0000e-04 Loss = 1.7822e-01, PNorm = 37.0573, GNorm = 1.4119, lr_0 = 1.0000e-04 Loss = 1.2261e-01, PNorm = 37.0653, GNorm = 1.2145, lr_0 = 1.0000e-04 Validation auc = 0.884231 100%|██████████| 30/30 [12:05&lt;00:00, 24.19s/it] Model 0 best validation auc = 0.884838 on epoch 28 Loading pretrained parameter &#34;encoder.encoder.0.cached_zero_vector&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_i.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_h.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.weight&#34;. Loading pretrained parameter &#34;encoder.encoder.0.W_o.bias&#34;. Loading pretrained parameter &#34;ffn.1.weight&#34;. Loading pretrained parameter &#34;ffn.1.bias&#34;. Loading pretrained parameter &#34;ffn.4.weight&#34;. Loading pretrained parameter &#34;ffn.4.bias&#34;. Model 0 test auc = 0.898943 Ensemble test auc = 0.898943 10-fold cross validation Seed 0 ==&gt; test auc = 0.844660 Seed 1 ==&gt; test auc = 0.859605 Seed 2 ==&gt; test auc = 0.884451 Seed 3 ==&gt; test auc = 0.893417 Seed 4 ==&gt; test auc = 0.938451 Seed 5 ==&gt; test auc = 0.876687 Seed 6 ==&gt; test auc = 0.906706 Seed 7 ==&gt; test auc = 0.897564 Seed 8 ==&gt; test auc = 0.899712 Seed 9 ==&gt; test auc = 0.898943 Overall test auc = 0.890020 +/- 0.024612 Elapsed time = 1:54:20 .",
            "url": "https://www.neongenes.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/12/data-splitting-and-training.ipynb.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/12/data-splitting-and-training.ipynb.html",
            "date": " • Nov 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a drug discovery web app - the roadmap",
            "content": "My goal: . The end goal for this project is to build a web app dashboard to monitor the accuracy of machine learning models on generating and classifying molecules with blood brain barrier permeability. .",
            "url": "https://www.neongenes.com//python/bioinformatics/datasets/smiles/cheminformatics/drug%20discovery/rdkit/2021/11/11/project-1-machine-learning.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/drug%20discovery/rdkit/2021/11/11/project-1-machine-learning.html",
            "date": " • Nov 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Processing the B3DB brain-blood barrier dataset",
            "content": "Gathering data on blood brain barrier permeability . This post is a part of a fullstack machine learning web app project and this notebook contains the data needed to build and train the models. The goal in this post is to clean and preprocess the B3DB dataset and merge it with the blood brain permeability data from MoleculeNet. The merged dataset should contain nearly 10,000 molecules labeled with their ability to pass through the blood brain barrier. The notebook provided in the B3DB repository also contains an interesting PCA plot, which is a good starting place for EDA when the data is merged. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from rdkit import Chem from rdkit.Chem import AllChem from sklearn.decomposition import PCA import datamol as dm %matplotlib inline . Removing, remapping, and creating features . This dataset has features that can be dropped since they won&#39;t contribute to model training. Some features are then calculated from the SMILES data. . # reading in the data bbb_df = pd.read_csv(&quot;data/B3DB_classification.tsv&quot;, sep=&quot; t&quot;) # dropping columns bbb_df = bbb_df.drop([&quot;CID&quot;, &quot;logBB&quot;, &quot;Inchi&quot;, &quot;threshold&quot;, &quot;reference&quot;, &quot;group&quot;, &quot;comments&quot;, &quot;NO.&quot;, &quot;IUPAC_name&quot;, &quot;compound_name&quot;], axis=1) # mapping given labels to binary bbb_df[&#39;BBB+/BBB-&#39;] = bbb_df[&#39;BBB+/BBB-&#39;].map({&#39;BBB+&#39;: 1, &#39;BBB-&#39;: 0}) . Feature generation . The function below processes and generates features such as mol objects, selfies, inchi, and inchikeys for each molecule using the datamol library. . # preprocessing function for molecules def preprocess_smiles(df): df[&quot;mol&quot;] = [dm.to_mol(x) for x in df[&#39;SMILES&#39;]] # generating mols from SMILES df[&quot;mol&quot;] = [dm.fix_mol(x) for x in df[&#39;mol&#39;]] # Fixing mols df = df.dropna() # dropping NA values df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=False) for x in df[&#39;mol&#39;]] # sanitize mol objects df[&quot;mol&quot;] = [dm.standardize_mol(x, disconnect_metals=False, normalize=True, reionize=True, uncharge=False, stereo=True) for x in df[&#39;mol&#39;]] # standardize mol objects df[&quot;standard_smiles&quot;] = [dm.standardize_smiles(x) for x in df[&#39;SMILES&#39;]] # standardize SMILES df[&quot;selfies&quot;] = [dm.to_selfies(x) for x in df[&#39;mol&#39;]] # generate SELFIES df[&quot;inchi&quot;] = [dm.to_inchi(x) for x in df[&#39;mol&#39;]] # Generating InChi df[&quot;inchikey&quot;] = [dm.to_inchikey(x) for x in df[&#39;mol&#39;]] # Generating InChIKey return df data_clean = preprocess_smiles(bbb_df) # Making a copy of the dataframe for later B3DB = data_clean data_clean.head() # Saving the data as B3DB; after where its found data_clean.to_csv(&#39;./data/B3DB.csv&#39;, index=False) . Merging the two datasets . The datasets would have ideally added up to 9846 molecules but that doesnt account for duplicates. Since the B3DB consists of data from across 50 studies, I assumed most of the molecules from MoleculeNet would appear in B3DB. . Counting the unique inchikey values confirmed this, leaving the final dataset with 8091 molecules. This number is lower than the sum of both dataframes but it adds 284 novel compounds to the final dataset. . # Loading the MoleculeNet dataset MolNet = pd.read_csv(&quot;data/MoleculeNet.csv&quot;) # concatenating both dataframes final_df = pd.concat([MolNet, B3DB]) . # Number of molecules before filtering for duplicates final_df.shape . (9846, 7) . # inchikey values should be unique to each molecule final_df.inchikey.value_counts() . FXHJGPDCPMCUKW-UHFFFAOYSA-N 4 UHSKFQJFRQCDBE-UHFFFAOYSA-N 4 CSIVCTHRYRVJCI-UHFFFAOYSA-N 4 UUQMNUMQCIQDMZ-UHFFFAOYSA-N 4 XHMYQXZLVLGNKX-UHFFFAOYSA-N 3 .. XYGVIBXOJOOCFR-BTJKTKAUSA-N 1 NINYZUDVKTUKIA-UHFFFAOYSA-N 1 BVCKFLJARNKCSS-ZJLJEUSSSA-N 1 HOCWPKXKMNXINF-CJIHYQBJSA-N 1 UWHAHBDBSBVMIY-VUXXLBMGSA-N 1 Name: inchikey, Length: 8091, dtype: int64 . # Dropping duplicates based on unique inchikey values final_df = final_df.drop_duplicates(subset=&#39;inchikey&#39;, keep=&quot;first&quot;) # Saving the final dataframe as b3_molecules final_df.to_csv(&#39;./data/b3_molecules.csv&#39;, index=False) . PCA analysis . PCA does not discard any variables and instead it reduces the number of dimensions by constructing principal components. Principal components describe variation and account for the varied influences of the original features. . Each SMILES string will have a morgan fingerprint generated. These fingerprints are binary 2048 bit arrays which describe molecular structure. . Three plots were made, the first using the MolNet dataset, the B3DB data next, and the combined data last. . # MolNet # compute ECFP6 Morgan fingerprints with radius 3 fps_molnet = [] for idx, row in MolNet.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_molnet.append(fp.ToBitString()) # Computing ECFP6 fingerprints for B3DB fps_B3DB = [] for idx, row in B3DB.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_B3DB.append(fp.ToBitString()) # Computing ECFP6 fingerprints for B3DB fps_final = [] for idx, row in final_df.iterrows(): mol = Chem.MolFromSmiles(row[&quot;SMILES&quot;]) mol = Chem.AddHs(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol=mol, radius=3, nBits=2048, useChirality=True, useFeatures=False) fps_final.append(fp.ToBitString()) # Create a numpy array and use the u1 datatype (uint8 8-bit unsigned integer) fps_arr_molnet = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_molnet]) fps_arr_B3DB = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_B3DB]) fps_arr_final = np.array([np.fromiter(fp, &quot;u1&quot;) for fp in fps_final]) # PCA on MolNet molecules molnet_fps = pd.DataFrame(fps_arr_molnet, index=MolNet.index) molnet_fps = pd.concat([MolNet, molnet_fps], axis=1) pca_molnet = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_molnet) molnet_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] molnet_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] # PCA on B3DB molecules B3DB_fps = pd.DataFrame(fps_arr_B3DB, index=B3DB.index) B3DB_fps = pd.concat([B3DB, B3DB_fps], axis=1) pca_B3DB = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_B3DB) B3DB_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] B3DB_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] # PCA on final set of molecules final_fps = pd.DataFrame(fps_arr_final, index=final_df.index) final_fps = pd.concat([final_df, final_fps], axis=1) pca_final = PCA(n_components=2) arr_fp_embedded = pca.fit_transform(fps_arr_final) final_fps[&quot;PC_1&quot;] = arr_fp_embedded[:, 0] final_fps[&quot;PC_2&quot;] = arr_fp_embedded[:, 1] . PCA Visualizations . PCA of molecules in the MolecularNet dataset . fig_molnet = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=molnet_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the MolecularNet dataset&#39;) plt.show() . PCA of molecules in the B3DB dataset . fig_B3DB = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=B3DB_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the B3DB dataset&#39;) plt.show() . PCA of molecules in the combined and filtered dataset . fig_final = plt.figure(figsize=(15, 10)) plt.xlabel(&quot;PC 1&quot;, fontsize=14) plt.ylabel(&quot;PC 2&quot;, fontsize=14) sns.scatterplot(data=final_fps, x=&quot;PC_1&quot;, y=&quot;PC_2&quot;, hue=&quot;BBB+/BBB-&quot;, palette=sns.color_palette([&quot;hotpink&quot;, &quot;dodgerblue&quot;]), linewidth=0.1, ).set(title=&#39;PCA of molecules in the combined and filtered dataset&#39;) plt.show() .",
            "url": "https://www.neongenes.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/09/using-the-B3DB-dataset.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/11/09/using-the-B3DB-dataset.html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Preprocessing a dataset of blood brain barrier molecules with Datamol",
            "content": "SMILES (Simplified Molecular Input Line Entry System) is a standard notation representing the molecular structure of a compound as a string representation that can be understood by a computer. The SMILES notation consists of a handful of rules which allow for converting the string to an image or graph. SMILES can then be easily used for generating further representations to train machine learning models with. . import datamol as dm import pandas as pd pd.options.mode.chained_assignment = None # default=&#39;warn&#39; . BBBP_df = pd.read_csv(&quot;data/BBBP.csv&quot;) BBBP_df.head() . num name p_np smiles . 0 1 | Propanolol | 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | . 1 2 | Terbutylchlorambucil | 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | . 2 3 | 40730 | 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | . 3 4 | 24 | 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | . 4 5 | cloxacillin | 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | . The dataframe shows 4 named columns, including the &quot;num&quot; of the molecule, the name, a binary label for blood brain barrier permeability status &quot;p_np&quot;, and the SMILES string. . # The name and number can be dropped BBBP_df = BBBP_df.drop([&quot;num&quot;, &quot;name&quot;], axis=1) # Checking the data for null values BBBP_df[&quot;smiles&quot;].isnull().values.any() # Renaming the binary label to &quot;BBB+/BBB-&quot; for clarity BBBP_df.columns = [&#39;BBB+/BBB-&#39;, &#39;SMILES&#39;] . BBBP_df . BBB+/BBB- SMILES . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | . 1 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | . 2 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | . 3 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | . 4 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | . ... ... | ... | . 2045 1 | C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl | . 2046 1 | [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](... | . 2047 1 | [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=... | . 2048 1 | C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC... | . 2049 1 | [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]... | . 2050 rows × 2 columns . Mols and smiles need to be sanitized as it will leave us with SMILES that are complete nonesense, for example, errors resulting from kekulization. . . RDkit generates the alternate position of double bonds, and then (in a second step they call &quot;aromatization&quot;) labels the ring as aromatic. In panel (2), there are three possible Lewis structures contributing to the actual structure (i.e. there is resonance), so the software would have to generate all three to be able to search for identical structures. [1] . Below is a function using datamol to preprocess the dataset, including steps to generate mol objects, SELFIES, inchi and inchikeys for each molecule. The function also standardizes mols and SMILES, drops NA values, and returns a dataframe. . def preprocess_smiles(df): df[&quot;mol&quot;] = [dm.to_mol(x) for x in df[&#39;SMILES&#39;]] # generating mols from SMILES df[&quot;mol&quot;] = [dm.fix_mol(x) for x in df[&#39;mol&#39;]] # Fixing mols df = df.dropna() # dropping NA values df[&quot;mol&quot;] = [dm.sanitize_mol(x, sanifix=True, charge_neutral=False) for x in df[&#39;mol&#39;]] # sanitize mol objects df[&quot;mol&quot;] = [dm.standardize_mol(x, disconnect_metals=False, normalize=True, reionize=True, uncharge=False, stereo=True) for x in df[&#39;mol&#39;]] # standardize mol objects df[&quot;standard_smiles&quot;] = [dm.standardize_smiles(x) for x in df[&#39;SMILES&#39;]] # standardize SMILES df[&quot;selfies&quot;] = [dm.to_selfies(x) for x in df[&#39;mol&#39;]] # generate SELFIES df[&quot;inchi&quot;] = [dm.to_inchi(x) for x in df[&#39;mol&#39;]] # Generating InChi df[&quot;inchikey&quot;] = [dm.to_inchikey(x) for x in df[&#39;mol&#39;]] # Generating InChIKey return df . Running the function and taking a look at the outputs . data_clean = preprocess_smiles(BBBP_df) . data_clean.shape . (2039, 7) . data_clean.head() . BBB+/BBB- SMILES mol standard_smiles selfies inchi inchikey . 0 1 | [Cl].CC(C)NCC(O)COc1cccc2ccccc12 | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)NCC(O)COc1cccc2ccccc12.[Cl-] | [C][C][Branch1][C][C][N][C][C][Branch1][C][O][... | InChI=1S/C16H21NO2.ClH/c1-12(2)17-10-14(18)11-... | ZMRUPTIKESYGQW-UHFFFAOYSA-M | . 1 1 | C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(C)(C)OC(=O)CCCc1ccc(N(CCCl)CCCl)cc1 | [C][C][Branch1][C][C][Branch1][C][C][O][C][=Br... | InChI=1S/C18H27Cl2NO2/c1-18(2,3)23-17(22)6-4-5... | SZXDOYFHSIIZCF-UHFFFAOYSA-N | . 2 1 | c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC1COc2c(N3CCN(C)CC3)c(F)cc3c(=O)c(C(=O)O)cn1c23 | [C][C][C][O][C][=C][Branch1][N][N][C][C][N][Br... | InChI=1S/C18H20FN3O4/c1-10-9-26-17-14-11(16(23... | GSDSWSVVBLHKDQ-UHFFFAOYSA-N | . 3 1 | C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | CC(=O)NCCCOc1cccc(CN2CCCCC2)c1 | [C][C][=Branch1][C][=O][N][C][C][C][O][C][=C][... | InChI=1S/C17H26N2O2/c1-15(20)18-9-6-12-21-17-8... | FAXLXLJWHQJMPK-UHFFFAOYSA-N | . 4 1 | Cc1onc(c2ccccc2Cl)c1C(=O)N[C@H]3[C@H]4SC(C)(C)... | &lt;img data-content=&quot;rdkit/molecule&quot; src=&quot;data:i... | Cc1onc(-c2ccccc2Cl)c1C(=O)N[C@@H]1C(=O)N2[C@@H... | [C][C][O][N][=C][Branch1][#Branch2][C][=C][C][... | InChI=1S/C19H18ClN3O5S/c1-8-11(12(22-28-8)9-6-... | LQOLIRLGBULYKD-JKIFEVAISA-N | . The data contains a 3:1 ratio of positive to negeative labels, which creates a bias towards molecules with blood brain permeability properties. This may need to be addressed when training models. The next steps are to save the cleaned data for further analysis. . counts = data_clean[&#39;BBB+/BBB-&#39;].value_counts().to_dict() print(counts) . {1: 1560, 0: 479} . data_clean.to_csv(&#39;./data/MoleculeNet.csv&#39;, index=False) . References . Urbaczek, Sascha. A consistent cheminformatics framework for automated virtual screening. Ph.D. Thesis, Universität Hamburg, August 2014. URL: http://ediss.sub.uni-hamburg.de/volltexte/2015/7349/; URN: urn:nbn:de:gbv:18-73491; PDF via Semantic Scholar |",
            "url": "https://www.neongenes.com//python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/10/22/working-with-SMILES.html",
            "relUrl": "/python/bioinformatics/datasets/smiles/cheminformatics/datamol/rdkit/molecules/2021/10/22/working-with-SMILES.html",
            "date": " • Oct 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Writing a function in python to perform a restriction enzyme digest",
            "content": "Creating a restriction enzyme dictionary . Restriction enzymes are proteins produced by bacteria that cleave DNA at specific sites along the molecule. The enzyme functions on a specific, short nucleotide sequence and cuts the DNA only at that specific site, which is known as restriction site or target sequence. In the bacterial cell, restriction enzymes cleave foreign DNA, thus eliminating infecting organisms. The activity of a restriction enzyme can be defined by its recognition site on the DNA sequence and the position relative to the recognition site, at which it cuts the DNA. . # create enzyme dictionary restrictionEnzymes = {} # add &quot;bamH1&quot; and &quot;sma1&quot; enzymes, their target sequence, and position releative to the recognition site restrictionEnzymes[&#39;bamH1&#39;] = [&#39;ggatcc&#39;,0] restrictionEnzymes[&#39;sma1&#39;] = [&#39;cccggg&#39;,2] # a function to calculate the molecular weight of dna sequences def oligoMolecularWeight(sequence): # create a dictionairy of DNA basepair molecular weights dnaMolecularWeight = {&#39;a&#39;:313.2,&#39;c&#39;:289.2,&#39;t&#39;:304.2,&#39;g&#39;:329.2} # initialize molecular weight molecularWeight = 0.0 # iterate through DNA sequnce and update weight of sequence for base in sequence: molecularWeight += dnaMolecularWeight[base] return molecularWeight # the primary function for restriction digest def digest(sequence, enzyme): # set target sequence target = restrictionEnzymes[enzyme][0] # enzyme cut position relative to recognition site cutPosition = restrictionEnzymes[enzyme][1] # a list to collect DNA fragments fragments = [] # counter for the position of the last restriction site; beginning of sequence found = 0 # a variable to store the position of the last cut; end of sequence lastCut = found # variable to set where to search for the next site from searchFrom = lastCut while found != -1: found = sequence.find(target, searchFrom) if found != -1: fragment = sequence[lastCut:found+cutPosition] mwt = oligoMolecularWeight(fragment) fragments.append((fragment,mwt)) else: fragment = sequence[lastCut:] mwt = oligoMolecularWeight(fragment) fragments.append((fragment,mwt)) lastCut = found + cutPosition searchFrom = lastCut + 1 return fragments . Running the function on a test sequence results in the following: . digestSequence = &quot;gcgatgctaggatccgcgatcgcgtacgatcgtacgcggtacggacggatccttctc&quot; . digested_dna = digest(digestSequence,&#39;bamH1&#39;) . print(digested_dna) . [(&#39;gcgatgcta&#39;, 2800.7999999999997), (&#39;ggatccgcgatcgcgtacgatcgtacgcggtacggac&#39;, 11478.400000000005), (&#39;ggatccttctc&#39;, 3345.1999999999994)] .",
            "url": "https://www.neongenes.com//python/bioinformatics/restriction%20enzyme/function/2021/10/18/restriction-enzyme-digest.html",
            "relUrl": "/python/bioinformatics/restriction%20enzyme/function/2021/10/18/restriction-enzyme-digest.html",
            "date": " • Oct 18, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "A primer on Apache Airflow",
            "content": "What is airflow? . Airflow is a platform used to author, schedule, and monitor workflows. It’s essentially a queuing system that runs on a metadata database and a scheduler that runs tasks. Workflows are written as Directed Acyclic Graphs (DAGs). A workflow and DAG are interchangeable. . What are DAGs? . A DAG is a collection of tasks you want to run and are organized in a way that illustrates dependencies and relationships between tasks. . The image below shows how a DAG is a unidirectional, acyclic graph, where each node in the graph is a task and edges define dependencies among tasks. There is no case where you should be able to go backwards from a forward node to one that&#39;s already been executed. . . A DAG can be broken up into smaller and smaller jobs and gives the user full control by generating dynamic pipelines written in code. Airflow DAGs are also extensible and can scale. DAGs are powerful because they allow for collaborative, manageable, and testable workflows. A bonus is that Airflow is developed in python and can interface with any python API. . . The image above shows how Airflow divides the tasks into branches so that if one fails, there is still output from the other. Also, the processing time is reduced as parallel computing occurs. The chances of failure should decrease overall as each task is independent. . How are tasks executed? . An operator represents a single task in a workflow that helps carry out your task (running a python function for example). Operators determine what actually gets to be done when your dag runs. A task is an operator when instantiated. It is something on which the worker works upon. . Airflow Architecture . Metadata — is a relational database with info on task state, such as the top ten tasks consuming the most memory, it contains all data pertaining to jobs currently running as well as historical data. . | Scheduler — decides which task to run, when, and in what order. . | Web server— the UI which is essentially a flask app that talks to the metadata. . | Executor — performs the task at ground level. The executor is a message queuing process which figures out which workers will execute which tasks. The default is the sequential executor — which cannot run tasks in parallel — meaning it can’t be used for production level code. The local executor can be used too which will run tasks till all resources on the server are at capacity. This is good for a moderate amount of DAGs. Both of these are used in single node clusters and therefore cannot be used to scaled. . | Multi node clusters — have the same components and only the scheduler and web server are placed in the same node (master), the workers are placed in a separate instance. This set up works well because it allows for scaling by letting you add more multi-node clusters (celery is the executor of choice here for python). . | . If you&#39;re not dealing with terabytes of data then it&#39;s better to have the scheduler, web server, and executor together in the master node/cluster. The downside is that this single cluster approach runs everything on the same machine, so if you make a change to a DAG/scheduler, then you need to restart the entire workflow — even tasks that were in the process of executing. Celery avoids this. . . If you do build a distributed workflow with celery then a queuing system component is needed (like Redis). For local workflows, the queuing is handled by the system. . The life cycle of a task . The scheduler periodically checks the DAG folder to see if there are any DAGS that need to be run. . | If any DAGS are found pending execution, the scheduler creates a diagram for it, which is an instantiation of a DAG in real time. . | The scheduler will update the DAG state to running in the metadata and the tasks will execute. . | The scheduler then reads the DAG and puts the tasks in order of execution into the queuing system in the form of a message. Each message contains info like DAG ID, TASK ID, and function to be executed. . | The status of these tasks changes to queued at that point. . | The executor then begins to execute tasks and sends fail/success messages for the tasks to the metadata. . | The scheduler finally updates the status of the diagram when all tasks have run to success or fail. . |",
            "url": "https://www.neongenes.com//data-engineering/python/airflow/dag/2021/04/20/airflow-primer.html",
            "relUrl": "/data-engineering/python/airflow/dag/2021/04/20/airflow-primer.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.neongenes.com//markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.neongenes.com//about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.neongenes.com//robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}